#!/usr/bin/env python3
"""
Crawl ph·∫ßn H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT t·ª´ trang loigiaihay.com
S·ª≠ d·ª•ng th∆∞ vi·ªán crawl4ai ƒë·ªÉ l·∫•y n·ªôi dung t·ª´ ph·∫ßn t·ª≠ c√≥ ID 'sub-question-2'
"""

import asyncio
import json
from pathlib import Path
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai import JsonCssExtractionStrategy


async def crawl_huong_dan_giai_chi_tiet(url: str):
    """
    Crawl ph·∫ßn H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT t·ª´ trang loigiaihay.com
    
    Args:
        url: URL c·ªßa trang ƒë·ªÅ thi c·∫ßn crawl
        
    Returns:
        dict: D·ªØ li·ªáu ƒë√£ crawl ƒë∆∞·ª£c
    """
    
    # C·∫•u h√¨nh ƒë·ªÉ crawl to√†n b·ªô trang tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra c·∫•u tr√∫c
    initial_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["script", "style", "nav", "footer"],
        word_count_threshold=5
    )
    
    print(f"üîç ƒêang ki·ªÉm tra c·∫•u tr√∫c trang: {url}")
    
    async with AsyncWebCrawler() as crawler:
        # Crawl to√†n b·ªô trang tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra
        result = await crawler.arun(url=url, config=initial_config)
        
        if not result.success:
            print(f"‚ùå L·ªói khi crawl trang: {result.error_message}")
            return None
        
        # L∆∞u HTML ƒë·ªÉ ki·ªÉm tra c·∫•u tr√∫c
        html_output_path = Path("crawl4ai/output/loigiaihay_full_structure.html")
        html_output_path.parent.mkdir(exist_ok=True)
        
        with open(html_output_path, 'w', encoding='utf-8') as f:
            f.write(result.cleaned_html)
        
        print(f"üíæ ƒê√£ l∆∞u c·∫•u tr√∫c HTML v√†o: {html_output_path}")
        
        # Ki·ªÉm tra xem c√≥ ph·∫ßn t·ª≠ v·ªõi ID 'sub-question-2' kh√¥ng
        if 'id="sub-question-2"' in result.cleaned_html:
            print("‚úÖ T√¨m th·∫•y ph·∫ßn t·ª≠ v·ªõi ID 'sub-question-2'")
        else:
            print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ph·∫ßn t·ª≠ v·ªõi ID 'sub-question-2', ƒëang t√¨m c√°c pattern kh√°c...")
            
            # T√¨m ki·∫øm c√°c pattern c√≥ th·ªÉ li√™n quan
            patterns_to_check = [
                'H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT',
                'sub-question',
                'box-question',
                'h∆∞·ªõng d·∫´n gi·∫£i',
                'gi·∫£i chi ti·∫øt'
            ]
            
            found_patterns = []
            for pattern in patterns_to_check:
                if pattern.lower() in result.cleaned_html.lower():
                    found_patterns.append(pattern)
            
            if found_patterns:
                print(f"üîç T√¨m th·∫•y c√°c pattern: {', '.join(found_patterns)}")
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y pattern n√†o li√™n quan")
    
    return result


async def crawl_specific_huong_dan_giai(url: str):
    """
    Crawl c·ª• th·ªÉ ph·∫ßn H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT
    
    Args:
        url: URL c·ªßa trang ƒë·ªÅ thi
        
    Returns:
        dict: N·ªôi dung ph·∫ßn h∆∞·ªõng d·∫´n gi·∫£i
    """
    
    # Schema ƒë·ªÉ extract n·ªôi dung H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT
    huong_dan_schema = {
        "name": "H∆∞·ªõng D·∫´n Gi·∫£i Chi Ti·∫øt",
        "baseSelector": "#sub-question-2, .box-question[id='sub-question-2']",
        "fields": [
            {
                "name": "tieu_de",
                "selector": "h2, h3, .title, .heading",
                "type": "text"
            },
            {
                "name": "noi_dung_giai",
                "selector": ".content, .solution, .answer, p, div",
                "type": "text"
            },
            {
                "name": "cac_buoc_giai", 
                "selector": "ol li, ul li, .step",
                "type": "text"
            },
            {
                "name": "cong_thuc",
                "selector": ".formula, .math, .equation",
                "type": "text" 
            },
            {
                "name": "hinh_anh",
                "selector": "img",
                "type": "attribute",
                "attribute": "src"
            }
        ]
    }
    
    # C·∫•u h√¨nh crawl v·ªõi CSS selector c·ª• th·ªÉ
    config = CrawlerRunConfig(
        css_selector="#sub-question-2",  # Ch·ªâ l·∫•y ph·∫ßn t·ª≠ n√†y
        extraction_strategy=JsonCssExtractionStrategy(huong_dan_schema),
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["script", "style"],
        word_count_threshold=3
    )
    
    print(f"üéØ ƒêang crawl ph·∫ßn H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT t·ª´: {url}")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        
        if not result.success:
            print(f"‚ùå L·ªói khi crawl: {result.error_message}")
            return None
        
        # Parse k·∫øt qu·∫£ JSON
        try:
            extracted_data = json.loads(result.extracted_content) if result.extracted_content else []
            
            if extracted_data:
                print(f"‚úÖ ƒê√£ extract ƒë∆∞·ª£c {len(extracted_data)} ph·∫ßn t·ª≠")
                
                # L∆∞u k·∫øt qu·∫£ v√†o file JSON
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = Path(f"crawl4ai/output/huong_dan_giai_chi_tiet_{timestamp}.json")
                output_file.parent.mkdir(exist_ok=True)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data, f, ensure_ascii=False, indent=2)
                
                print(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_file}")
                
                # Hi·ªÉn th·ªã n·ªôi dung ƒë√£ extract
                for i, item in enumerate(extracted_data, 1):
                    print(f"\nüìù Ph·∫ßn {i}:")
                    for key, value in item.items():
                        if value and value.strip():
                            print(f"  {key}: {value[:200]}{'...' if len(str(value)) > 200 else ''}")
                
                return extracted_data
            else:
                print("‚ö†Ô∏è  Kh√¥ng extract ƒë∆∞·ª£c d·ªØ li·ªáu, th·ª≠ v·ªõi selector kh√°c...")
                return await crawl_fallback_method(url)
                
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói parse JSON: {e}")
            print(f"Raw content: {result.extracted_content[:500]}...")
            return None


async def crawl_fallback_method(url: str):
    """
    Ph∆∞∆°ng ph√°p d·ª± ph√≤ng khi kh√¥ng t√¨m th·∫•y ID c·ª• th·ªÉ
    """
    print("üîÑ ƒêang th·ª≠ ph∆∞∆°ng ph√°p d·ª± ph√≤ng...")
    
    # Schema m·ªü r·ªông ƒë·ªÉ t√¨m c√°c pattern kh√°c
    fallback_schema = {
        "name": "H∆∞·ªõng D·∫´n Gi·∫£i - Fallback",
        "baseSelector": ".box-question, .solution-box, [class*='question'], [class*='answer'], [class*='solution']",
        "fields": [
            {
                "name": "tieu_de",
                "selector": "h1, h2, h3, h4, .title",
                "type": "text"
            },
            {
                "name": "noi_dung",
                "selector": "p, div, .content",
                "type": "text"
            }
        ]
    }
    
    config = CrawlerRunConfig(
        extraction_strategy=JsonCssExtractionStrategy(fallback_schema),
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=10
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        
        if result.success and result.extracted_content:
            try:
                data = json.loads(result.extracted_content)
                
                # L·ªçc ch·ªâ l·∫•y nh·ªØng ph·∫ßn c√≥ ch·ª©a "h∆∞·ªõng d·∫´n" ho·∫∑c "gi·∫£i"
                filtered_data = []
                for item in data:
                    content = str(item).lower()
                    if any(keyword in content for keyword in ['h∆∞·ªõng d·∫´n', 'gi·∫£i', 'chi ti·∫øt', 'b√†i gi·∫£i']):
                        filtered_data.append(item)
                
                if filtered_data:
                    print(f"‚úÖ T√¨m th·∫•y {len(filtered_data)} ph·∫ßn c√≥ li√™n quan ƒë·∫øn h∆∞·ªõng d·∫´n gi·∫£i")
                    return filtered_data
                
            except json.JSONDecodeError:
                pass
    
    return None


async def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y crawler"""
    url = "https://loigiaihay.com/de-thi-vao-lop-6-mon-toan-truong-cau-giay-nam-2023-a142098.html"
    
    print("üöÄ B·∫Øt ƒë·∫ßu crawl trang loigiaihay.com")
    print(f"üîó URL: {url}")
    print("=" * 60)
    
    # B∆∞·ªõc 1: Ki·ªÉm tra c·∫•u tr√∫c trang
    await crawl_huong_dan_giai_chi_tiet(url)
    
    print("\n" + "=" * 60)
    
    # B∆∞·ªõc 2: Crawl c·ª• th·ªÉ ph·∫ßn H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT
    result = await crawl_specific_huong_dan_giai(url)
    
    if result:
        print("\n‚úÖ Ho√†n th√†nh crawl th√†nh c√¥ng!")
    else:
        print("\n‚ùå Kh√¥ng th·ªÉ crawl ƒë∆∞·ª£c n·ªôi dung mong mu·ªën")
    
    return result


if __name__ == "__main__":
    # Ch·∫°y crawler
    asyncio.run(main()) 