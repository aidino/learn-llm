import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode
import json
from datetime import datetime
import os

async def crawl_medium_article():
    """
    Crawl content tá»« Medium article vá» "A Real-time Retrieval System for RAG on Social Media Data"
    """
    
    # URL cáº§n crawl
    url = "https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on-social-media-data-9cc01d50a2a0"
    
    # Cáº¥u hÃ¬nh browser
    browser_config = BrowserConfig(
        headless=True,  # Cháº¡y áº©n browser
        verbose=True,   # Hiá»ƒn thá»‹ thÃ´ng tin debug
        viewport_width=1280,
        viewport_height=720
    )
    
    # Cáº¥u hÃ¬nh crawler
    crawler_config = CrawlerRunConfig(
        # Content processing
        word_count_threshold=10,  # Lá»c cÃ¡c Ä‘oáº¡n text cÃ³ Ã­t hÆ¡n 10 tá»«
        excluded_tags=['nav', 'header', 'footer', 'aside'],  # Loáº¡i bá» cÃ¡c tháº» khÃ´ng cáº§n thiáº¿t
        exclude_external_links=True,  # Loáº¡i bá» link ngoÃ i
        
        # Cache control
        cache_mode=CacheMode.BYPASS,  # Bá» qua cache Ä‘á»ƒ láº¥y content má»›i
        
        # Media handling
        exclude_external_images=False,  # Giá»¯ láº¡i áº£nh tá»« external
        wait_for_images=True,  # Äá»£i áº£nh load xong
        
        # Screenshot
        screenshot=True,  # Chá»¥p screenshot trang
        
        # Xá»­ lÃ½ content Ä‘á»™ng
        process_iframes=True,
        remove_overlay_elements=True
    )
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u crawl Medium article...")
    print(f"ğŸ“ URL: {url}")
    print("-" * 80)
    
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # Thá»±c hiá»‡n crawl
            result = await crawler.arun(
                url=url,
                config=crawler_config
            )
            
            if result.success:
                print("âœ… Crawl thÃ nh cÃ´ng!")
                
                # ThÃ´ng tin cÆ¡ báº£n
                print(f"\nğŸ“Š THÃ”NG TIN CÆ  Báº¢N:")
                print(f"   - URL: {result.url}")
                print(f"   - Status Code: {result.status_code}")
                print(f"   - Äá»™ dÃ i HTML: {len(result.html):,} kÃ½ tá»±")
                print(f"   - Äá»™ dÃ i Markdown: {len(result.markdown):,} kÃ½ tá»±")
                print(f"   - Thá»i gian crawl: {result.response_headers.get('date', 'N/A')}")
                
                # In má»™t pháº§n content Ä‘á»ƒ xem trÆ°á»›c
                print(f"\nğŸ“ CONTENT PREVIEW (500 kÃ½ tá»± Ä‘áº§u):")
                print("-" * 50)
                print(result.markdown[:500])
                print("-" * 50)
                
                # ThÃ´ng tin vá» links
                internal_links = result.links.get("internal", [])
                external_links = result.links.get("external", [])
                print(f"\nğŸ”— LINKS:")
                print(f"   - Internal links: {len(internal_links)}")
                print(f"   - External links: {len(external_links)}")
                
                # ThÃ´ng tin vá» media
                images = result.media.get("images", [])
                print(f"\nğŸ–¼ï¸ MEDIA:")
                print(f"   - Tá»•ng sá»‘ áº£nh: {len(images)}")
                
                if images:
                    print("   - Top 3 áº£nh:")
                    for i, img in enumerate(images[:3]):
                        print(f"     {i+1}. {img['src']}")
                        print(f"        Alt: {img.get('alt', 'N/A')}")
                
                # LÆ°u káº¿t quáº£ vÃ o files
                await save_results(result, url)
                
                return result
                
            else:
                print(f"âŒ Crawl tháº¥t báº¡i: {result.error_message}")
                return None
                
    except Exception as e:
        print(f"âŒ Lá»—i khi crawl: {str(e)}")
        return None

async def save_results(result, url):
    """
    LÆ°u káº¿t quáº£ crawl vÃ o cÃ¡c file khÃ¡c nhau
    """
    # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"medium_article_{timestamp}"
    
    print(f"\nğŸ’¾ ÄANG LÆ¯U Káº¾T QUáº¢...")
    
    # 1. LÆ°u raw HTML
    html_file = os.path.join(output_dir, f"{base_filename}_raw.html")
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(result.html)
    print(f"   âœ… Raw HTML: {html_file}")
    
    # 2. LÆ°u cleaned HTML 
    if result.cleaned_html:
        cleaned_html_file = os.path.join(output_dir, f"{base_filename}_cleaned.html")
        with open(cleaned_html_file, 'w', encoding='utf-8') as f:
            f.write(result.cleaned_html)
        print(f"   âœ… Cleaned HTML: {cleaned_html_file}")
    
    # 3. LÆ°u Markdown
    markdown_file = os.path.join(output_dir, f"{base_filename}.md")
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(result.markdown)
    print(f"   âœ… Markdown: {markdown_file}")
    
    # 4. LÆ°u thÃ´ng tin metadata dÆ°á»›i dáº¡ng JSON
    metadata = {
        "url": result.url,
        "status_code": result.status_code,
        "crawl_timestamp": timestamp,
        "html_length": len(result.html),
        "markdown_length": len(result.markdown),
        "internal_links_count": len(result.links.get("internal", [])),
        "external_links_count": len(result.links.get("external", [])),
        "images_count": len(result.media.get("images", [])),
        "response_headers": dict(result.response_headers) if result.response_headers else {}
    }
    
    metadata_file = os.path.join(output_dir, f"{base_filename}_metadata.json")
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   âœ… Metadata: {metadata_file}")
    
    # 5. LÆ°u links
    links_data = {
        "internal_links": result.links.get("internal", []),
        "external_links": result.links.get("external", [])
    }
    
    links_file = os.path.join(output_dir, f"{base_filename}_links.json")
    with open(links_file, 'w', encoding='utf-8') as f:
        json.dump(links_data, f, indent=2, ensure_ascii=False)
    print(f"   âœ… Links: {links_file}")
    
    # 6. LÆ°u thÃ´ng tin images
    if result.media.get("images"):
        images_file = os.path.join(output_dir, f"{base_filename}_images.json")
        with open(images_file, 'w', encoding='utf-8') as f:
            json.dump(result.media["images"], f, indent=2, ensure_ascii=False)
        print(f"   âœ… Images: {images_file}")
    
    # 7. LÆ°u screenshot náº¿u cÃ³
    if result.screenshot:
        screenshot_file = os.path.join(output_dir, f"{base_filename}_screenshot.png")
        try:
            with open(screenshot_file, 'wb') as f:
                # Kiá»ƒm tra xem screenshot lÃ  bytes hay string
                if isinstance(result.screenshot, str):
                    # Náº¿u lÃ  string (base64), decode trÆ°á»›c
                    import base64
                    f.write(base64.b64decode(result.screenshot))
                else:
                    # Náº¿u Ä‘Ã£ lÃ  bytes, ghi trá»±c tiáº¿p
                    f.write(result.screenshot)
            print(f"   âœ… Screenshot: {screenshot_file}")
        except Exception as e:
            print(f"   âš ï¸ KhÃ´ng thá»ƒ lÆ°u screenshot: {str(e)}")

async def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y crawler
    """
    print("ğŸ¤– CRAWL4AI - MEDIUM ARTICLE CRAWLER")
    print("=" * 50)
    
    result = await crawl_medium_article()
    
    if result:
        print(f"\nğŸ‰ HOÃ€N THÃ€NH!")
        print("ğŸ“ Táº¥t cáº£ file Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c 'output/'")
        print("\nğŸ’¡ Báº¡n cÃ³ thá»ƒ:")
        print("   - Äá»c file .md Ä‘á»ƒ xem content Ä‘Ã£ Ä‘Æ°á»£c format")
        print("   - Xem file .html Ä‘á»ƒ xem raw content")
        print("   - Kiá»ƒm tra file .json Ä‘á»ƒ xem metadata vÃ  links")
    else:
        print("\nğŸ˜ Crawl khÃ´ng thÃ nh cÃ´ng. Vui lÃ²ng kiá»ƒm tra láº¡i URL hoáº·c káº¿t ná»‘i máº¡ng.")

if __name__ == "__main__":
    asyncio.run(main()) 