#!/usr/bin/env python3
"""
Script crawl HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T vá»›i cáº¥u hÃ¬nh tá»« file JSON
Sá»­ dá»¥ng file config_crawl.json Ä‘á»ƒ tÃ¹y chá»‰nh cÃ¡c tham sá»‘
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


def load_config(config_file="crawl4ai/config_crawl.json"):
    """Äá»c file cáº¥u hÃ¬nh JSON"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file cáº¥u hÃ¬nh: {config_file}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ Lá»—i Ä‘á»c file cáº¥u hÃ¬nh: {e}")
        return None


async def crawl_with_config(url: str, config: dict):
    """
    Crawl vá»›i cáº¥u hÃ¬nh tá»« file JSON
    
    Args:
        url: URL cáº§n crawl
        config: Dictionary cáº¥u hÃ¬nh tá»« JSON
    """
    
    # Láº¥y cáº¥u hÃ¬nh crawl
    crawl_conf = config.get("crawl_config", {})
    selectors = config.get("selectors", {})
    output_conf = config.get("output", {})
    
    # Táº¡o CrawlerRunConfig
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    crawler_config = CrawlerRunConfig(
        css_selector=selectors.get("primary"),
        cache_mode=cache_mode,
        excluded_tags=crawl_conf.get("excluded_tags", []),
        word_count_threshold=crawl_conf.get("word_count_threshold", 3),
        exclude_external_links=crawl_conf.get("exclude_external_links", True),
        exclude_external_images=crawl_conf.get("exclude_external_images", True)
    )
    
    print(f"ğŸ” Äang crawl: {url}")
    print(f"ğŸ¯ Primary selector: {selectors.get('primary')}")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=crawler_config)
        
        if not result.success:
            print(f"âŒ Lá»—i: {result.error_message}")
            return None
        
        if result.cleaned_html:
            print("âœ… TÃ¬m tháº¥y ná»™i dung vá»›i primary selector!")
            return await save_results(result, config, "primary")
        else:
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i primary selector")
            return await crawl_fallback(url, config)


async def crawl_fallback(url: str, config: dict):
    """PhÆ°Æ¡ng phÃ¡p dá»± phÃ²ng"""
    print("ğŸ”„ Thá»­ fallback selector...")
    
    selectors = config.get("selectors", {})
    crawl_conf = config.get("crawl_config", {})
    
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    fallback_config = CrawlerRunConfig(
        css_selector=selectors.get("fallback"),
        cache_mode=cache_mode,
        excluded_tags=crawl_conf.get("excluded_tags", []),
        word_count_threshold=crawl_conf.get("word_count_threshold", 5)
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=fallback_config)
        
        if result.success and result.markdown:
            # TÃ¬m kiáº¿m keywords trong ná»™i dung
            keywords = config.get("keywords", {}).get("target", [])
            content = result.markdown
            
            # TÃ¬m section chá»©a HÆ¯á»šNG DáºªN GIáº¢I
            lines = content.split('\n')
            found_section = []
            capture = False
            
            for line in lines:
                if any(keyword.upper() in line.upper() for keyword in keywords):
                    capture = True
                    found_section.append(line)
                elif capture:
                    if line.strip() and not line.startswith('#'):
                        found_section.append(line)
                    elif line.startswith('##') and capture:
                        break
            
            if found_section:
                print("âœ… TÃ¬m tháº¥y section HÆ¯á»šNG DáºªN GIáº¢I!")
                
                # Táº¡o result object cho fallback
                class FallbackResult:
                    def __init__(self, content, url):
                        self.markdown = '\n'.join(content)
                        self.cleaned_html = f"<div>{self.markdown}</div>"
                        self.url = url
                        self.success = True
                
                fallback_result = FallbackResult(found_section, url)
                return await save_results(fallback_result, config, "fallback")
        
        print("âŒ KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i fallback method")
        return None


async def save_results(result, config: dict, method: str):
    """LÆ°u káº¿t quáº£ theo cáº¥u hÃ¬nh"""
    output_conf = config.get("output", {})
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path(output_conf.get("directory", "crawl4ai/output"))
    output_dir.mkdir(exist_ok=True)
    
    prefix = output_conf.get("prefix", "huong_dan_giai")
    formats = output_conf.get("formats", ["markdown"])
    
    results = {
        'method': method,
        'timestamp': timestamp,
        'files': []
    }
    
    # LÆ°u theo format Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
    if "html" in formats:
        html_file = output_dir / f"{prefix}_html_{method}_{timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(result.cleaned_html)
        results['files'].append(str(html_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u HTML: {html_file}")
    
    if "markdown" in formats:
        md_file = output_dir / f"{prefix}_md_{method}_{timestamp}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(result.markdown)
        results['files'].append(str(md_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u Markdown: {md_file}")
    
    if "json" in formats:
        json_file = output_dir / f"{prefix}_info_{method}_{timestamp}.json"
        info = {
            'url': result.url,
            'method': method,
            'timestamp': timestamp,
            'content_length': len(result.markdown),
            'html_length': len(result.cleaned_html)
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        results['files'].append(str(json_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u JSON: {json_file}")
    
    # Hiá»ƒn thá»‹ preview
    print(f"\nğŸ“– Preview ná»™i dung:")
    print("-" * 50)
    preview = result.markdown[:1000] + "..." if len(result.markdown) > 1000 else result.markdown
    print(preview)
    print("-" * 50)
    
    return results


def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ CRAWL4AI - HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T (Config Mode)")
    print("=" * 60)
    
    # Äá»c cáº¥u hÃ¬nh
    config = load_config()
    if not config:
        print("âŒ KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c file cáº¥u hÃ¬nh!")
        return
    
    # Chá»n URL
    urls = config.get("urls", {})
    if not urls:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y URL nÃ o trong cáº¥u hÃ¬nh!")
        return
    
    print("ğŸ“‹ CÃ¡c URL cÃ³ sáºµn:")
    for key, url in urls.items():
        print(f"  {key}: {url}")
    
    # Sá»­ dá»¥ng URL Ä‘áº§u tiÃªn lÃ m máº·c Ä‘á»‹nh
    url_key = list(urls.keys())[0]
    url = urls[url_key]
    
    print(f"\nğŸ”— Sá»­ dá»¥ng URL: {url_key}")
    print(f"ğŸŒ {url}")
    print("=" * 60)
    
    # Cháº¡y crawler
    result = asyncio.run(crawl_with_config(url, config))
    
    if result:
        print(f"\nâœ… THÃ€NH CÃ”NG!")
        print(f"ğŸ“ PhÆ°Æ¡ng phÃ¡p: {result['method']}")
        print(f"ğŸ“‚ Files Ä‘Ã£ táº¡o: {len(result['files'])}")
        for file_path in result['files']:
            print(f"   ğŸ“„ {file_path}")
    else:
        print(f"\nâŒ KHÃ”NG THÃ€NH CÃ”NG!")
        print("ğŸ’¡ HÃ£y kiá»ƒm tra URL hoáº·c cáº­p nháº­t cáº¥u hÃ¬nh selectors")


if __name__ == "__main__":
    main() 