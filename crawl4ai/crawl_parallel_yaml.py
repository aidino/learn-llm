#!/usr/bin/env python3
"""
Script crawl HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T vá»›i crawling song song
Crawl táº¥t cáº£ URLs tá»« config YAML Ä‘á»“ng thá»i Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t
"""

import asyncio
import json
import sys
import yaml
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


@dataclass
class CrawlResult:
    """Káº¿t quáº£ crawl cho má»™t URL"""
    url: str
    success: bool
    method: str
    content: str
    html: str
    error: Optional[str] = None
    duration: float = 0.0
    content_length: int = 0


def load_config(config_file="crawl4ai/config_crawl.yaml") -> Optional[Dict]:
    """Äá»c file cáº¥u hÃ¬nh YAML"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"âœ… ÄÃ£ Ä‘á»c file cáº¥u hÃ¬nh: {config_file}")
            return config
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file cáº¥u hÃ¬nh: {config_file}")
        print("ğŸ’¡ HÃ£y táº¡o file config_crawl.yaml hoáº·c kiá»ƒm tra Ä‘Æ°á»ng dáº«n")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ Lá»—i Ä‘á»c file YAML: {e}")
        return None
    except ImportError:
        print("âŒ Thiáº¿u thÆ° viá»‡n PyYAML. CÃ i Ä‘áº·t báº±ng: pip install pyyaml")
        return None


async def crawl_single_url(url: str, config: Dict, url_index: int) -> CrawlResult:
    """
    Crawl má»™t URL duy nháº¥t vá»›i retry mechanism
    
    Args:
        url: URL cáº§n crawl
        config: Dictionary cáº¥u hÃ¬nh tá»« YAML
        url_index: Index cá»§a URL trong danh sÃ¡ch (Ä‘á»ƒ tracking)
    
    Returns:
        CrawlResult: Káº¿t quáº£ crawl
    """
    start_time = time.time()
    
    # Láº¥y cáº¥u hÃ¬nh
    crawl_conf = config.get("crawl_config", {})
    selectors = config.get("selectors", {})
    advanced = config.get("advanced", {})
    parallel_conf = config.get("parallel_config", {})
    
    print(f"ğŸ” [{url_index+1}] Äang crawl: {url[:80]}...")
    
    # Táº¡o CrawlerRunConfig
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    crawler_config = CrawlerRunConfig(
        css_selector=selectors.get("id_selector", selectors.get("primary")),
        cache_mode=cache_mode,
        excluded_tags=crawl_conf.get("excluded_tags", []),
        word_count_threshold=crawl_conf.get("word_count_threshold", 3),
        exclude_external_links=crawl_conf.get("exclude_external_links", True),
        exclude_external_images=crawl_conf.get("exclude_external_images", True),
        exclude_social_media_links=crawl_conf.get("exclude_social_media_links", True)
    )
    
    # Retry mechanism
    max_retries = advanced.get("max_retries", 3)
    delay = advanced.get("delay_between_requests", 1)
    timeout = parallel_conf.get("per_url_timeout", 45)
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f"ğŸ”„ [{url_index+1}] Thá»­ láº¡i láº§n {attempt + 1}/{max_retries}...")
                await asyncio.sleep(delay)
            
            # Sá»­ dá»¥ng timeout cho má»—i URL
            async with AsyncWebCrawler() as crawler:
                result = await asyncio.wait_for(
                    crawler.arun(url=url, config=crawler_config),
                    timeout=timeout
                )
                
                if not result.success:
                    print(f"âš ï¸  [{url_index+1}] Attempt {attempt + 1} failed: {result.error_message}")
                    continue
                
                if result.cleaned_html and result.cleaned_html.strip():
                    duration = time.time() - start_time
                    print(f"âœ… [{url_index+1}] ThÃ nh cÃ´ng vá»›i primary selector! ({duration:.1f}s)")
                    
                    return CrawlResult(
                        url=url,
                        success=True,
                        method="primary",
                        content=result.markdown or "",
                        html=result.cleaned_html,
                        duration=duration,
                        content_length=len(result.markdown or "")
                    )
                else:
                    print(f"âš ï¸  [{url_index+1}] KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i primary selector")
                    if attempt == max_retries - 1:  # Chá»‰ thá»­ fallback á»Ÿ láº§n cuá»‘i
                        return await crawl_fallback_single(url, config, url_index, start_time)
                    
        except asyncio.TimeoutError:
            print(f"â° [{url_index+1}] Timeout sau {timeout}s (attempt {attempt + 1})")
            if attempt == max_retries - 1:
                return await crawl_fallback_single(url, config, url_index, start_time)
        except Exception as e:
            print(f"âŒ [{url_index+1}] Lá»—i attempt {attempt + 1}: {str(e)}")
            if attempt == max_retries - 1:
                return await crawl_fallback_single(url, config, url_index, start_time)
    
    # Náº¿u táº¥t cáº£ attempts Ä‘á»u tháº¥t báº¡i
    duration = time.time() - start_time
    return CrawlResult(
        url=url,
        success=False,
        method="failed",
        content="",
        html="",
        error="Táº¥t cáº£ attempts Ä‘á»u tháº¥t báº¡i",
        duration=duration
    )


async def crawl_fallback_single(url: str, config: Dict, url_index: int, start_time: float) -> CrawlResult:
    """PhÆ°Æ¡ng phÃ¡p dá»± phÃ²ng cho má»™t URL"""
    print(f"ğŸ”„ [{url_index+1}] Thá»­ cÃ¡c phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng...")
    
    selectors = config.get("selectors", {})
    crawl_conf = config.get("crawl_config", {})
    
    # Thá»­ cÃ¡c selector khÃ¡c nhau theo thá»© tá»±
    fallback_selectors = [
        ("fallback", selectors.get("fallback")),
        ("alternative", selectors.get("alternative")),
        ("primary_without_id", 'div.box-question'),
        ("any_box", '.box, .question, .answer, .solution')
    ]
    
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    for method_name, selector in fallback_selectors:
        if not selector:
            continue
            
        print(f"ğŸ¯ [{url_index+1}] Thá»­ {method_name} selector: {selector}")
        
        fallback_config = CrawlerRunConfig(
            css_selector=selector,
            cache_mode=cache_mode,
            excluded_tags=crawl_conf.get("excluded_tags", []),
            word_count_threshold=crawl_conf.get("word_count_threshold", 5)
        )
        
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=fallback_config)
                
                if result.success and result.markdown:
                    # TÃ¬m kiáº¿m keywords trong ná»™i dung
                    keywords = config.get("keywords", {}).get("target", [])
                    found_section = extract_target_section(result.markdown, keywords)
                    
                    if found_section:
                        duration = time.time() - start_time
                        print(f"âœ… [{url_index+1}] TÃ¬m tháº¥y section HÆ¯á»šNG DáºªN GIáº¢I vá»›i {method_name}! ({duration:.1f}s)")
                        
                        return CrawlResult(
                            url=url,
                            success=True,
                            method=f"fallback_{method_name}",
                            content='\n'.join(found_section),
                            html=f"<div class='huong-dan-giai'>{'\n'.join(found_section)}</div>",
                            duration=duration,
                            content_length=len('\n'.join(found_section))
                        )
                        
        except Exception as e:
            print(f"âš ï¸  [{url_index+1}] Lá»—i vá»›i {method_name}: {str(e)}")
            continue
    
    duration = time.time() - start_time
    return CrawlResult(
        url=url,
        success=False,
        method="fallback_failed",
        content="",
        html="",
        error="KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i báº¥t ká»³ phÆ°Æ¡ng phÃ¡p nÃ o",
        duration=duration
    )


def extract_target_section(content: str, keywords: list) -> Optional[List[str]]:
    """TrÃ­ch xuáº¥t section chá»©a keywords target"""
    lines = content.split('\n')
    found_section = []
    capture = False
    
    for line in lines:
        line_upper = line.upper()
        if any(keyword.upper() in line_upper for keyword in keywords):
            capture = True
            found_section.append(line)
        elif capture:
            if line.strip():
                # Tiáº¿p tá»¥c capture náº¿u khÃ´ng pháº£i header má»›i
                if not (line.startswith('##') and len(found_section) > 10):
                    found_section.append(line)
                else:
                    break  # Káº¿t thÃºc section
            else:
                found_section.append(line)  # Giá»¯ láº¡i dÃ²ng trá»‘ng
    
    return found_section if found_section else None


async def crawl_all_urls_parallel(urls: List[str], config: Dict) -> List[CrawlResult]:
    """
    Crawl táº¥t cáº£ URLs song song vá»›i giá»›i háº¡n concurrent workers
    
    Args:
        urls: Danh sÃ¡ch URLs cáº§n crawl
        config: Dictionary cáº¥u hÃ¬nh tá»« YAML
    
    Returns:
        List[CrawlResult]: Danh sÃ¡ch káº¿t quáº£ cho táº¥t cáº£ URLs
    """
    parallel_conf = config.get("parallel_config", {})
    max_workers = parallel_conf.get("max_concurrent_workers", 3)
    batch_delay = parallel_conf.get("batch_delay", 2)
    show_progress = parallel_conf.get("show_progress", True)
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u crawl {len(urls)} URLs vá»›i {max_workers} workers song song")
    
    # Táº¡o semaphore Ä‘á»ƒ giá»›i háº¡n sá»‘ lÆ°á»£ng concurrent workers
    semaphore = asyncio.Semaphore(max_workers)
    
    async def crawl_with_semaphore(url: str, url_index: int) -> CrawlResult:
        """Wrapper Ä‘á»ƒ sá»­ dá»¥ng semaphore"""
        async with semaphore:
            return await crawl_single_url(url, config, url_index)
    
    # Táº¡o tasks cho táº¥t cáº£ URLs
    start_time = time.time()
    tasks = [
        crawl_with_semaphore(url, i) 
        for i, url in enumerate(urls)
    ]
    
    # Cháº¡y táº¥t cáº£ tasks song song vá»›i progress tracking
    if show_progress:
        print(f"â³ Äang xá»­ lÃ½ {len(tasks)} URLs...")
        
    # Sá»­ dá»¥ng asyncio.gather Ä‘á»ƒ cháº¡y song song
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Xá»­ lÃ½ káº¿t quáº£ vÃ  exceptions
    final_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"âŒ [{i+1}] Exception: {str(result)}")
            final_results.append(CrawlResult(
                url=urls[i],
                success=False,
                method="exception",
                content="",
                html="",
                error=str(result),
                duration=0.0
            ))
        else:
            final_results.append(result)
    
    total_duration = time.time() - start_time
    
    # Thá»‘ng kÃª tá»•ng há»£p
    successful = sum(1 for r in final_results if r.success)
    failed = len(final_results) - successful
    
    print(f"\nğŸ“Š Tá»”NG Káº¾T CRAWLING:")
    print(f"â±ï¸  Tá»•ng thá»i gian: {total_duration:.1f}s")
    print(f"âœ… ThÃ nh cÃ´ng: {successful}/{len(urls)} URLs")
    print(f"âŒ Tháº¥t báº¡i: {failed}/{len(urls)} URLs")
    print(f"âš¡ Tá»‘c Ä‘á»™ trung bÃ¬nh: {len(urls)/total_duration:.1f} URLs/giÃ¢y")
    
    return final_results


async def save_parallel_results(results: List[CrawlResult], config: Dict) -> Dict:
    """LÆ°u káº¿t quáº£ crawl song song"""
    output_conf = config.get("output", {})
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path(output_conf.get("directory", "crawl4ai/output"))
    output_dir.mkdir(exist_ok=True)
    
    # Táº¡o thÆ° má»¥c theo ngÃ y náº¿u Ä‘Æ°á»£c cáº¥u hÃ¬nh
    if output_conf.get("create_date_folder", False):
        date_folder = datetime.now().strftime("%Y-%m-%d")
        output_dir = output_dir / date_folder
        output_dir.mkdir(exist_ok=True)
    
    prefix = output_conf.get("prefix", "huong_dan_giai")
    formats = output_conf.get("formats", ["markdown"])
    include_timestamp = output_conf.get("include_timestamp", True)
    create_summary = output_conf.get("create_summary", True)
    
    timestamp_suffix = f"_{timestamp}" if include_timestamp else ""
    
    summary_data = {
        'timestamp': timestamp,
        'total_urls': len(results),
        'successful_urls': sum(1 for r in results if r.success),
        'failed_urls': sum(1 for r in results if not r.success),
        'results': [],
        'files_created': []
    }
    
    # LÆ°u tá»«ng URL result
    for i, result in enumerate(results):
        if not result.success:
            print(f"âš ï¸  [{i+1}] Bá» qua URL tháº¥t báº¡i: {result.url}")
            summary_data['results'].append({
                'url': result.url,
                'success': False,
                'error': result.error,
                'method': result.method,
                'duration': result.duration
            })
            continue
        
        url_safe_name = f"url_{i+1:02d}"
        
        result_info = {
            'url': result.url,
            'success': True,
            'method': result.method,
            'duration': result.duration,
            'content_length': result.content_length,
            'files': []
        }
        
        # LÆ°u theo format Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
        if "html" in formats:
            html_file = output_dir / f"{prefix}_{url_safe_name}_html{timestamp_suffix}.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(result.html)
            result_info['files'].append(str(html_file))
            summary_data['files_created'].append(str(html_file))
            print(f"ğŸ’¾ [{i+1}] ÄÃ£ lÆ°u HTML: {html_file}")
        
        if "markdown" in formats:
            md_file = output_dir / f"{prefix}_{url_safe_name}_md{timestamp_suffix}.md"
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(f"# URL: {result.url}\n\n")
                f.write(f"Method: {result.method}\n")
                f.write(f"Duration: {result.duration:.1f}s\n\n")
                f.write("---\n\n")
                f.write(result.content)
            result_info['files'].append(str(md_file))
            summary_data['files_created'].append(str(md_file))
            print(f"ğŸ’¾ [{i+1}] ÄÃ£ lÆ°u Markdown: {md_file}")
        
        if "json" in formats:
            json_file = output_dir / f"{prefix}_{url_safe_name}_info{timestamp_suffix}.json"
            info = {
                'url': result.url,
                'method': result.method,
                'duration': result.duration,
                'content_length': result.content_length,
                'timestamp': timestamp,
                'success': True
            }
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
            result_info['files'].append(str(json_file))
            summary_data['files_created'].append(str(json_file))
            print(f"ğŸ’¾ [{i+1}] ÄÃ£ lÆ°u JSON: {json_file}")
        
        summary_data['results'].append(result_info)
    
    # Táº¡o summary file náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if create_summary:
        summary_file = output_dir / f"{prefix}_summary{timestamp_suffix}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ ÄÃ£ táº¡o summary file: {summary_file}")
        summary_data['files_created'].append(str(summary_file))
    
    # Hiá»ƒn thá»‹ preview cá»§a results thÃ nh cÃ´ng
    successful_results = [r for r in results if r.success]
    if successful_results:
        print(f"\nğŸ“– Preview má»™t sá»‘ káº¿t quáº£ thÃ nh cÃ´ng:")
        print("-" * 80)
        for i, result in enumerate(successful_results[:3]):  # Chá»‰ hiá»ƒn thá»‹ 3 Ä‘áº§u tiÃªn
            preview = result.content[:300] + "..." if len(result.content) > 300 else result.content
            print(f"[{i+1}] URL: {result.url}")
            print(f"    Method: {result.method} | Duration: {result.duration:.1f}s")
            print(f"    Content: {preview}")
            print("-" * 80)
    
    return summary_data


def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ CRAWL4AI - CRAWLING SONG SONG HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T")
    print("=" * 80)
    
    # Äá»c cáº¥u hÃ¬nh
    config = load_config()
    if not config:
        print("âŒ KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c file cáº¥u hÃ¬nh!")
        return
    
    # Láº¥y danh sÃ¡ch URLs
    urls = config.get("urls", [])
    if not urls:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y URLs nÃ o trong cáº¥u hÃ¬nh!")
        return
    
    if not isinstance(urls, list):
        print("âŒ URLs pháº£i lÃ  dáº¡ng list trong config YAML!")
        print("ğŸ’¡ Format Ä‘Ãºng:")
        print("urls:")
        print("  - 'https://example1.com'")
        print("  - 'https://example2.com'")
        return
    
    # Hiá»ƒn thá»‹ thÃ´ng tin cáº¥u hÃ¬nh
    parallel_conf = config.get("parallel_config", {})
    output_conf = config.get("output", {})
    
    print(f"ğŸ“‹ Sá»‘ lÆ°á»£ng URLs: {len(urls)}")
    print(f"âš™ï¸  Max concurrent workers: {parallel_conf.get('max_concurrent_workers', 3)}")
    print(f"â° Timeout per URL: {parallel_conf.get('per_url_timeout', 45)}s")
    print(f"ğŸ“ Output directory: {output_conf.get('directory', 'crawl4ai/output')}")
    print(f"ğŸ“„ Output formats: {', '.join(output_conf.get('formats', ['markdown']))}")
    print("=" * 80)
    
    # Hiá»ƒn thá»‹ danh sÃ¡ch URLs
    print("ğŸ”— Danh sÃ¡ch URLs sáº½ Ä‘Æ°á»£c crawl:")
    for i, url in enumerate(urls, 1):
        print(f"  {i:2d}. {url}")
    
    # XÃ¡c nháº­n trÆ°á»›c khi crawl
    try:
        response = input(f"\nâ“ Báº¡n cÃ³ muá»‘n crawl {len(urls)} URLs nÃ y khÃ´ng? (y/N): ").strip().lower()
        if response not in ['y', 'yes']:
            print("ğŸ‘‹ ThoÃ¡t chÆ°Æ¡ng trÃ¬nh")
            return
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ThoÃ¡t chÆ°Æ¡ng trÃ¬nh")
        return
    
    print("=" * 80)
    
    # Cháº¡y crawler song song
    try:
        results = asyncio.run(crawl_all_urls_parallel(urls, config))
        summary = asyncio.run(save_parallel_results(results, config))
        
        print(f"\nâœ… HOÃ€N THÃ€NH!")
        print(f"ğŸ“Š Thá»‘ng kÃª cuá»‘i cÃ¹ng:")
        print(f"   ğŸ¯ URLs thÃ nh cÃ´ng: {summary['successful_urls']}/{summary['total_urls']}")
        print(f"   ğŸ“„ Files Ä‘Ã£ táº¡o: {len(summary['files_created'])}")
        print(f"   ğŸ“‚ ThÆ° má»¥c output: {output_conf.get('directory', 'crawl4ai/output')}")
        
        if summary['successful_urls'] == 0:
            print("\nâš ï¸  KhÃ´ng cÃ³ URL nÃ o thÃ nh cÃ´ng!")
            print("ğŸ’¡ HÃ£y kiá»ƒm tra:")
            print("   - Káº¿t ná»‘i internet")
            print("   - URLs cÃ³ Ä‘Ãºng khÃ´ng")
            print("   - Cáº­p nháº­t selectors trong config YAML")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ÄÃ£ dá»«ng crawling theo yÃªu cáº§u")
    except Exception as e:
        print(f"\nâŒ Lá»—i khÃ´ng mong muá»‘n: {str(e)}")


if __name__ == "__main__":
    main() 