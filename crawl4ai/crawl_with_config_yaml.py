#!/usr/bin/env python3
"""
Script crawl H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT v·ªõi c·∫•u h√¨nh t·ª´ file YAML
S·ª≠ d·ª•ng file config_crawl.yaml ƒë·ªÉ t√πy ch·ªânh c√°c tham s·ªë
"""

import asyncio
import json
import sys
import yaml
from datetime import datetime
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


def load_config(config_file="crawl4ai/config_crawl.yaml"):
    """ƒê·ªçc file c·∫•u h√¨nh YAML"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"‚úÖ ƒê√£ ƒë·ªçc file c·∫•u h√¨nh: {config_file}")
            return config
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh: {config_file}")
        print("üí° H√£y t·∫°o file config_crawl.yaml ho·∫∑c ki·ªÉm tra ƒë∆∞·ªùng d·∫´n")
        return None
    except yaml.YAMLError as e:
        print(f"‚ùå L·ªói ƒë·ªçc file YAML: {e}")
        return None
    except ImportError:
        print("‚ùå Thi·∫øu th∆∞ vi·ªán PyYAML. C√†i ƒë·∫∑t b·∫±ng: pip install pyyaml")
        return None


async def crawl_with_config(url: str, config: dict):
    """
    Crawl v·ªõi c·∫•u h√¨nh t·ª´ file YAML
    
    Args:
        url: URL c·∫ßn crawl
        config: Dictionary c·∫•u h√¨nh t·ª´ YAML
    """
    
    # L·∫•y c·∫•u h√¨nh crawl
    crawl_conf = config.get("crawl_config", {})
    selectors = config.get("selectors", {})
    output_conf = config.get("output", {})
    advanced = config.get("advanced", {})
    
    # T·∫°o CrawlerRunConfig v·ªõi nhi·ªÅu t√πy ch·ªçn h∆°n
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    crawler_config = CrawlerRunConfig(
        css_selector=selectors.get("id_selector", selectors.get("primary")),  # Th·ª≠ id_selector tr∆∞·ªõc
        cache_mode=cache_mode,
        excluded_tags=crawl_conf.get("excluded_tags", []),
        word_count_threshold=crawl_conf.get("word_count_threshold", 3),
        exclude_external_links=crawl_conf.get("exclude_external_links", True),
        exclude_external_images=crawl_conf.get("exclude_external_images", True),
        exclude_social_media_links=crawl_conf.get("exclude_social_media_links", True)
    )
    
    print(f"üîç ƒêang crawl: {url}")
    print(f"üéØ Primary selector: {selectors.get('id_selector', selectors.get('primary'))}")
    
    # Retry mechanism
    max_retries = advanced.get("max_retries", 3)
    delay = advanced.get("delay_between_requests", 1)
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f"üîÑ Th·ª≠ l·∫°i l·∫ßn {attempt + 1}/{max_retries}...")
                await asyncio.sleep(delay)
            
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=crawler_config)
                
                if not result.success:
                    print(f"‚ö†Ô∏è  Attempt {attempt + 1} failed: {result.error_message}")
                    continue
                
                if result.cleaned_html and result.cleaned_html.strip():
                    print("‚úÖ T√¨m th·∫•y n·ªôi dung v·ªõi primary selector!")
                    return await save_results(result, config, "primary")
                else:
                    print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y n·ªôi dung v·ªõi primary selector")
                    if attempt == max_retries - 1:  # Ch·ªâ th·ª≠ fallback ·ªü l·∫ßn cu·ªëi
                        return await crawl_fallback(url, config)
                    
        except Exception as e:
            print(f"‚ùå L·ªói attempt {attempt + 1}: {str(e)}")
            if attempt == max_retries - 1:
                return await crawl_fallback(url, config)
    
    return None


async def crawl_fallback(url: str, config: dict):
    """Ph∆∞∆°ng ph√°p d·ª± phong v·ªõi nhi·ªÅu strategies"""
    print("üîÑ Th·ª≠ c√°c ph∆∞∆°ng ph√°p d·ª± ph√≤ng...")
    
    selectors = config.get("selectors", {})
    crawl_conf = config.get("crawl_config", {})
    
    # Th·ª≠ c√°c selector kh√°c nhau theo th·ª© t·ª±
    fallback_selectors = [
        ("fallback", selectors.get("fallback")),
        ("alternative", selectors.get("alternative")),
        ("primary_without_id", 'div.box-question'),
        ("any_box", '.box, .question, .answer, .solution')
    ]
    
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    for method_name, selector in fallback_selectors:
        if not selector:
            continue
            
        print(f"üéØ Th·ª≠ {method_name} selector: {selector}")
        
        fallback_config = CrawlerRunConfig(
            css_selector=selector,
            cache_mode=cache_mode,
            excluded_tags=crawl_conf.get("excluded_tags", []),
            word_count_threshold=crawl_conf.get("word_count_threshold", 5)
        )
        
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=fallback_config)
                
                if result.success and result.markdown:
                    # T√¨m ki·∫øm keywords trong n·ªôi dung
                    keywords = config.get("keywords", {}).get("target", [])
                    content = result.markdown
                    
                    # T√¨m section ch·ª©a H∆Ø·ªöNG D·∫™N GI·∫¢I
                    found_section = extract_target_section(content, keywords)
                    
                    if found_section:
                        print(f"‚úÖ T√¨m th·∫•y section H∆Ø·ªöNG D·∫™N GI·∫¢I v·ªõi {method_name}!")
                        
                        # T·∫°o result object cho fallback
                        fallback_result = create_fallback_result(found_section, url)
                        return await save_results(fallback_result, config, f"fallback_{method_name}")
                        
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói v·ªõi {method_name}: {str(e)}")
            continue
    
    print("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung v·ªõi b·∫•t k·ª≥ ph∆∞∆°ng ph√°p n√†o")
    return None


def extract_target_section(content: str, keywords: list):
    """Tr√≠ch xu·∫•t section ch·ª©a keywords target"""
    lines = content.split('\n')
    found_section = []
    capture = False
    
    for line in lines:
        line_upper = line.upper()
        if any(keyword.upper() in line_upper for keyword in keywords):
            capture = True
            found_section.append(line)
        elif capture:
            if line.strip():
                # Ti·∫øp t·ª•c capture n·∫øu kh√¥ng ph·∫£i header m·ªõi
                if not (line.startswith('##') and len(found_section) > 10):
                    found_section.append(line)
                else:
                    break  # K·∫øt th√∫c section
            else:
                found_section.append(line)  # Gi·ªØ l·∫°i d√≤ng tr·ªëng
    
    return found_section if found_section else None


def create_fallback_result(content_lines: list, url: str):
    """T·∫°o result object cho fallback method"""
    class FallbackResult:
        def __init__(self, content, url):
            self.markdown = '\n'.join(content)
            self.cleaned_html = f"<div class='huong-dan-giai'>{self.markdown}</div>"
            self.url = url
            self.success = True
    
    return FallbackResult(content_lines, url)


async def save_results(result, config: dict, method: str):
    """L∆∞u k·∫øt qu·∫£ theo c·∫•u h√¨nh YAML"""
    output_conf = config.get("output", {})
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path(output_conf.get("directory", "crawl4ai/output"))
    output_dir.mkdir(exist_ok=True)
    
    # T·∫°o th∆∞ m·ª•c theo ng√†y n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh
    if output_conf.get("create_date_folder", False):
        date_folder = datetime.now().strftime("%Y-%m-%d")
        output_dir = output_dir / date_folder
        output_dir.mkdir(exist_ok=True)
    
    prefix = output_conf.get("prefix", "huong_dan_giai")
    formats = output_conf.get("formats", ["markdown"])
    include_timestamp = output_conf.get("include_timestamp", True)
    
    timestamp_suffix = f"_{timestamp}" if include_timestamp else ""
    
    results = {
        'method': method,
        'timestamp': timestamp,
        'files': [],
        'stats': {
            'content_length': len(result.markdown),
            'html_length': len(result.cleaned_html),
            'url': result.url
        }
    }
    
    # L∆∞u theo format ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if "html" in formats:
        html_file = output_dir / f"{prefix}_html_{method}{timestamp_suffix}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(result.cleaned_html)
        results['files'].append(str(html_file))
        print(f"üíæ ƒê√£ l∆∞u HTML: {html_file}")
    
    if "markdown" in formats:
        md_file = output_dir / f"{prefix}_md_{method}{timestamp_suffix}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(result.markdown)
        results['files'].append(str(md_file))
        print(f"üíæ ƒê√£ l∆∞u Markdown: {md_file}")
    
    if "json" in formats:
        json_file = output_dir / f"{prefix}_info_{method}{timestamp_suffix}.json"
        info = {
            'url': result.url,
            'method': method,
            'timestamp': timestamp,
            'content_length': len(result.markdown),
            'html_length': len(result.cleaned_html),
            'file_paths': results['files']
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        results['files'].append(str(json_file))
        print(f"üíæ ƒê√£ l∆∞u JSON: {json_file}")
    
    # Hi·ªÉn th·ªã preview
    print(f"\nüìñ Preview n·ªôi dung:")
    print("-" * 50)
    preview = result.markdown[:1000] + "..." if len(result.markdown) > 1000 else result.markdown
    print(preview)
    print("-" * 50)
    
    return results


def select_url_interactive(urls: dict):
    """Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn URL interactively"""
    if len(urls) == 1:
        url_key = list(urls.keys())[0]
        return url_key, urls[url_key]
    
    print("üìã C√°c URL c√≥ s·∫µn:")
    url_list = list(urls.items())
    for i, (key, url) in enumerate(url_list, 1):
        print(f"  {i}. {key}")
        print(f"     {url[:80]}...")
    
    while True:
        try:
            choice = input(f"\nüî¢ Ch·ªçn URL (1-{len(url_list)}) ho·∫∑c Enter ƒë·ªÉ d√πng m·∫∑c ƒë·ªãnh: ").strip()
            
            if not choice:  # Default choice
                url_key = url_list[0][0]
                return url_key, urls[url_key]
            
            idx = int(choice) - 1
            if 0 <= idx < len(url_list):
                url_key = url_list[idx][0]
                return url_key, urls[url_key]
            else:
                print(f"‚ùå Vui l√≤ng ch·ªçn s·ªë t·ª´ 1 ƒë·∫øn {len(url_list)}")
                
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")
        except KeyboardInterrupt:
            print("\nüëã Tho√°t ch∆∞∆°ng tr√¨nh")
            sys.exit(0)


def main():
    """H√†m ch√≠nh"""
    print("üöÄ CRAWL4AI - H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT (YAML Config)")
    print("=" * 60)
    
    # ƒê·ªçc c·∫•u h√¨nh
    config = load_config()
    if not config:
        print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c file c·∫•u h√¨nh!")
        return
    
    # Hi·ªÉn th·ªã th√¥ng tin c·∫•u h√¨nh
    print(f"‚öôÔ∏è  Log level: {config.get('advanced', {}).get('log_level', 'INFO')}")
    print(f"üìÅ Output directory: {config.get('output', {}).get('directory', 'crawl4ai/output')}")
    print(f"üìÑ Output formats: {', '.join(config.get('output', {}).get('formats', ['markdown']))}")
    
    # Ch·ªçn URL
    urls = config.get("urls", {})
    if not urls:
        print("‚ùå Kh√¥ng t√¨m th·∫•y URL n√†o trong c·∫•u h√¨nh!")
        return
    
    url_key, url = select_url_interactive(urls)
    
    print(f"\nüîó S·ª≠ d·ª•ng URL: {url_key}")
    print(f"üåê {url}")
    print("=" * 60)
    
    # Ch·∫°y crawler
    result = asyncio.run(crawl_with_config(url, config))
    
    if result:
        print(f"\n‚úÖ TH√ÄNH C√îNG!")
        print(f"üìÅ Ph∆∞∆°ng ph√°p: {result['method']}")
        print(f"üìä Th·ªëng k√™:")
        print(f"   üìù ƒê·ªô d√†i n·ªôi dung: {result['stats']['content_length']} k√Ω t·ª±")
        print(f"   üìÑ S·ªë files t·∫°o: {len(result['files'])}")
        print(f"üìÇ Files ƒë√£ t·∫°o:")
        for file_path in result['files']:
            print(f"   üìÑ {file_path}")
    else:
        print(f"\n‚ùå KH√îNG TH√ÄNH C√îNG!")
        print("üí° H√£y ki·ªÉm tra:")
        print("   - URL c√≥ ƒë√∫ng kh√¥ng")
        print("   - K·∫øt n·ªëi internet")
        print("   - C·∫≠p nh·∫≠t selectors trong config YAML")


if __name__ == "__main__":
    main() 