#!/usr/bin/env python3
"""
Script crawl HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T vá»›i cáº¥u hÃ¬nh tá»« file YAML
Sá»­ dá»¥ng file config_crawl.yaml Ä‘á»ƒ tÃ¹y chá»‰nh cÃ¡c tham sá»‘
"""

import asyncio
import json
import sys
import yaml
from datetime import datetime
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


def load_config(config_file="crawl4ai/config_crawl.yaml"):
    """Äá»c file cáº¥u hÃ¬nh YAML"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"âœ… ÄÃ£ Ä‘á»c file cáº¥u hÃ¬nh: {config_file}")
            return config
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file cáº¥u hÃ¬nh: {config_file}")
        print("ğŸ’¡ HÃ£y táº¡o file config_crawl.yaml hoáº·c kiá»ƒm tra Ä‘Æ°á»ng dáº«n")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ Lá»—i Ä‘á»c file YAML: {e}")
        return None
    except ImportError:
        print("âŒ Thiáº¿u thÆ° viá»‡n PyYAML. CÃ i Ä‘áº·t báº±ng: pip install pyyaml")
        return None


async def crawl_with_config(url: str, config: dict):
    """
    Crawl vá»›i cáº¥u hÃ¬nh tá»« file YAML
    
    Args:
        url: URL cáº§n crawl
        config: Dictionary cáº¥u hÃ¬nh tá»« YAML
    """
    
    # Láº¥y cáº¥u hÃ¬nh crawl
    crawl_conf = config.get("crawl_config", {})
    selectors = config.get("selectors", {})
    output_conf = config.get("output", {})
    advanced = config.get("advanced", {})
    
    # Táº¡o CrawlerRunConfig vá»›i nhiá»u tÃ¹y chá»n hÆ¡n
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    crawler_config = CrawlerRunConfig(
        css_selector=selectors.get("id_selector", selectors.get("primary")),  # Thá»­ id_selector trÆ°á»›c
        cache_mode=cache_mode,
        excluded_tags=crawl_conf.get("excluded_tags", []),
        word_count_threshold=crawl_conf.get("word_count_threshold", 3),
        exclude_external_links=crawl_conf.get("exclude_external_links", True),
        exclude_external_images=crawl_conf.get("exclude_external_images", True),
        exclude_social_media_links=crawl_conf.get("exclude_social_media_links", True)
    )
    
    print(f"ğŸ” Äang crawl: {url}")
    print(f"ğŸ¯ Primary selector: {selectors.get('id_selector', selectors.get('primary'))}")
    
    # Retry mechanism
    max_retries = advanced.get("max_retries", 3)
    delay = advanced.get("delay_between_requests", 1)
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f"ğŸ”„ Thá»­ láº¡i láº§n {attempt + 1}/{max_retries}...")
                await asyncio.sleep(delay)
            
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=crawler_config)
                
                if not result.success:
                    print(f"âš ï¸  Attempt {attempt + 1} failed: {result.error_message}")
                    continue
                
                if result.cleaned_html and result.cleaned_html.strip():
                    print("âœ… TÃ¬m tháº¥y ná»™i dung vá»›i primary selector!")
                    return await save_results(result, config, "primary")
                else:
                    print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i primary selector")
                    if attempt == max_retries - 1:  # Chá»‰ thá»­ fallback á»Ÿ láº§n cuá»‘i
                        return await crawl_fallback(url, config)
                    
        except Exception as e:
            print(f"âŒ Lá»—i attempt {attempt + 1}: {str(e)}")
            if attempt == max_retries - 1:
                return await crawl_fallback(url, config)
    
    return None


async def crawl_fallback(url: str, config: dict):
    """PhÆ°Æ¡ng phÃ¡p dá»± phong vá»›i nhiá»u strategies"""
    print("ğŸ”„ Thá»­ cÃ¡c phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng...")
    
    selectors = config.get("selectors", {})
    crawl_conf = config.get("crawl_config", {})
    
    # Thá»­ cÃ¡c selector khÃ¡c nhau theo thá»© tá»±
    fallback_selectors = [
        ("fallback", selectors.get("fallback")),
        ("alternative", selectors.get("alternative")),
        ("primary_without_id", 'div.box-question'),
        ("any_box", '.box, .question, .answer, .solution')
    ]
    
    cache_mode = getattr(CacheMode, crawl_conf.get("cache_mode", "BYPASS"))
    
    for method_name, selector in fallback_selectors:
        if not selector:
            continue
            
        print(f"ğŸ¯ Thá»­ {method_name} selector: {selector}")
        
        fallback_config = CrawlerRunConfig(
            css_selector=selector,
            cache_mode=cache_mode,
            excluded_tags=crawl_conf.get("excluded_tags", []),
            word_count_threshold=crawl_conf.get("word_count_threshold", 5)
        )
        
        try:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url, config=fallback_config)
                
                if result.success and result.markdown:
                    # TÃ¬m kiáº¿m keywords trong ná»™i dung
                    keywords = config.get("keywords", {}).get("target", [])
                    content = result.markdown
                    
                    # TÃ¬m section chá»©a HÆ¯á»šNG DáºªN GIáº¢I
                    found_section = extract_target_section(content, keywords)
                    
                    if found_section:
                        print(f"âœ… TÃ¬m tháº¥y section HÆ¯á»šNG DáºªN GIáº¢I vá»›i {method_name}!")
                        
                        # Táº¡o result object cho fallback
                        fallback_result = create_fallback_result(found_section, url)
                        return await save_results(fallback_result, config, f"fallback_{method_name}")
                        
        except Exception as e:
            print(f"âš ï¸  Lá»—i vá»›i {method_name}: {str(e)}")
            continue
    
    print("âŒ KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i báº¥t ká»³ phÆ°Æ¡ng phÃ¡p nÃ o")
    return None


def extract_target_section(content: str, keywords: list):
    """TrÃ­ch xuáº¥t section chá»©a keywords target"""
    lines = content.split('\n')
    found_section = []
    capture = False
    
    for line in lines:
        line_upper = line.upper()
        if any(keyword.upper() in line_upper for keyword in keywords):
            capture = True
            found_section.append(line)
        elif capture:
            if line.strip():
                # Tiáº¿p tá»¥c capture náº¿u khÃ´ng pháº£i header má»›i
                if not (line.startswith('##') and len(found_section) > 10):
                    found_section.append(line)
                else:
                    break  # Káº¿t thÃºc section
            else:
                found_section.append(line)  # Giá»¯ láº¡i dÃ²ng trá»‘ng
    
    return found_section if found_section else None


def create_fallback_result(content_lines: list, url: str):
    """Táº¡o result object cho fallback method"""
    class FallbackResult:
        def __init__(self, content, url):
            self.markdown = '\n'.join(content)
            self.cleaned_html = f"<div class='huong-dan-giai'>{self.markdown}</div>"
            self.url = url
            self.success = True
    
    return FallbackResult(content_lines, url)


async def save_results(result, config: dict, method: str):
    """LÆ°u káº¿t quáº£ theo cáº¥u hÃ¬nh YAML"""
    output_conf = config.get("output", {})
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_dir = Path(output_conf.get("directory", "crawl4ai/output"))
    output_dir.mkdir(exist_ok=True)
    
    # Táº¡o thÆ° má»¥c theo ngÃ y náº¿u Ä‘Æ°á»£c cáº¥u hÃ¬nh
    if output_conf.get("create_date_folder", False):
        date_folder = datetime.now().strftime("%Y-%m-%d")
        output_dir = output_dir / date_folder
        output_dir.mkdir(exist_ok=True)
    
    prefix = output_conf.get("prefix", "huong_dan_giai")
    formats = output_conf.get("formats", ["markdown"])
    include_timestamp = output_conf.get("include_timestamp", True)
    
    timestamp_suffix = f"_{timestamp}" if include_timestamp else ""
    
    results = {
        'method': method,
        'timestamp': timestamp,
        'files': [],
        'stats': {
            'content_length': len(result.markdown),
            'html_length': len(result.cleaned_html),
            'url': result.url
        }
    }
    
    # LÆ°u theo format Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
    if "html" in formats:
        html_file = output_dir / f"{prefix}_html_{method}{timestamp_suffix}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(result.cleaned_html)
        results['files'].append(str(html_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u HTML: {html_file}")
    
    if "markdown" in formats:
        md_file = output_dir / f"{prefix}_md_{method}{timestamp_suffix}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(result.markdown)
        results['files'].append(str(md_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u Markdown: {md_file}")
    
    if "json" in formats:
        json_file = output_dir / f"{prefix}_info_{method}{timestamp_suffix}.json"
        info = {
            'url': result.url,
            'method': method,
            'timestamp': timestamp,
            'content_length': len(result.markdown),
            'html_length': len(result.cleaned_html),
            'file_paths': results['files']
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        results['files'].append(str(json_file))
        print(f"ğŸ’¾ ÄÃ£ lÆ°u JSON: {json_file}")
    
    # Hiá»ƒn thá»‹ preview
    print(f"\nğŸ“– Preview ná»™i dung:")
    print("-" * 50)
    preview = result.markdown[:1000] + "..." if len(result.markdown) > 1000 else result.markdown
    print(preview)
    print("-" * 50)
    
    return results


def select_url_interactive(urls: dict):
    """Cho phÃ©p ngÆ°á»i dÃ¹ng chá»n URL interactively"""
    if len(urls) == 1:
        url_key = list(urls.keys())[0]
        return url_key, urls[url_key]
    
    print("ğŸ“‹ CÃ¡c URL cÃ³ sáºµn:")
    url_list = list(urls.items())
    for i, (key, url) in enumerate(url_list, 1):
        print(f"  {i}. {key}")
        print(f"     {url[:80]}...")
    
    while True:
        try:
            choice = input(f"\nğŸ”¢ Chá»n URL (1-{len(url_list)}) hoáº·c Enter Ä‘á»ƒ dÃ¹ng máº·c Ä‘á»‹nh: ").strip()
            
            if not choice:  # Default choice
                url_key = url_list[0][0]
                return url_key, urls[url_key]
            
            idx = int(choice) - 1
            if 0 <= idx < len(url_list):
                url_key = url_list[idx][0]
                return url_key, urls[url_key]
            else:
                print(f"âŒ Vui lÃ²ng chá»n sá»‘ tá»« 1 Ä‘áº¿n {len(url_list)}")
                
        except ValueError:
            print("âŒ Vui lÃ²ng nháº­p sá»‘ há»£p lá»‡")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ThoÃ¡t chÆ°Æ¡ng trÃ¬nh")
            sys.exit(0)


def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ CRAWL4AI - HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T (YAML Config)")
    print("=" * 60)
    
    # Äá»c cáº¥u hÃ¬nh
    config = load_config()
    if not config:
        print("âŒ KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c file cáº¥u hÃ¬nh!")
        return
    
    # Hiá»ƒn thá»‹ thÃ´ng tin cáº¥u hÃ¬nh
    print(f"âš™ï¸  Log level: {config.get('advanced', {}).get('log_level', 'INFO')}")
    print(f"ğŸ“ Output directory: {config.get('output', {}).get('directory', 'crawl4ai/output')}")
    print(f"ğŸ“„ Output formats: {', '.join(config.get('output', {}).get('formats', ['markdown']))}")
    
    # Chá»n URL
    urls = config.get("urls", {})
    if not urls:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y URL nÃ o trong cáº¥u hÃ¬nh!")
        return
    
    url_key, url = select_url_interactive(urls)
    
    print(f"\nğŸ”— Sá»­ dá»¥ng URL: {url_key}")
    print(f"ğŸŒ {url}")
    print("=" * 60)
    
    # Cháº¡y crawler
    result = asyncio.run(crawl_with_config(url, config))
    
    if result:
        print(f"\nâœ… THÃ€NH CÃ”NG!")
        print(f"ğŸ“ PhÆ°Æ¡ng phÃ¡p: {result['method']}")
        print(f"ğŸ“Š Thá»‘ng kÃª:")
        print(f"   ğŸ“ Äá»™ dÃ i ná»™i dung: {result['stats']['content_length']} kÃ½ tá»±")
        print(f"   ğŸ“„ Sá»‘ files táº¡o: {len(result['files'])}")
        print(f"ğŸ“‚ Files Ä‘Ã£ táº¡o:")
        for file_path in result['files']:
            print(f"   ğŸ“„ {file_path}")
    else:
        print(f"\nâŒ KHÃ”NG THÃ€NH CÃ”NG!")
        print("ğŸ’¡ HÃ£y kiá»ƒm tra:")
        print("   - URL cÃ³ Ä‘Ãºng khÃ´ng")
        print("   - Káº¿t ná»‘i internet")
        print("   - Cáº­p nháº­t selectors trong config YAML")


if __name__ == "__main__":
    main() 