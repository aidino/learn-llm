#!/usr/bin/env python3
"""
Script Ä‘Æ¡n giáº£n Ä‘á»ƒ crawl pháº§n HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T tá»« loigiaihay.com
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


async def crawl_huong_dan_giai_simple(url: str):
    """
    Crawl Ä‘Æ¡n giáº£n pháº§n HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T
    
    Args:
        url: URL trang cáº§n crawl
        
    Returns:
        str: Ná»™i dung text Ä‘Ã£ crawl Ä‘Æ°á»£c
    """
    
    # Cáº¥u hÃ¬nh crawl vá»›i CSS selector cho pháº§n tá»­ cÃ³ ID sub-question-2
    config = CrawlerRunConfig(
        css_selector='div.box-question[id="sub-question-2"]',  # Selector cá»¥ thá»ƒ
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["script", "style", "nav", "footer", "header"],
        word_count_threshold=3,
        exclude_external_links=True,
        exclude_external_images=True
    )
    
    print(f"ğŸ” Äang crawl: {url}")
    print("ğŸ¯ TÃ¬m kiáº¿m pháº§n HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T...")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        
        if not result.success:
            print(f"âŒ Lá»—i: {result.error_message}")
            return None
        
        # Kiá»ƒm tra náº¿u cÃ³ ná»™i dung
        if result.cleaned_html:
            print("âœ… TÃ¬m tháº¥y ná»™i dung!")
            
            # LÆ°u káº¿t quáº£
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # LÆ°u HTML
            html_file = Path(f"crawl4ai/output/huong_dan_giai_html_{timestamp}.html")
            html_file.parent.mkdir(exist_ok=True)
            
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(result.cleaned_html)
            
            # LÆ°u Markdown
            md_file = Path(f"crawl4ai/output/huong_dan_giai_md_{timestamp}.md")
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(result.markdown)
            
            print(f"ğŸ’¾ ÄÃ£ lÆ°u HTML: {html_file}")
            print(f"ğŸ’¾ ÄÃ£ lÆ°u Markdown: {md_file}")
            
            # Hiá»ƒn thá»‹ má»™t pháº§n ná»™i dung
            print("\nğŸ“– Ná»™i dung Ä‘Ã£ crawl (Markdown):")
            print("-" * 50)
            print(result.markdown[:1000] + "..." if len(result.markdown) > 1000 else result.markdown)
            print("-" * 50)
            
            return {
                'html': result.cleaned_html,
                'markdown': result.markdown,
                'url': result.url,
                'timestamp': timestamp
            }
        else:
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá»›i selector nÃ y")
            return await crawl_fallback_simple(url)


async def crawl_fallback_simple(url: str):
    """
    PhÆ°Æ¡ng phÃ¡p dá»± phÃ²ng - crawl táº¥t cáº£ cÃ¡c box-question
    """
    print("ğŸ”„ Thá»­ phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng...")
    
    config = CrawlerRunConfig(
        css_selector='.box-question',  # Táº¥t cáº£ box-question
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["script", "style", "nav", "footer"],
        word_count_threshold=5
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        
        if result.success and result.markdown:
            # TÃ¬m pháº§n cÃ³ chá»©a "HÆ¯á»šNG DáºªN GIáº¢I"
            lines = result.markdown.split('\n')
            huong_dan_section = []
            capture = False
            
            for line in lines:
                if 'HÆ¯á»šNG DáºªN GIáº¢I' in line.upper():
                    capture = True
                    huong_dan_section.append(line)
                elif capture:
                    if line.strip() and not line.startswith('#'):
                        huong_dan_section.append(line)
                    elif line.startswith('##') and capture:
                        break  # Káº¿t thÃºc section nÃ y
            
            if huong_dan_section:
                content = '\n'.join(huong_dan_section)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = Path(f"crawl4ai/output/huong_dan_fallback_{timestamp}.md")
                output_file.parent.mkdir(exist_ok=True)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"âœ… TÃ¬m tháº¥y section HÆ¯á»šNG DáºªN GIáº¢I!")
                print(f"ğŸ’¾ ÄÃ£ lÆ°u: {output_file}")
                print(f"\nğŸ“– Ná»™i dung:")
                print("-" * 50)
                print(content[:800] + "..." if len(content) > 800 else content)
                print("-" * 50)
                
                return {
                    'content': content,
                    'method': 'fallback',
                    'timestamp': timestamp
                }
        
        print("âŒ KhÃ´ng tÃ¬m tháº¥y ná»™i dung HÆ¯á»šNG DáºªN GIáº¢I")
        return None


def main():
    """HÃ m chÃ­nh"""
    # URL máº·c Ä‘á»‹nh
    url = "https://loigiaihay.com/de-thi-vao-lop-6-mon-toan-truong-cau-giay-nam-2023-a142098.html"
    
    print("ğŸš€ CRAWL4AI - HÆ¯á»šNG DáºªN GIáº¢I CHI TIáº¾T")
    print("=" * 60)
    print(f"URL: {url}")
    print("=" * 60)
    
    # Cháº¡y crawler
    result = asyncio.run(crawl_huong_dan_giai_simple(url))
    
    if result:
        print("\nâœ… THÃ€NH CÃ”NG!")
        print("ğŸ“ Kiá»ƒm tra thÆ° má»¥c crawl4ai/output/ Ä‘á»ƒ xem káº¿t quáº£")
    else:
        print("\nâŒ KHÃ”NG THÃ€NH CÃ”NG!")
        print("ğŸ’¡ HÃ£y kiá»ƒm tra URL hoáº·c cáº¥u trÃºc trang web")


if __name__ == "__main__":
    main() 