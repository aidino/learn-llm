#!/usr/bin/env python3
"""
Demo script ƒë·ªÉ test parallel crawling v·ªõi 2-3 URLs
"""

import asyncio
import sys
from pathlib import Path

# Import script ch√≠nh
try:
    from crawl_parallel_yaml import load_config, crawl_all_urls_parallel, save_parallel_results
except ImportError:
    print("‚ùå Kh√¥ng th·ªÉ import script ch√≠nh. H√£y ƒë·∫£m b·∫£o file crawl_parallel_yaml.py t·ªìn t·∫°i")
    sys.exit(1)


def create_demo_config():
    """T·∫°o config demo v·ªõi 2-3 URLs ƒë·ªÉ test"""
    return {
        "urls": [
            "https://loigiaihay.com/de-thi-vao-lop-6-mon-toan-truong-cau-giay-nam-2023-a142098.html",
            "https://loigiaihay.com/de-thi-vao-lop-6-mon-toan-truong-vinschool-nam-2023-a142088.html"
        ],
        "parallel_config": {
            "max_concurrent_workers": 2,
            "per_url_timeout": 30,
            "batch_delay": 1,
            "continue_on_error": True,
            "show_progress": True
        },
        "selectors": {
            "primary": 'div.box-question[id="sub-question-2"]',
            "fallback": ".box-question",
            "alternative": ".solution-box, [class*='question'], [class*='answer'], [class*='solution']",
            "id_selector": "#sub-question-2"
        },
        "crawl_config": {
            "excluded_tags": ["script", "style", "nav", "footer", "header", "aside", "noscript"],
            "word_count_threshold": 3,
            "exclude_external_links": True,
            "exclude_external_images": True,
            "exclude_social_media_links": True,
            "cache_mode": "BYPASS",
            "timeout": 30,
            "user_agent": "Mozilla/5.0 (compatible; Crawl4AI/0.6.3; Educational Purpose)"
        },
        "output": {
            "directory": "crawl4ai/output",
            "prefix": "demo_parallel",
            "formats": ["html", "markdown", "json"],
            "include_timestamp": True,
            "create_date_folder": False,
            "create_summary": True
        },
        "keywords": {
            "target": [
                "H∆Ø·ªöNG D·∫™N GI·∫¢I CHI TI·∫æT",
                "h∆∞·ªõng d·∫´n gi·∫£i",
                "gi·∫£i chi ti·∫øt",
                "b√†i gi·∫£i",
                "c√°ch gi·∫£i",
                "l·ªùi gi·∫£i"
            ],
            "exclude": [
                "qu·∫£ng c√°o",
                "advertisement",
                "sidebar",
                "footer",
                "navigation",
                "menu",
                "banner",
                "popup"
            ]
        },
        "advanced": {
            "max_retries": 2,
            "delay_between_requests": 1,
            "use_proxy": False,
            "proxy_settings": {
                "http": "",
                "https": ""
            },
            "log_level": "INFO",
            "save_raw_html": False,
            "verify_ssl": True
        }
    }


async def demo_parallel_crawl():
    """Demo ch·∫°y parallel crawling"""
    print("üß™ DEMO PARALLEL CRAWLING")
    print("=" * 50)
    
    # S·ª≠ d·ª•ng config demo
    config = create_demo_config()
    urls = config["urls"]
    
    print(f"üìã Demo v·ªõi {len(urls)} URLs:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url[:80]}...")
    
    print(f"\n‚öôÔ∏è  Config:")
    print(f"   Max workers: {config['parallel_config']['max_concurrent_workers']}")
    print(f"   Timeout per URL: {config['parallel_config']['per_url_timeout']}s")
    print(f"   Output formats: {', '.join(config['output']['formats'])}")
    
    print("\n" + "=" * 50)
    
    try:
        # Ch·∫°y parallel crawling
        results = await crawl_all_urls_parallel(urls, config)
        
        # L∆∞u k·∫øt qu·∫£
        summary = await save_parallel_results(results, config)
        
        print(f"\n‚úÖ DEMO HO√ÄN TH√ÄNH!")
        print(f"üìä K·∫øt qu·∫£:")
        print(f"   üéØ URLs th√†nh c√¥ng: {summary['successful_urls']}/{summary['total_urls']}")
        print(f"   üìÑ Files ƒë√£ t·∫°o: {len(summary['files_created'])}")
        
        # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt
        for i, result in enumerate(summary['results']):
            status = "‚úÖ" if result['success'] else "‚ùå"
            if result['success']:
                print(f"   {status} URL {i+1}: {result['method']} ({result['duration']:.1f}s, {result['content_length']} chars)")
            else:
                print(f"   {status} URL {i+1}: {result.get('error', 'Unknown error')}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå L·ªói trong demo: {str(e)}")
        return False


def main():
    """H√†m ch√≠nh"""
    print("üöÄ Demo Script - Parallel Crawling Test")
    
    # Ki·ªÉm tra dependencies
    try:
        import yaml
        import crawl4ai
        print("‚úÖ Dependencies OK")
    except ImportError as e:
        print(f"‚ùå Missing dependency: {e}")
        print("üí° C√†i ƒë·∫∑t: pip install pyyaml crawl4ai")
        return
    
    # T·∫°o output directory n·∫øu ch∆∞a c√≥
    output_dir = Path("crawl4ai/output")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Ch·∫°y demo
    try:
        success = asyncio.run(demo_parallel_crawl())
        if success:
            print("\nüéâ Demo th√†nh c√¥ng! Ki·ªÉm tra th∆∞ m·ª•c crawl4ai/output ƒë·ªÉ xem k·∫øt qu·∫£")
        else:
            print("\nüòû Demo kh√¥ng th√†nh c√¥ng")
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo ƒë√£ b·ªã d·ª´ng")
    except Exception as e:
        print(f"\nüí• L·ªói kh√¥ng mong mu·ªën: {str(e)}")


if __name__ == "__main__":
    main() 