import os
import asyncio
import json
from pydantic import BaseModel, Field
from typing import List, Optional
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai import LLMExtractionStrategy
from dotenv import load_dotenv

_=load_dotenv()

# URL_FILE = 'dethi_lop6_toan.depth2.url.txt'
URL_FILE = 'sample_link.txt'


def write_output_to_file(filepath, content):
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(content, f, ensure_ascii=False, indent=4)

class Exam(BaseModel):
    question: str = Field(..., description="C√¢u h·ªèi ch√≠nh c·ªßa b√†i thi, th∆∞·ªùng b·∫Øt ƒë·∫ßu b·∫±ng 'C√¢u ...'. C√¢u h·ªèi c√≥ th·ªÉ ch·ª©a c√°c c√¥ng th·ª©c latex")
    image_url: Optional[str] = Field(None, description="URL h√¨nh ·∫£nh minh h·ªça, trong tr∆∞·ªùng h·ª£p c√¢u h·ªèi c·∫ßn h√¨nh v·∫Ω ƒë·ªÉ m√¥ t·∫£")
    solution: str = Field(..., description="H∆∞·ªõng d·∫´n gi·∫£i, trong h∆∞·ªõng d·∫´n c√≥ th·ªÉ c√≥ c√°c c√¥ng th·ª©c latex")
    result: Optional[str] = Field(None, description="ƒê√°p √°n c·ªßa nh·ªØng c√¢u h·ªèi tr·∫Øc nghi·ªám")

async def main():
    try:
        with open(URL_FILE, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        if not urls:
            print(f"L·ªói: File {URL_FILE} tr·ªëng ho·∫∑c kh√¥ng t·ªìn t·∫°i.")
            return
        print(f"üîé T√¨m th·∫•y {len(urls)} link trong file {URL_FILE}.")
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{URL_FILE}'. Vui l√≤ng t·∫°o file n√†y v√† th√™m c√°c link v√†o.")
        return
    # 1. Define the LLM extraction strategy
    # gemini_token = os.getenv("GEMINI_API_KEY")
    
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openrouter/qwen/qwen3-8b:free", api_token=os.getenv("OPEN_ROUTER")),
        schema=Exam.model_json_schema(), # Or use model_json_schema()
        extraction_type="schema",
        instruction="""
        B·∫°n l√† m·ªôt chuy√™n gia tr√≠ch xu·∫•t d·ªØ li·ªáu web. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc n·ªôi dung c·ªßa m·ªôt ƒë·ªÅ thi ƒë∆∞·ª£c cung c·∫•p v√† tr√≠ch xu·∫•t TO√ÄN B·ªò c√°c c√¢u h·ªèi c√≥ trong ƒë√≥.
        - H√£y tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c c√¢u h·ªèi, b·∫Øt ƒë·∫ßu t·ª´ 'C√¢u 1' cho ƒë·∫øn c√¢u cu·ªëi c√πng.
        - V·ªõi m·ªói c√¢u h·ªèi, l·∫•y ƒë·∫ßy ƒë·ªß n·ªôi dung c√¢u h·ªèi, h√¨nh ·∫£nh minh h·ªça (n·∫øu c√≥), l·ªùi gi·∫£i chi ti·∫øt v√† ƒë√°p √°n cu·ªëi c√πng.
        - C√°c c√¥ng th·ª©c to√°n h·ªçc trong c√¢u h·ªèi v√† l·ªùi gi·∫£i PH·∫¢I ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng b·∫±ng LaTeX.
        - B·ªè qua t·∫•t c·∫£ c√°c n·ªôi dung kh√¥ng ph·∫£i l√† c√¢u h·ªèi nh∆∞: l·ªùi gi·ªõi thi·ªáu ƒë·∫ßu trang, c√°c b√¨nh lu·∫≠n, qu·∫£ng c√°o, ho·∫∑c c√°c link li√™n quan ·ªü cu·ªëi trang.
        - ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra theo ƒë√∫ng c·∫•u tr√∫c JSON schema ƒë√£ ƒë∆∞·ª£c cung c·∫•p.
        """,
        chunk_token_threshold=1000,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="fit_markdown",   # or "html", "fit_markdown"
    )

    # 2. Build the crawler config
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        css_selector="#sub-question-2",
        cache_mode=CacheMode.BYPASS
    )

    # 3. Create a browser config if needed
    browser_cfg = BrowserConfig(headless=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # 4. Let's say we want to crawl a single page
        results = []
        file_index=1
        for index, url in enumerate(urls):
            result = await crawler.arun(
                url=url,
                config=crawl_config,
            )

            if result.success:
                # 5. The extracted content is presumably JSON
                data = json.loads(result.extracted_content)
                results.extend(data)
                # print(f"ƒë√£ th√™m {len(data)} b√†i, t·ªïng s·ªë hi·ªán nay l√†: {len(results)}")
                # print("Extracted items:", data)
                # write_output_to_file('extracted.json', data)
                
                # 6. Show usage stats
                # llm_strategy.show_usage()  # prints token usage
            else:
                print("Error:", result.error_message)
            
            print(f"crawed {index+1}/{len(urls)}")
            if (index+1)%5 == 0:
                write_output_to_file(f"crawled/openrouter_crawled_p{file_index}.json", results)
                results=[]
                file_index +=1
            
            
            
        write_output_to_file('crawled/openrouter_crawled_last.json', results)

if __name__ == "__main__":
    asyncio.run(main())
