#!/usr/bin/env python3
"""
Script Ä‘á»ƒ gá»™p cÃ¡c file crawled_*.json vÃ  loáº¡i bá» cÃ¢u há»i trÃ¹ng láº·p
"""

import json
import glob
import os
from typing import List, Dict, Set
import hashlib

def load_json_file(file_path: str) -> List[Dict]:
    """Äá»c file JSON vÃ  tráº£ vá» dá»¯ liá»‡u"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… ÄÃ£ Ä‘á»c {len(data)} cÃ¢u há»i tá»« {os.path.basename(file_path)}")
        return data
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {file_path}: {e}")
        return []

def create_question_hash(question: str) -> str:
    """Táº¡o hash cho cÃ¢u há»i Ä‘á»ƒ so sÃ¡nh trÃ¹ng láº·p"""
    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a vÃ  táº¡o hash
    cleaned_question = " ".join(question.split())
    return hashlib.md5(cleaned_question.encode('utf-8')).hexdigest()

def remove_duplicates(data: List[Dict]) -> List[Dict]:
    """Loáº¡i bá» cÃ¢u há»i trÃ¹ng láº·p dá»±a trÃªn ná»™i dung cÃ¢u há»i"""
    seen_questions: Set[str] = set()
    unique_data: List[Dict] = []
    duplicates_count = 0
    
    for item in data:
        question = item.get('question', '')
        question_hash = create_question_hash(question)
        
        if question_hash not in seen_questions:
            seen_questions.add(question_hash)
            unique_data.append(item)
        else:
            duplicates_count += 1
    
    print(f"ğŸ” ÄÃ£ loáº¡i bá» {duplicates_count} cÃ¢u há»i trÃ¹ng láº·p")
    return unique_data

def merge_crawled_files():
    """Gá»™p táº¥t cáº£ cÃ¡c file crawled_*.json"""
    # TÃ¬m táº¥t cáº£ cÃ¡c file crawled_*.json
    file_pattern = "crawled_*.json"
    files = glob.glob(file_pattern)
    
    if not files:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file nÃ o vá»›i pattern: {file_pattern}")
        return
    
    print(f"ğŸ“ TÃ¬m tháº¥y {len(files)} file crawled_*.json")
    
    # Äá»c vÃ  gá»™p dá»¯ liá»‡u tá»« táº¥t cáº£ cÃ¡c file
    all_data: List[Dict] = []
    total_questions = 0
    
    for file_path in sorted(files):
        data = load_json_file(file_path)
        all_data.extend(data)
        total_questions += len(data)
    
    print(f"ğŸ“Š Tá»•ng sá»‘ cÃ¢u há»i gá»‘c: {total_questions}")
    
    # Loáº¡i bá» trÃ¹ng láº·p
    print("ğŸ”„ Äang loáº¡i bá» cÃ¢u há»i trÃ¹ng láº·p...")
    unique_data = remove_duplicates(all_data)
    
    print(f"âœ¨ Sá»‘ cÃ¢u há»i sau khi loáº¡i bá» trÃ¹ng láº·p: {len(unique_data)}")
    
    # LÆ°u káº¿t quáº£
    output_file = "merged_questions.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o file: {output_file}")
        
        # Táº¡o file thá»‘ng kÃª
        stats = {
            "source_files": len(files),
            "total_original_questions": total_questions,
            "unique_questions": len(unique_data),
            "duplicates_removed": total_questions - len(unique_data),
            "file_list": [os.path.basename(f) for f in sorted(files)]
        }
        
        stats_file = "merge_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ˆ ÄÃ£ lÆ°u thá»‘ng kÃª vÃ o file: {stats_file}")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file: {e}")

def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ Báº¯t Ä‘áº§u gá»™p cÃ¡c file crawled_*.json")
    print("=" * 50)
    
    # Kiá»ƒm tra thÆ° má»¥c hiá»‡n táº¡i
    current_dir = os.getcwd()
    print(f"ğŸ“‚ ThÆ° má»¥c lÃ m viá»‡c: {current_dir}")
    
    merge_crawled_files()
    
    print("=" * 50)
    print("âœ… HoÃ n thÃ nh!")

if __name__ == "__main__":
    main() 