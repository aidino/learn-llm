#!/usr/bin/env python3
"""
Fine-tuning QLora model Gemma multimodal v·ªõi dataset to√°n h·ªçc ti·∫øng Vi·ªát
Phi√™n b·∫£n c·∫£i ti·∫øn v·ªõi x·ª≠ l√Ω l·ªói tensor conversion
T·ªëi ∆∞u cho RTX 3070 8GB VRAM
"""

# Import Unsloth FIRST ƒë·ªÉ tr√°nh warning
from unsloth import FastLanguageModel, is_bfloat16_supported

import os
import torch
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
from PIL import Image
import base64
from io import BytesIO

# Core libraries
from datasets import load_dataset, Dataset
from transformers import TrainingArguments, TextStreamer, DataCollatorForSeq2Seq
from trl import SFTTrainer
import torch.nn.functional as F

# Opik imports for tracking  
import opik
from opik import track, Opik

# Environment setup
warnings.filterwarnings("ignore")
torch.backends.cudnn.benchmark = True

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Configuration
class Config:
    # Model configuration - Gemma 3 4B multimodal t·ªëi ∆∞u cho RTX 3070 8GB
    MODEL_NAME = "unsloth/gemma-3-4b-pt-unsloth-bnb-4bit"  # Multimodal Gemma 3 4B
    
    # Fallback models n·∫øu model ch√≠nh kh√¥ng ho·∫°t ƒë·ªông
    MODEL_CANDIDATES = [
        "unsloth/gemma-3-4b-pt-unsloth-bnb-4bit",  # Multimodal Gemma 3 4B (target)
        "unsloth/gemma-2b-it",                      # Fallback 1
        "unsloth/llama-3.2-1b-it",                  # Fallback 2 (smallest)
    ]
    
    # Extreme memory optimization cho 4B model
    MAX_SEQ_LENGTH = 512    # Gi·∫£m m·∫°nh cho 4B model
    LOAD_IN_4BIT = True
    DTYPE = None  # Auto-detect
    
    # LoRA configuration - t·ªëi ∆∞u cho model l·ªõn
    LORA_R = 4              # Gi·∫£m rank xu·ªëng minimum
    LORA_ALPHA = 8          # Gi·∫£m alpha t∆∞∆°ng ·ª©ng  
    LORA_DROPOUT = 0.05     # Gi·∫£m dropout
    TARGET_MODULES = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ]
    
    # Training configuration - extreme memory optimization cho 4B
    PER_DEVICE_TRAIN_BATCH_SIZE = 1
    GRADIENT_ACCUMULATION_STEPS = 8  # TƒÉng ƒë·ªÉ b√π batch size nh·ªè
    WARMUP_STEPS = 3
    MAX_STEPS = 20          # Gi·∫£m cho testing v·ªõi model l·ªõn
    LEARNING_RATE = 1e-4    # Gi·∫£m learning rate cho model l·ªõn
    LOGGING_STEPS = 1
    OPTIM = "adamw_8bit"
    WEIGHT_DECAY = 0.01
    LR_SCHEDULER_TYPE = "linear"
    SEED = 3407
    
    # Dataset configuration
    DATASET_NAME = "ngohongthai/dethivaolop6-montoan"
    
    # Output paths
    OUTPUT_DIR = "./outputs_multimodal"
    MODEL_SAVE_PATH = "./gemma_multimodal_finetuned"
    
    # Opik configuration
    OPIK_PROJECT_NAME = "gemma-multimodal-vietnamese-math"


class MultimodalDataProcessor:
    """X·ª≠ l√Ω dataset multimodal v·ªõi error handling t·ªët h∆°n"""
    
    def __init__(self):
        self.tokenizer = None
    
    def set_tokenizer(self, tokenizer):
        """Set tokenizer - compatible with both regular tokenizer and multimodal processor"""
        self.tokenizer = tokenizer
        
        # Handle Gemma3Processor (multimodal) vs regular tokenizer
        if hasattr(tokenizer, 'tokenizer'):
            # Multimodal processor - get the underlying tokenizer
            actual_tokenizer = tokenizer.tokenizer
            print("üîç Detected multimodal processor, using underlying tokenizer")
        else:
            # Regular tokenizer
            actual_tokenizer = tokenizer
            print("üîç Using regular tokenizer")
        
        # Ensure proper special tokens for the actual tokenizer
        try:
            if hasattr(actual_tokenizer, 'pad_token') and actual_tokenizer.pad_token is None:
                actual_tokenizer.pad_token = actual_tokenizer.eos_token
                print("‚úÖ Set pad_token")
                
            if hasattr(actual_tokenizer, 'unk_token') and actual_tokenizer.unk_token is None:
                actual_tokenizer.unk_token = actual_tokenizer.eos_token
                print("‚úÖ Set unk_token")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not set special tokens: {e}")
            print("üí° Continuing without special token setup")
    
    def process_image_safely(self, image_data) -> Optional[str]:
        """X·ª≠ l√Ω image data an to√†n"""
        if image_data is None:
            return None
        
        try:
            if isinstance(image_data, dict) and 'bytes' in image_data:
                image = Image.open(BytesIO(image_data['bytes']))
            elif isinstance(image_data, Image.Image):
                image = image_data
            else:
                return None
            
            # Resize ƒë·ªÉ ti·∫øt ki·ªám memory
            if image.size[0] > 256 or image.size[1] > 256:
                image.thumbnail((256, 256), Image.Resampling.LANCZOS)
            
            # Convert to base64
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            return f"[IMAGE:{img_str[:100]}...]"  # Truncate ƒë·ªÉ ti·∫øt ki·ªám memory
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω image: {e}")
            return None
    
    def format_single_example(self, text: str, solution: str = "", result: str = "", image_data=None) -> str:
        """Format m·ªôt example th√†nh text format ƒë∆°n gi·∫£n"""
        
        # T·∫°o user content
        user_content = str(text).strip() if text else "Gi·∫£i b√†i to√°n n√†y."
        if image_data:
            image_desc = self.process_image_safely(image_data)
            if image_desc:
                user_content = f"{image_desc}\n{user_content}"
        
        # T·∫°o assistant response
        solution_str = str(solution).strip() if solution else ""
        result_str = str(result).strip() if result else ""
        
        if solution_str and result_str and solution_str != result_str:
            assistant_content = f"**Gi·∫£i:**\n{solution_str}\n\n**ƒê√°p √°n:** {result_str}"
        elif result_str:
            assistant_content = f"**ƒê√°p √°n:** {result_str}"
        elif solution_str:
            assistant_content = solution_str
        else:
            assistant_content = "C·∫ßn th√™m th√¥ng tin ƒë·ªÉ gi·∫£i b√†i n√†y."
        
        # Format theo Gemma format
        formatted_text = f"<start_of_turn>user\n{user_content}<end_of_turn>\n<start_of_turn>model\n{assistant_content}<end_of_turn>"
        
        return formatted_text
    
    def formatting_prompts_func(self, examples):
        """Simplified formatting for SFTTrainer compatibility"""
        
        try:
            # Handle batch processing
            if isinstance(examples, dict) and any(isinstance(v, list) for v in examples.values()):
                # Batch format
                texts = []
                
                # Get field data safely
                text_list = examples.get('text', examples.get('question', examples.get('problem', [])))
                solution_list = examples.get('Solution', examples.get('solution', []))
                result_list = examples.get('Result', examples.get('result', []))
                
                if not text_list:
                    print("‚ö†Ô∏è No text data found, creating dummy")
                    return {"text": ["B√†i to√°n m·∫´u: 2+2=?"]}
                
                # Process each example
                for i in range(len(text_list)):
                    try:
                        # Get values safely
                        question = str(text_list[i]) if i < len(text_list) else "B√†i to√°n m·∫´u"
                        solution = str(solution_list[i]) if i < len(solution_list) and solution_list[i] else ""
                        result = str(result_list[i]) if i < len(result_list) and result_list[i] else ""
                        
                        # Create simple format
                        if solution and result and solution != result:
                            answer = f"Gi·∫£i: {solution}\nƒê√°p √°n: {result}"
                        elif result:
                            answer = f"ƒê√°p √°n: {result}"
                        elif solution:
                            answer = solution
                        else:
                            answer = "C·∫ßn th√™m th√¥ng tin"
                        
                        # Simple Gemma format
                        formatted = f"<start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n{answer}<end_of_turn>"
                        
                        # Ensure it's a simple string
                        texts.append(str(formatted))
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error formatting item {i}: {e}")
                        texts.append("<start_of_turn>user\nSample question<end_of_turn>\n<start_of_turn>model\nSample answer<end_of_turn>")
                
                print(f"‚úÖ Formatted {len(texts)} examples")
                
                # Ensure we return proper structure
                result = {"text": texts}
                return result
                
            else:
                # Single example
                print("‚ö†Ô∏è Single example format")
                question = str(examples.get('text', 'Sample question'))
                answer = str(examples.get('Result', 'Sample answer'))
                formatted = f"<start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n{answer}<end_of_turn>"
                return {"text": [str(formatted)]}
                
        except Exception as e:
            print(f"‚ùå Critical formatting error: {e}")
            # Return minimal valid data
            return {"text": ["<start_of_turn>user\nTest question<end_of_turn>\n<start_of_turn>model\nTest answer<end_of_turn>"]}


def clear_gpu_memory():
    """Clear GPU memory cache - enhanced for large models"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        torch.cuda.ipc_collect()  # Additional cleanup for multiprocessing
        import gc
        gc.collect()  # Python garbage collection


def print_gpu_memory():
    """Print current GPU memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        free = total - allocated
        
        print(f"üî• GPU: {torch.cuda.get_device_name()}")
        print(f"üíæ VRAM Total: {total:.1f}GB")
        print(f"üìä VRAM Status: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {free:.1f}GB free")
        
        # Warning thresholds for 4B model
        if free < 3.0:
            print("üî¥ VRAM r·∫•t th·∫•p! Model 4B c√≥ th·ªÉ OOM")
            print("üí° ƒê·ªÅ xu·∫•t: gi·∫£m MAX_SEQ_LENGTH xu·ªëng 256 ho·∫∑c s·ª≠ d·ª•ng model nh·ªè h∆°n")
        elif free < 4.0:
            print("üü° VRAM h∆°i th·∫•p cho model 4B")
            print("üí° N√™n monitor memory usage trong qu√° tr√¨nh training")
        else:
            print(f"‚úÖ C√≥ {free:.1f}GB VRAM - ƒë·ªß cho Gemma 3 4B training")


def load_model_and_tokenizer(config: Config):
    """Load model v√† tokenizer v·ªõi fallback mechanism"""
    print("üîÑ ƒêang load model v√† tokenizer...")
    
    # Try each model candidate
    for i, model_name in enumerate(config.MODEL_CANDIDATES):
        try:
            print(f"üîÑ Th·ª≠ model {i+1}/{len(config.MODEL_CANDIDATES)}: {model_name}")
            
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_name,
                max_seq_length=config.MAX_SEQ_LENGTH,
                dtype=config.DTYPE,
                load_in_4bit=config.LOAD_IN_4BIT,
            )
            
            print(f"‚úÖ ƒê√£ load model th√†nh c√¥ng: {model_name}")
            print(f"üìä Max sequence length: {config.MAX_SEQ_LENGTH}")
            
            # Update config with working model name
            config.MODEL_NAME = model_name
            
            return model, tokenizer
            
        except Exception as e:
            print(f"‚ùå L·ªói load model {model_name}: {e}")
            if i < len(config.MODEL_CANDIDATES) - 1:
                print(f"üîÑ Th·ª≠ model ti·∫øp theo...")
                continue
            else:
                print("‚ùå T·∫•t c·∫£ models ƒë·ªÅu failed!")
                
                # Try one more fallback with a very small model
                print("üîÑ Th·ª≠ model backup cu·ªëi c√πng...")
                try:
                    # Use the smallest available model for RTX 3070
                    fallback_model = "microsoft/DialoGPT-small"
                    print(f"üîÑ Th·ª≠ fallback model: {fallback_model}")
                    
                    from transformers import AutoModelForCausalLM, AutoTokenizer
                    model = AutoModelForCausalLM.from_pretrained(
                        fallback_model,
                        load_in_4bit=True,
                        device_map="auto"
                    )
                    tokenizer = AutoTokenizer.from_pretrained(fallback_model)
                    
                    if tokenizer.pad_token is None:
                        tokenizer.pad_token = tokenizer.eos_token
                    
                    print(f"‚úÖ Fallback model loaded: {fallback_model}")
                    config.MODEL_NAME = fallback_model
                    
                    return model, tokenizer
                    
                except Exception as fallback_error:
                    print(f"‚ùå Fallback model c≈©ng failed: {fallback_error}")
                    raise RuntimeError(f"Kh√¥ng th·ªÉ load b·∫•t k·ª≥ model n√†o. L·ªói cu·ªëi c√πng: {e}")
    
    raise RuntimeError("Unexpected error in model loading")


def setup_lora(model, config: Config):
    """Setup LoRA - compatible with both Unsloth and regular models"""
    print("üîÑ ƒêang setup LoRA...")
    
    try:
        # Try Unsloth LoRA first
        if hasattr(FastLanguageModel, 'get_peft_model'):
            model = FastLanguageModel.get_peft_model(
                model,
                r=config.LORA_R,
                target_modules=config.TARGET_MODULES,
                lora_alpha=config.LORA_ALPHA,
                lora_dropout=config.LORA_DROPOUT,
                bias="none",
                use_gradient_checkpointing="unsloth",
                random_state=config.SEED,
                use_rslora=False,
                loftq_config=None,
            )
            print("‚úÖ Unsloth LoRA setup ho√†n t·∫•t")
        else:
            raise Exception("Unsloth LoRA not available")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Unsloth LoRA failed ({e}), th·ª≠ PEFT standard...")
        
        # Fallback to standard PEFT
        try:
            from peft import LoraConfig, get_peft_model, TaskType
            
            # Adjust target modules based on model type
            target_modules = config.TARGET_MODULES
            if "DialoGPT" in config.MODEL_NAME:
                target_modules = ["c_attn", "c_proj"]  # DialoGPT specific
            
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=config.LORA_R,
                lora_alpha=config.LORA_ALPHA,
                lora_dropout=config.LORA_DROPOUT,
                target_modules=target_modules,
                bias="none"
            )
            
            model = get_peft_model(model, peft_config)
            print("‚úÖ Standard PEFT LoRA setup ho√†n t·∫•t")
            
        except Exception as peft_error:
            print(f"‚ùå PEFT c≈©ng failed: {peft_error}")
            print("‚ö†Ô∏è Ti·∫øp t·ª•c kh√¥ng c√≥ LoRA - full fine-tuning")
            return model
    
    # Print trainable parameters
    try:
        if hasattr(model, 'print_trainable_parameters'):
            model.print_trainable_parameters()
        else:
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in model.parameters())
            print(f"üìä Trainable params: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ hi·ªÉn th·ªã th·ªëng k√™ parameters: {e}")
    
    return model


def load_and_process_dataset(config: Config, data_processor):
    """Load v√† process dataset - t·ªëi ∆∞u cho model 4B"""
    print("üîÑ ƒêang load dataset...")
    
    try:
        # Load dataset - gi·∫£m samples cho model l·ªõn h∆°n
        train_dataset = load_dataset(
            config.DATASET_NAME, 
            split="train[:50]",   # Gi·∫£m xu·ªëng 50 samples cho 4B model
            trust_remote_code=True
        )
        
        eval_dataset = load_dataset(
            config.DATASET_NAME,
            split="train[50:55]",  # 5 samples cho eval
            trust_remote_code=True
        )
        
        print(f"üìä Train samples: {len(train_dataset)}")
        print(f"üìä Eval samples: {len(eval_dataset)}")
        
        # Process datasets with debugging
        print("üîç Processing train dataset...")
        train_dataset = train_dataset.map(
            data_processor.formatting_prompts_func,
            batched=True,
            batch_size=5,   # Smaller batch for debugging
            remove_columns=train_dataset.column_names,
            desc="Formatting train"
        )
        
        print("üîç Processing eval dataset...")
        eval_dataset = eval_dataset.map(
            data_processor.formatting_prompts_func,
            batched=True,
            batch_size=5,
            remove_columns=eval_dataset.column_names,
            desc="Formatting eval"
        )
        
        # Debug: check dataset structure
        print(f"üîç Train dataset sample keys: {train_dataset[0].keys() if len(train_dataset) > 0 else 'Empty'}")
        if len(train_dataset) > 0 and 'text' in train_dataset[0]:
            sample_text = train_dataset[0]['text']
            print(f"üîç Sample text type: {type(sample_text)}")
            print(f"üîç Sample text length: {len(sample_text) if isinstance(sample_text, str) else 'Not string'}")
            print(f"üîç Sample text preview: {sample_text[:100] if isinstance(sample_text, str) else str(sample_text)[:100]}...")
        
        # Filter out empty or invalid samples
        def filter_valid_samples(example):
            return (
                'text' in example and 
                isinstance(example['text'], str) and 
                len(example['text']) > 0 and
                len(example['text']) < 2000  # Max length check
            )
        
        train_dataset = train_dataset.filter(filter_valid_samples)
        eval_dataset = eval_dataset.filter(filter_valid_samples)
        
        print(f"üìä After filtering - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}")
        
        print("‚úÖ Dataset processing ho√†n t·∫•t")
        
        return train_dataset, eval_dataset
        
    except Exception as e:
        print(f"‚ùå L·ªói load dataset: {e}")
        print("üîÑ T·∫°o dummy dataset...")
        return create_dummy_dataset(data_processor)


def create_dummy_dataset(data_processor):
    """T·∫°o dummy dataset cho testing - t·ªëi ∆∞u cho model 4B"""
    dummy_data = [
        {
            "text": "T√≠nh di·ªán t√≠ch h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 8cm, chi·ªÅu r·ªông 5cm.",
            "Solution": "Di·ªán t√≠ch = chi·ªÅu d√†i √ó chi·ªÅu r·ªông = 8 √ó 5 = 40cm¬≤",
            "Result": "40cm¬≤",
            "image": None
        },
        {
            "text": "T√¨m x: 2x + 5 = 13",
            "Solution": "2x = 13 - 5 = 8\nx = 4",
            "Result": "x = 4", 
            "image": None
        },
        {
            "text": "Tam gi√°c c√≥ c·∫°nh 3, 4, 5cm. ƒê√¢y c√≥ ph·∫£i tam gi√°c vu√¥ng?",
            "Solution": "Ki·ªÉm tra: 3¬≤ + 4¬≤ = 9 + 16 = 25 = 5¬≤\nV·∫≠y ƒë√¢y l√† tam gi√°c vu√¥ng",
            "Result": "C√≥",
            "image": None
        },
        {
            "text": "T√≠nh chu vi h√¨nh tr√≤n c√≥ b√°n k√≠nh 7cm.",
            "Solution": "Chu vi = 2œÄr = 2 √ó 3.14 √ó 7 = 43.96cm",
            "Result": "43.96cm",
            "image": None
        },
        {
            "text": "Gi·∫£i ph∆∞∆°ng tr√¨nh: 3x - 2 = 10",
            "Solution": "3x = 10 + 2 = 12\nx = 12/3 = 4",
            "Result": "x = 4",
            "image": None
        }
    ] * 10  # 50 samples - gi·∫£m t·ª´ 90
    
    dataset = Dataset.from_list(dummy_data)
    
    train_dataset = dataset.map(
        data_processor.formatting_prompts_func,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    eval_dataset = Dataset.from_list(dummy_data[:5]).map(  # Gi·∫£m t·ª´ 6 xu·ªëng 5
        data_processor.formatting_prompts_func,
        batched=True,
        remove_columns=["text", "Solution", "Result", "image"]
    )
    
    print("‚úÖ Dummy dataset created")
    return train_dataset, eval_dataset


def setup_trainer(model, tokenizer, train_dataset, eval_dataset, config: Config):
    """Setup simplified trainer for Gemma 3 4B"""
    print("‚öôÔ∏è ƒêang setup simplified trainer...")
    
    # Handle tokenizer for SFTTrainer
    if hasattr(tokenizer, 'tokenizer'):
        # Multimodal processor - use underlying tokenizer
        actual_tokenizer = tokenizer.tokenizer
        print("üîç Using underlying tokenizer for SFT")
    else:
        actual_tokenizer = tokenizer
        print("üîç Using regular tokenizer for SFT")
    
    # Ensure tokenizer has required attributes
    if not hasattr(actual_tokenizer, 'pad_token_id') or actual_tokenizer.pad_token_id is None:
        actual_tokenizer.pad_token_id = actual_tokenizer.eos_token_id
        
    if not hasattr(actual_tokenizer, 'pad_token') or actual_tokenizer.pad_token is None:
        actual_tokenizer.pad_token = actual_tokenizer.eos_token
    
    # Training arguments v·ªõi extreme memory optimization cho Gemma 3 4B
    training_args = TrainingArguments(
        per_device_train_batch_size=config.PER_DEVICE_TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=config.WARMUP_STEPS,
        max_steps=config.MAX_STEPS,
        learning_rate=config.LEARNING_RATE,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=config.LOGGING_STEPS,
        optim=config.OPTIM,
        weight_decay=config.WEIGHT_DECAY,
        lr_scheduler_type=config.LR_SCHEDULER_TYPE,
        seed=config.SEED,
        output_dir=config.OUTPUT_DIR,
        report_to=None,
        save_steps=15,
        save_total_limit=1,
        dataloader_num_workers=0,
        
        # Extreme memory optimizations
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        dataloader_drop_last=True,
        max_grad_norm=1.0,
        
        # Simplify evaluation
        eval_strategy="no",  # Disable evaluation to save memory
        
        # Advanced optimizations
        tf32=True if torch.cuda.is_available() else False,
        auto_find_batch_size=False,
        group_by_length=False,
        
        # Logging optimizations
        logging_first_step=True,
        save_safetensors=True,
        load_best_model_at_end=False,
        remove_unused_columns=True,  # Let SFT handle columns
    )
    
    # Create SFTTrainer with minimal configuration
    try:
        trainer = SFTTrainer(
            model=model,
            tokenizer=actual_tokenizer,
            train_dataset=train_dataset,
            dataset_text_field="text",
            max_seq_length=config.MAX_SEQ_LENGTH,
            args=training_args,
            
            # Simplified settings to avoid tensor issues
            packing=False,                    # Disable packing 
            dataset_num_proc=1,              # Single process
            data_collator=None,              # Let SFT create default
            formatting_func=None,            # Use dataset text field directly
        )
        
        print("‚úÖ SFTTrainer setup th√†nh c√¥ng")
        return trainer
        
    except Exception as e:
        print(f"‚ùå SFTTrainer setup failed: {e}")
        print("üîÑ Th·ª≠ v·ªõi c·∫•u h√¨nh ƒë∆°n gi·∫£n h∆°n...")
        
        # Fallback: even simpler trainer
        trainer = SFTTrainer(
            model=model,
            tokenizer=actual_tokenizer,
            train_dataset=train_dataset,
            dataset_text_field="text",
            max_seq_length=config.MAX_SEQ_LENGTH,
            args=training_args,
            packing=False,
        )
        
        print("‚úÖ Fallback trainer setup")
        return trainer


@track(project_name=Config.OPIK_PROJECT_NAME)
def train_model(trainer):
    """Train model"""
    print("üöÄ B·∫Øt ƒë·∫ßu extreme memory optimized training...")
    
    # Clear memory before training
    clear_gpu_memory()
    print("üóëÔ∏è ƒê√£ clear memory cache")
    
    # Check GPU memory
    print_gpu_memory()
    
    try:
        trainer_stats = trainer.train()
        print("‚úÖ Training ho√†n t·∫•t!")
        return trainer_stats
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"‚ùå CUDA OOM Error: {e}")
        print("üí° Th·ª≠ gi·∫£m batch size ho·∫∑c sequence length")
        raise
    except Exception as e:
        print(f"‚ùå L·ªói training: {e}")
        raise


def test_inference(model, tokenizer):
    """Test inference - compatible with multimodal processor"""
    print("üß™ Testing inference...")
    
    # Try to enable Unsloth fast inference
    try:
        if hasattr(FastLanguageModel, 'for_inference'):
            FastLanguageModel.for_inference(model)
            print("‚úÖ Unsloth fast inference enabled")
    except Exception as e:
        print(f"‚ö†Ô∏è Unsloth fast inference not available: {e}")
    
    # Prepare test prompt
    test_prompt = "<start_of_turn>user\nT√≠nh di·ªán t√≠ch h√¨nh vu√¥ng c√≥ c·∫°nh 6cm<end_of_turn>\n<start_of_turn>model\n"
    
    try:
        # Handle multimodal processor vs regular tokenizer
        if hasattr(tokenizer, 'tokenizer'):
            # Multimodal processor - use underlying tokenizer for text-only
            actual_tokenizer = tokenizer.tokenizer
            inputs = actual_tokenizer([test_prompt], return_tensors="pt")
            decode_func = actual_tokenizer.decode
            eos_token_id = actual_tokenizer.eos_token_id
            pad_token_id = getattr(actual_tokenizer, 'pad_token_id', eos_token_id)
        else:
            # Regular tokenizer
            inputs = tokenizer([test_prompt], return_tensors="pt")
            decode_func = tokenizer.decode
            eos_token_id = tokenizer.eos_token_id
            pad_token_id = getattr(tokenizer, 'pad_token_id', eos_token_id)
        
        # Move to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=64,
                temperature=0.3,
                do_sample=True,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                use_cache=True
            )
        
        response = decode_func(outputs[0], skip_special_tokens=True)
        
        # Extract only the generated part
        if "<start_of_turn>model\n" in response:
            response = response.split("<start_of_turn>model\n")[-1].strip()
        else:
            # Fallback: remove the input prompt
            response = response[len(test_prompt):].strip()
        
        print("ü§ñ Model response:")
        print(response)
        
    except Exception as e:
        print(f"‚ùå L·ªói inference: {e}")
        print("üí° C√≥ th·ªÉ do multimodal processor - c·∫ßn text-only input format")


def save_model(model, tokenizer, config: Config):
    """Save model - compatible with both Unsloth and regular models"""
    print("üíæ ƒêang save model...")
    
    try:
        # Try Unsloth save methods first
        if hasattr(model, 'save_pretrained_merged') and 'unsloth' in config.MODEL_NAME.lower():
            try:
                # Save LoRA weights
                model.save_pretrained(config.MODEL_SAVE_PATH)
                tokenizer.save_pretrained(config.MODEL_SAVE_PATH)
                print(f"‚úÖ LoRA weights saved t·∫°i: {config.MODEL_SAVE_PATH}")
                
                # Try to save merged model
                merged_path = f"{config.MODEL_SAVE_PATH}_merged"
                model.save_pretrained_merged(merged_path, tokenizer, save_method="merged_16bit")
                print(f"‚úÖ Merged model saved t·∫°i: {merged_path}")
                
            except Exception as merge_error:
                print(f"‚ö†Ô∏è Merge save failed: {merge_error}, ch·ªâ save LoRA weights")
                model.save_pretrained(config.MODEL_SAVE_PATH)
                tokenizer.save_pretrained(config.MODEL_SAVE_PATH)
        else:
            # Standard save for regular models
            model.save_pretrained(config.MODEL_SAVE_PATH)
            tokenizer.save_pretrained(config.MODEL_SAVE_PATH)
            print(f"‚úÖ Model saved (standard method)")
        
        print(f"‚úÖ Model ƒë√£ ƒë∆∞·ª£c save t·∫°i: {config.MODEL_SAVE_PATH}")
        
        # Save config for reference
        import json
        config_dict = {
            "model_name": config.MODEL_NAME,
            "max_seq_length": config.MAX_SEQ_LENGTH,
            "lora_r": config.LORA_R,
            "lora_alpha": config.LORA_ALPHA,
            "training_steps": config.MAX_STEPS
        }
        
        with open(f"{config.MODEL_SAVE_PATH}/training_config.json", "w") as f:
            json.dump(config_dict, f, indent=2)
        print("‚úÖ Training config saved")
        
    except Exception as e:
        print(f"‚ùå L·ªói save model: {e}")
        print("üí° C√≥ th·ªÉ c·∫ßn check disk space ho·∫∑c permissions")


@track(project_name=Config.OPIK_PROJECT_NAME)  
def main():
    """Main function"""
    print("üöÄ B·∫Øt ƒë·∫ßu Gemma Multimodal QLora Fine-tuning")
    print("=" * 60)
    
    # Setup Opik
    try:
        opik.configure()
        print("‚úÖ Opik configured")
    except Exception as e:
        print(f"‚ö†Ô∏è Opik setup warning: {e}")
    
    config = Config()
    
    try:
        # Load model
        model, tokenizer = load_model_and_tokenizer(config)
        
        # Setup data processor
        data_processor = MultimodalDataProcessor()
        data_processor.set_tokenizer(tokenizer)
        
        # Setup LoRA
        model = setup_lora(model, config)
        
        # Load dataset
        train_dataset, eval_dataset = load_and_process_dataset(config, data_processor)
        
        # Setup trainer (simplified, no eval to save memory)
        trainer = setup_trainer(model, tokenizer, train_dataset, None, config)
        
        # Train
        trainer_stats = train_model(trainer)
        
        # Test inference
        test_inference(model, tokenizer)
        
        # Save model
        save_model(model, tokenizer, config)
        
        print("üéâ Pipeline ho√†n t·∫•t th√†nh c√¥ng!")
        
        return {
            "status": "success",
            "training_stats": trainer_stats,
            "model_path": config.MODEL_SAVE_PATH
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "failed", "error": str(e)}


if __name__ == "__main__":
    result = main()
    
    if result["status"] == "success":
        print("\nüéØ TH√ÄNH C√îNG!")
        print(f"üìÅ Model: {result['model_path']}")
    else:
        print(f"\n‚ùå TH·∫§T B·∫†I: {result['error']}") 