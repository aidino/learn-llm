# Fine-tuning Gemma3N vá»›i Unsloth

Dá»± Ã¡n nÃ y cung cáº¥p code hoÃ n chá»‰nh Ä‘á»ƒ fine-tuning model Gemma3N (má»™t vision-language model) sá»­ dá»¥ng thÆ° viá»‡n Unsloth.

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
gemma3n/
â”œâ”€â”€ requirements.md          # YÃªu cáº§u chi tiáº¿t tá»« user
â”œâ”€â”€ gemma3n_finetuning.ipynb # Jupyter notebook chÃ­nh
â”œâ”€â”€ demo_script.py           # Python script demo nhanh
â””â”€â”€ README.md               # HÆ°á»›ng dáº«n nÃ y
```

## ğŸš€ Báº¯t Ä‘áº§u nhanh

### 1. CÃ i Ä‘áº·t

```bash
# CÃ i Ä‘áº·t Unsloth
pip install unsloth

# CÃ i Ä‘áº·t cÃ¡c dependencies cáº§n thiáº¿t
pip install torch torchvision torchaudio
pip install transformers datasets pillow
```

### 2. Cháº¡y demo nhanh

```bash
python demo_script.py
```

### 3. Sá»­ dá»¥ng Jupyter notebook

```bash
jupyter notebook gemma3n_finetuning.ipynb
```

## ğŸ“‹ CÃ¡c tÃ­nh nÄƒng chÃ­nh

### âœ… Model Loading
- Sá»­ dá»¥ng `FastVisionModel` tá»« Unsloth
- Load model "unsloth/gemma-3n-E4B"  
- 4-bit quantization Ä‘á»ƒ tiáº¿t kiá»‡m memory
- Gradient checkpointing optimization

### âœ… PEFT Configuration
- Fine-tuning cáº£ vision vÃ  language layers
- LoRA vá»›i r=32, alpha=32
- Target modules = "all-linear"
- Modules to save: ["lm_head", "embed_tokens"]

### âœ… Dataset Support
- Support instruction vá»›i text + nhiá»u images
- Support answer vá»›i text + nhiá»u images  
- Chat format vá»›i system/user/assistant roles
- Sample dataset Ä‘Æ°á»£c táº¡o sáºµn cho demo

### âœ… Training Configuration
- SFTTrainer tá»« TRL library
- Memory-optimized settings
- AdamW 8-bit optimizer
- Mixed precision training (fp16/bf16)

### âœ… Inference & Deployment  
- Fast inference mode
- Model saving & loading
- Merged model export cho deployment

## ğŸ“Š YÃªu cáº§u há»‡ thá»‘ng

- **GPU**: Ãt nháº¥t 16GB VRAM (khuyáº¿n nghá»‹ RTX 4090 hoáº·c A100)
- **RAM**: Ãt nháº¥t 32GB
- **Storage**: 50GB+ cho model vÃ  checkpoints
- **CUDA**: Version 11.8+ hoáº·c 12.x

## ğŸ”§ Cáº¥u hÃ¬nh dataset thá»±c táº¿

Äá»ƒ sá»­ dá»¥ng vá»›i dataset thá»±c táº¿, hÃ£y chuáº©n bá»‹ data theo format:

```python
{
    "messages": [
        {
            "role": "system",
            "content": [{"type": "text", "text": "System prompt"}]
        },
        {
            "role": "user", 
            "content": [
                {"type": "text", "text": "Your instruction"},
                {"type": "image", "image": PIL_Image_object},
                {"type": "image", "image": PIL_Image_object},  # Nhiá»u images
                {"type": "text", "text": "Additional text"}
            ]
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": "Response text"},
                {"type": "image", "image": PIL_Image_object}  # Optional response image
            ]
        }
    ]
}
```

## âš™ï¸ TÃ¹y chá»‰nh hyperparameters

### Training Parameters
```python
training_args = TrainingArguments(
    per_device_train_batch_size=1,     # Äiá»u chá»‰nh theo GPU memory
    gradient_accumulation_steps=4,      # Effective batch size = 4
    max_steps=1000,                    # TÄƒng cho training thá»±c táº¿
    learning_rate=2e-5,                # Äiá»u chá»‰nh theo dataset
    warmup_steps=100,                  # 10% cá»§a max_steps
    save_steps=250,                    # Save checkpoint má»—i 250 steps
)
```

### PEFT Parameters
```python
model = FastVisionModel.get_peft_model(
    model,
    r=32,                              # Rank: cao hÆ¡n = accuracy tá»‘t hÆ¡n, risk overfit
    lora_alpha=32,                     # Khuyáº¿n nghá»‹ = r
    finetune_vision_layers=True,       # Fine-tune vision encoder
    finetune_language_layers=True,     # Fine-tune language model
)
```

## ğŸš¨ Troubleshooting

### Out of Memory (OOM)
- Giáº£m `per_device_train_batch_size` xuá»‘ng 1
- Giáº£m `max_seq_length` xuá»‘ng 1024  
- TÄƒng `gradient_accumulation_steps`
- Sá»­ dá»¥ng `load_in_4bit=True`

### Slow Training
- Báº­t `use_gradient_checkpointing="unsloth"`
- Sá»­ dá»¥ng `optim="adamw_8bit"`
- Táº¯t `packing=False` cho vision model
- Set `dataloader_pin_memory=False`

### Model Loading Issues
- Kiá»ƒm tra internet connection
- Sá»­ dá»¥ng HuggingFace token náº¿u cáº§n
- Thá»­ download model trÆ°á»›c: `huggingface-cli download unsloth/gemma-3n-E4B`

## ğŸ“ˆ Monitoring Training

### Using TensorBoard
```bash
pip install tensorboard
tensorboard --logdir ./gemma3n_finetuned/logs
```

### Using Weights & Biases
```python
import wandb
wandb.init(project="gemma3n-finetuning")

training_args = TrainingArguments(
    ...
    report_to="wandb",
    run_name="gemma3n-experiment-1"
)
```

## ğŸ¯ Use Cases

Gemma3N fine-tuned model cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho:

- **Multimodal Question Answering**: Tráº£ lá»i cÃ¢u há»i vá» nhiá»u images
- **Image Captioning**: Táº¡o mÃ´ táº£ cho images  
- **Visual Instruction Following**: Thá»±c hiá»‡n instructions dá»±a trÃªn images
- **Document Understanding**: PhÃ¢n tÃ­ch documents vá»›i text + images
- **Educational Content**: Táº¡o content giÃ¡o dá»¥c vá»›i visual aids

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [Gemma Model Card](https://huggingface.co/google/gemma-3n-e4b)
- [TRL Library](https://github.com/huggingface/trl)
- [PEFT Documentation](https://github.com/huggingface/peft)

## ğŸ¤ ÄÃ³ng gÃ³p

Má»i contributions, issues vÃ  feature requests Ä‘á»u Ä‘Æ°á»£c welcome!

## ğŸ“„ License

Project nÃ y sá»­ dá»¥ng MIT License.

---

**ChÃºc báº¡n fine-tuning thÃ nh cÃ´ng! ğŸ‰**