# Gemma3N Math Tutor Fine-tuning

Tá»± Ä‘á»™ng fine-tune model Gemma3N Ä‘á»ƒ giáº£i cÃ¡c bÃ i toÃ¡n ToÃ¡n lá»›p 6 vá»›i há»— trá»£ Ä‘a phÆ°Æ¡ng thá»©c (text + image).

## ğŸ¯ Má»¥c tiÃªu

Fine-tune model [unsloth/gemma-3n-E4B](https://huggingface.co/unsloth/gemma-3n-E4B) Ä‘á»ƒ:
- Giáº£i cÃ¡c bÃ i toÃ¡n ToÃ¡n lá»›p 6
- Xá»­ lÃ½ cáº£ text vÃ  hÃ¬nh áº£nh minh há»a
- Táº¡o ra cÃ¡c lá»i giáº£i chi tiáº¿t vÃ  chÃ­nh xÃ¡c

## ğŸ“Š Dataset

Sá»­ dá»¥ng dataset: [ngohongthai/exam-sixth_grade-instruct-dataset](https://huggingface.co/datasets/ngohongthai/exam-sixth_grade-instruct-dataset)

**Äáº·c Ä‘iá»ƒm dataset:**
- 1,123 máº«u dá»¯ liá»‡u (1,010 train + 113 test)
- 2 cá»™t: `question` vÃ  `solution`
- Chá»©a hÃ¬nh áº£nh minh há»a trong format markdown
- Bao gá»“m cÃ¡c dáº¡ng toÃ¡n: hÃ¬nh há»c, sá»‘ há»c, á»©ng dá»¥ng

## ğŸ› ï¸ CÃ i Ä‘áº·t

1. **Clone repository:**
```bash
git clone <repository-url>
cd gemma3n-math-tutor
```

2. **Táº¡o virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows
```

3. **CÃ i Ä‘áº·t dependencies:**
```bash
pip install -r requirements.txt
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Training cÆ¡ báº£n
```bash
python gemma3n_math_finetuning.py
```

### TÃ¹y chá»‰nh cáº¥u hÃ¬nh
Chá»‰nh sá»­a `CONFIG` trong file `gemma3n_math_finetuning.py`:

```python
CONFIG = {
    # Model settings
    "model_name": "unsloth/gemma-3n-E4B",
    "max_seq_length": 2048,
    
    # Training settings
    "max_steps": 200,
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 8,
    "learning_rate": 2e-4,
    
    # Output
    "output_dir": "outputs/gemma3n-math-tutor",
    "report_to": "tensorboard",  # hoáº·c "wandb", "comet_ml"
}
```

## ğŸ—ï¸ Kiáº¿n trÃºc giáº£i phÃ¡p

### 1. Xá»­ lÃ½ dá»¯ liá»‡u Ä‘a phÆ°Æ¡ng thá»©c
- **Image URL extraction**: Tá»± Ä‘á»™ng tÃ¬m vÃ  táº£i hÃ¬nh áº£nh tá»« markdown
- **Conversation format**: Chuyá»ƒn Ä‘á»•i sang format chat chuáº©n
- **Mixed data handling**: Xá»­ lÃ½ cáº£ samples cÃ³ vÃ  khÃ´ng cÃ³ hÃ¬nh áº£nh

### 2. Tá»‘i Æ°u hÃ³a training
- **Custom Data Collator**: Xá»­ lÃ½ batch mixed text/image
- **Memory optimization**: Táº¯t gradient checkpointing Ä‘á»ƒ trÃ¡nh lá»—i
- **LoRA fine-tuning**: Hiá»‡u quáº£ vÃ  tiáº¿t kiá»‡m memory

### 3. Robustness
- **Error handling**: Graceful fallback cho images bá»‹ lá»—i
- **Placeholder images**: Äáº£m báº£o consistency cho text-only samples
- **Progress tracking**: Detailed logging vÃ  monitoring

## ğŸ“ Cáº¥u trÃºc file

```
.
â”œâ”€â”€ gemma3n_math_finetuning.py  # Script chÃ­nh
â”œâ”€â”€ comet_config.py             # Cáº¥u hÃ¬nh Comet ML
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.md                  # HÆ°á»›ng dáº«n
â””â”€â”€ outputs/                   # ThÆ° má»¥c output models
    â””â”€â”€ gemma3n-math-tutor/    # Model Ä‘Ã£ train
```

## âš¡ Optimizations

### Giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p:

1. **CheckpointError**: Táº¯t gradient checkpointing
2. **Image token mismatch**: Custom collator vá»›i auto image token insertion
3. **Mixed data types**: Unified processing pipeline
4. **Memory efficiency**: 4-bit quantization + LoRA

### Performance metrics:
- **Memory usage**: ~14GB VRAM (Tesla T4)
- **Training speed**: 2x faster vá»›i Unsloth
- **Model size**: ~97% parameters frozen (LoRA)

## ğŸ§ª Testing vÃ  Inference

```python
# Test inference sau khi training
from gemma3n_math_finetuning import test_inference

test_inference("outputs/gemma3n-math-tutor")
```

## ğŸ“ˆ Monitoring

### TensorBoard
```bash
tensorboard --logdir outputs/gemma3n-math-tutor/logs
```

### Comet ML (recommended)
1. **Táº¡o tÃ i khoáº£n**: ÄÄƒng kÃ½ táº¡i [comet.com](https://www.comet.com/)
2. **Táº¡o workspace vÃ  project** trÃªn Comet ML dashboard
3. **Láº¥y API key**: Tá»« [Settings](https://www.comet.com/api/my/settings/)
4. **Cáº¥u hÃ¬nh**:
```bash
# CÃ¡ch 1: Environment variable (khuyáº¿n nghá»‹)
export COMET_API_KEY="your-api-key-here"

# CÃ¡ch 2: Chá»‰nh sá»­a comet_config.py
```
5. **Cáº­p nháº­t workspace vÃ  project** trong `comet_config.py`:
```python
COMET_CONFIG = {
    "workspace": "your-workspace-name",  # Thay báº±ng workspace cá»§a báº¡n
    "project": "gemma3n-math-tutor",     # TÃªn project
}
```

### Weights & Biases (alternative)
1. Uncomment `wandb` trong requirements.txt
2. Set `"report_to": "wandb"` trong CONFIG
3. Run: `wandb login`

## ğŸ”§ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **CUDA out of memory**:
   - Giáº£m `per_device_train_batch_size`
   - TÄƒng `gradient_accumulation_steps`

2. **Image loading failed**:
   - Check internet connection
   - Má»™t sá»‘ URLs cÃ³ thá»ƒ bá»‹ expired

3. **Model convergence issues**:
   - Adjust learning rate
   - Increase max_steps

### Debug mode:
```python
# Enable verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“Š Expected Results

Sau khi training, model sáº½ cÃ³ kháº£ nÄƒng:
- âœ… Hiá»ƒu vÃ  phÃ¢n tÃ­ch Ä‘á» bÃ i toÃ¡n cÃ³ hÃ¬nh áº£nh
- âœ… Táº¡o lá»i giáº£i step-by-step chi tiáº¿t
- âœ… Xá»­ lÃ½ cÃ¡c dáº¡ng toÃ¡n: hÃ¬nh há»c, Ä‘áº¡i sá»‘, á»©ng dá»¥ng
- âœ… Format output mathematical theo chuáº©n LaTeX

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Táº¡o Pull Request

## ğŸ“œ License

MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ™ Acknowledgments

- [Unsloth AI](https://github.com/unslothai/unsloth) - Fast LLM training
- [Hugging Face](https://huggingface.co/) - Transformers vÃ  Datasets
- Dataset creator: [ngohongthai](https://huggingface.co/ngohongthai)