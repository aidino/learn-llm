{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D05ixTbH4PV_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r89i4KPM4pEe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install latest transformers for Gemma 3N\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N\n",
        "!pip install comet-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVnUnFmNQutB",
        "outputId": "d97e5ea0-3df2-4042-da6b-5ebb2527ca6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "from typing import Tuple, List, Dict, Any, Optional\n",
        "from PIL import Image\n",
        "import requests\n",
        "import comet_ml\n",
        "from unsloth import FastVisionModel, get_chat_template\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOBZvX2gOKPK"
      },
      "source": [
        "## COMET logging config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSiQU67-OR3x"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "# Load the Comet.ml API key from Colab secrets\n",
        "os.environ['COMET_API_KEY'] = userdata.get('COMET_API_KEY')\n",
        "os.environ['COMET_PROJECT'] = userdata.get('COMET_PROJECT')\n",
        "os.environ['COMET_WORKSPACE'] = userdata.get('COMET_WORKSPACE')\n",
        "\n",
        "COMET_CONFIG = {\n",
        "    # API Key - REQUIRED\n",
        "    # C√°ch 1: Set environment variable\n",
        "    # export COMET_API_KEY=\"your-api-key-here\"\n",
        "\n",
        "    # C√°ch 2: Set tr·ª±c ti·∫øp (kh√¥ng khuy·∫øn ngh·ªã cho production)\n",
        "    \"api_key\": os.getenv(\"COMET_API_KEY\"),  # Ho·∫∑c thay b·∫±ng API key c·ªßa b·∫°n\n",
        "\n",
        "    # Workspace - REQUIRED\n",
        "    # T√™n workspace tr√™n Comet ML\n",
        "    \"workspace\": os.getenv(\"COMET_WORKSPACE\"),  # Thay b·∫±ng workspace c·ªßa b·∫°n\n",
        "\n",
        "    # Project Name - REQUIRED\n",
        "    # T√™n project tr√™n Comet ML\n",
        "    \"project\": os.getenv(\"COMET_PROJECT\"),  # C√≥ th·ªÉ thay ƒë·ªïi t√™n project\n",
        "\n",
        "    # Experiment Name - OPTIONAL\n",
        "    # T√™n experiment c·ª• th·ªÉ (t·ª± ƒë·ªông generate n·∫øu kh√¥ng set)\n",
        "    \"experiment_name\": \"base_line\",  # Ho·∫∑c ƒë·∫∑t t√™n custom nh∆∞ \"exp-001\"\n",
        "\n",
        "    # Tags - OPTIONAL\n",
        "    # Tags ƒë·ªÉ ph√¢n lo·∫°i experiments\n",
        "    \"tags\": [\n",
        "        \"gemma3n\",\n",
        "        \"vision-language\",\n",
        "        \"math-tutor\",\n",
        "        \"vietnamese\",\n",
        "        \"sixth-grade\",\n",
        "        \"fine-tuning\"\n",
        "    ],\n",
        "\n",
        "    # Additional Settings\n",
        "    \"auto_metric_logging\": True,     # T·ª± ƒë·ªông log metrics\n",
        "    \"auto_param_logging\": True,      # T·ª± ƒë·ªông log parameters\n",
        "    \"auto_histogram_weight_logging\": True,   # Log weight histograms\n",
        "    \"auto_histogram_gradient_logging\": True, # Log gradient histograms\n",
        "    \"auto_histogram_activation_logging\": False,  # T·∫Øt ƒë·ªÉ ti·∫øt ki·ªám memory\n",
        "    \"auto_output_logging\": \"default\",  # Log output (stdout/stderr)\n",
        "\n",
        "    # Model Logging\n",
        "    \"log_model\": True,              # Upload model artifacts\n",
        "    \"log_graph\": False,             # Log model graph (c√≥ th·ªÉ ch·∫≠m)\n",
        "    \"log_code\": True,               # Log source code\n",
        "    \"log_git_metadata\": True,       # Log git information\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTbqSvbUCEN4",
        "outputId": "b9a06cd1-ce23-41fe-dd45-7c26678e59a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Comet ML Configuration:\n",
            "   Workspace: mathpal\n",
            "   Project: mathpal-gemma3n\n",
            "   API Key: ‚úÖ Set\n",
            "   Tags: gemma3n, vision-language, math-tutor, vietnamese, sixth-grade, fine-tuning\n"
          ]
        }
      ],
      "source": [
        "def print_comet_info():\n",
        "    \"\"\"Print Comet ML configuration info.\"\"\"\n",
        "\n",
        "    print(\"üîß Comet ML Configuration:\")\n",
        "    print(f\"   Workspace: {COMET_CONFIG['workspace']}\")\n",
        "    print(f\"   Project: {COMET_CONFIG['project']}\")\n",
        "    print(f\"   API Key: {'‚úÖ Set' if COMET_CONFIG['api_key'] else '‚ùå Not set'}\")\n",
        "    print(f\"   Tags: {', '.join(COMET_CONFIG['tags'])}\")\n",
        "\n",
        "print_comet_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzklD6ZEPmtB"
      },
      "source": [
        "## Training config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRBFb0TtPSVZ"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Model settings\n",
        "    \"model_name\": \"unsloth/gemma-3n-E4B\",\n",
        "    \"max_seq_length\": 2048,\n",
        "    \"load_in_4bit\": True,\n",
        "\n",
        "    # Dataset settings\n",
        "    \"dataset_name\": \"ngohongthai/exam-sixth_grade-instruct-dataset\",\n",
        "    \"train_split\": \"train\",\n",
        "    \"test_split\": \"test\",\n",
        "\n",
        "    # Training settings\n",
        "    \"output_dir\": f\"{COMET_CONFIG['project']}/{COMET_CONFIG['experiment_name']}\",\n",
        "    \"max_steps\": 200,\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"logging_steps\": 5,\n",
        "    \"save_steps\": 50,\n",
        "\n",
        "    # LoRA settings\n",
        "    \"lora_r\": 32,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.0,\n",
        "\n",
        "    # System settings\n",
        "    \"use_gradient_checkpointing\": False,  # Disabled to avoid CheckpointError\n",
        "    \"report_to\": \"comet_ml\",  # Change to \"tensorboard\", \"wandb\" if needed\n",
        "    \"seed\": 42,\n",
        "\n",
        "    # Comet ML settings\n",
        "    \"comet_workspace\": COMET_CONFIG['workspace'],  # Set your Comet workspace name\n",
        "    \"comet_project\": COMET_CONFIG['project'],  # Set your Comet project name\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY2arn82QjOR"
      },
      "source": [
        "## IMAGE PROCESSING UTILITIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeP1VMJlQbbR"
      },
      "outputs": [],
      "source": [
        "def url_to_image(url: str, timeout: int = 10) -> Optional[Image.Image]:\n",
        "    \"\"\"\n",
        "    Download and convert URL to PIL Image.\n",
        "\n",
        "    Args:\n",
        "        url: Image URL\n",
        "        timeout: Request timeout in seconds\n",
        "\n",
        "    Returns:\n",
        "        PIL Image object or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "        return image\n",
        "    except (requests.exceptions.RequestException, IOError) as e:\n",
        "        print(f\"Failed to load image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_image_urls_from_markdown(text: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract image URLs from markdown text and replace with placeholders.\n",
        "\n",
        "    Args:\n",
        "        text: Markdown text containing image links\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cleaned_text, list_of_image_urls)\n",
        "    \"\"\"\n",
        "    # Pattern for markdown images: ![alt](url)\n",
        "    image_pattern = r\"!\\[.*?\\]\\((.*?)\\)\"\n",
        "    image_urls = re.findall(image_pattern, text)\n",
        "\n",
        "    # Remove image markdown syntax\n",
        "    cleaned_text = re.sub(image_pattern, \" \", text).strip()\n",
        "\n",
        "    return cleaned_text, image_urls\n",
        "\n",
        "def process_markdown_for_model(text: str) -> Tuple[str, List[Image.Image]]:\n",
        "    \"\"\"\n",
        "    Process markdown text to extract text and images for multimodal model.\n",
        "\n",
        "    Args:\n",
        "        text: Input markdown text\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (processed_text, list_of_pil_images)\n",
        "    \"\"\"\n",
        "    cleaned_text, image_urls = extract_image_urls_from_markdown(text)\n",
        "\n",
        "    # Download images\n",
        "    images = []\n",
        "    for url in image_urls:\n",
        "        image = url_to_image(url)\n",
        "        if image:\n",
        "            images.append(image)\n",
        "        else:\n",
        "            print(f\"Warning: Failed to load image from {url}\")\n",
        "\n",
        "    return cleaned_text, images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_VHgYuDRJhD"
      },
      "source": [
        "## DATASET PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyy-li4Qndx"
      },
      "outputs": [],
      "source": [
        "def create_conversation_content(text: str, images: List[Image.Image]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Create conversation content list with text and images.\n",
        "\n",
        "    Args:\n",
        "        text: Text content\n",
        "        images: List of PIL images\n",
        "\n",
        "    Returns:\n",
        "        List of content dictionaries\n",
        "    \"\"\"\n",
        "    content = [{\"type\": \"text\", \"text\": text}]\n",
        "\n",
        "    # Add images\n",
        "    for image in images:\n",
        "        content.append({\"type\": \"image\", \"image\": image})\n",
        "\n",
        "    return content\n",
        "\n",
        "def process_math_sample(sample: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Process a single math problem sample into conversation format.\n",
        "\n",
        "    Args:\n",
        "        sample: Dataset sample with 'question' and 'solution' keys\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'conversations' key containing the formatted conversation\n",
        "    \"\"\"\n",
        "    # Process question\n",
        "    question_text, question_images = process_markdown_for_model(sample[\"question\"])\n",
        "    user_content = create_conversation_content(question_text, question_images)\n",
        "\n",
        "    # Process solution (usually text-only, but check for images)\n",
        "    solution_text, solution_images = process_markdown_for_model(sample[\"solution\"])\n",
        "    assistant_content = create_conversation_content(solution_text, solution_images)\n",
        "\n",
        "    # Create conversation\n",
        "    conversations = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_content\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_content\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "def prepare_dataset(dataset_name: str, split: str) -> Dataset:\n",
        "    \"\"\"\n",
        "    Load and prepare the math dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: HuggingFace dataset name\n",
        "        split: Dataset split to load\n",
        "\n",
        "    Returns:\n",
        "        Processed Dataset object\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset: {dataset_name}, split: {split}\")\n",
        "    raw_dataset = load_dataset(dataset_name, split=split)\n",
        "\n",
        "    print(f\"Processing {len(raw_dataset)} samples...\")\n",
        "    processed_data = []\n",
        "\n",
        "    for i, sample in enumerate(raw_dataset):\n",
        "        try:\n",
        "            processed_sample = process_math_sample(sample)\n",
        "            processed_data.append(processed_sample)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Processed {i + 1}/{len(raw_dataset)} samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_data)} samples\")\n",
        "    return Dataset.from_list(processed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4XTPvDkRR3C"
      },
      "source": [
        "## CUSTOM DATA COLLATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejUJ0KGkRPli"
      },
      "outputs": [],
      "source": [
        "class HybridVisionDataCollator:\n",
        "    \"\"\"\n",
        "    Advanced data collator x·ª≠ l√Ω c·∫£ text-only v√† text+image samples.\n",
        "    T·ª± ƒë·ªông detect v√† x·ª≠ l√Ω mixed batches m·ªôt c√°ch th√¥ng minh.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, processor, handle_text_only=True):\n",
        "        self.processor = processor\n",
        "        self.handle_text_only = handle_text_only\n",
        "        self.placeholder_image = None\n",
        "\n",
        "    def _create_placeholder_image(self):\n",
        "        \"\"\"Create a minimal placeholder image cho text-only samples.\"\"\"\n",
        "        if self.placeholder_image is None:\n",
        "            # T·∫°o image nh·ªè ƒë·ªÉ minimize memory impact\n",
        "            self.placeholder_image = Image.new('RGB', (32, 32), color=(245, 245, 245))\n",
        "        return self.placeholder_image\n",
        "\n",
        "    def _validate_and_process_image(self, img):\n",
        "        \"\"\"Validate and process a single image.\"\"\"\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if not hasattr(img, 'convert'):\n",
        "                return None\n",
        "\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "            if img.size[0] < 1 or img.size[1] < 1:\n",
        "                return None\n",
        "\n",
        "            return img\n",
        "\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def _extract_images_from_conversation(self, conv):\n",
        "        \"\"\"Extract and validate all images from a conversation.\"\"\"\n",
        "        images = []\n",
        "\n",
        "        for message in conv:\n",
        "            for content in message.get(\"content\", []):\n",
        "                if content.get(\"type\") == \"image\" and \"image\" in content:\n",
        "                    img = content[\"image\"]\n",
        "                    processed_img = self._validate_and_process_image(img)\n",
        "                    if processed_img is not None:\n",
        "                        images.append(processed_img)\n",
        "\n",
        "        return images\n",
        "\n",
        "    def _has_real_images(self, conv):\n",
        "        \"\"\"Check if conversation has any real images.\"\"\"\n",
        "        images = self._extract_images_from_conversation(conv)\n",
        "        return len(images) > 0\n",
        "\n",
        "    def _create_text_only_conversation(self, conv):\n",
        "        \"\"\"Convert conversation to text-only format cho processor.\"\"\"\n",
        "        text_only_conv = []\n",
        "\n",
        "        for message in conv:\n",
        "            text_only_message = {\n",
        "                \"role\": message[\"role\"],\n",
        "                \"content\": []\n",
        "            }\n",
        "\n",
        "            # Extract ch·ªâ text content\n",
        "            for content in message.get(\"content\", []):\n",
        "                if content.get(\"type\") == \"text\":\n",
        "                    text_only_message[\"content\"].append(content)\n",
        "\n",
        "            # Ensure c√≥ √≠t nh·∫•t empty text content\n",
        "            if not text_only_message[\"content\"]:\n",
        "                text_only_message[\"content\"] = [{\"type\": \"text\", \"text\": \"\"}]\n",
        "\n",
        "            text_only_conv.append(text_only_message)\n",
        "\n",
        "        return text_only_conv\n",
        "\n",
        "    def _insert_image_token_strategically(self, text, num_images=1):\n",
        "        \"\"\"Insert image token v√†o position th√¥ng minh trong text.\"\"\"\n",
        "        if '<image>' in text:\n",
        "            return text\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        # Strategy 1: Insert sau user role marker\n",
        "        for i, line in enumerate(lines):\n",
        "            if any(marker in line.lower() for marker in ['<|user|>', 'user:', 'human:']):\n",
        "                # Insert sau line n√†y\n",
        "                insert_pos = i + 1\n",
        "                for _ in range(num_images):\n",
        "                    lines.insert(insert_pos, '<image>')\n",
        "                    insert_pos += 1\n",
        "                return '\\n'.join(lines)\n",
        "\n",
        "        # Strategy 2: Insert ·ªü ƒë·∫ßu content n·∫øu kh√¥ng t√¨m th·∫•y role marker\n",
        "        if len(lines) > 0:\n",
        "            # Insert sau line ƒë·∫ßu ti√™n (th∆∞·ªùng l√† role header)\n",
        "            for _ in range(num_images):\n",
        "                lines.insert(1, '<image>')\n",
        "            return '\\n'.join(lines)\n",
        "\n",
        "        # Fallback: insert ·ªü ƒë·∫ßu\n",
        "        image_tokens = '\\n'.join(['<image>'] * num_images)\n",
        "        return image_tokens + '\\n' + text\n",
        "\n",
        "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Collate mixed batch c·ªßa text-only v√† text+image samples.\n",
        "\n",
        "        Args:\n",
        "            examples: List of processed conversation examples\n",
        "\n",
        "        Returns:\n",
        "            Batch dictionary with tensors\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"Processing hybrid batch of {len(examples)} examples...\")\n",
        "\n",
        "            # Classify samples\n",
        "            image_samples = []\n",
        "            text_only_samples = []\n",
        "\n",
        "            for idx, example in enumerate(examples):\n",
        "                conv = example[\"conversations\"]\n",
        "                if self._has_real_images(conv):\n",
        "                    image_samples.append((idx, example))\n",
        "                    print(f\"Sample {idx}: IMAGE SAMPLE ‚úÖ\")\n",
        "                else:\n",
        "                    text_only_samples.append((idx, example))\n",
        "                    print(f\"Sample {idx}: TEXT ONLY üìù\")\n",
        "\n",
        "            print(f\"Batch composition: {len(image_samples)} image samples, {len(text_only_samples)} text-only\")\n",
        "\n",
        "            # Handle different scenarios\n",
        "            if len(image_samples) > 0 and len(text_only_samples) > 0:\n",
        "                # Mixed batch - x·ª≠ l√Ω hybrid\n",
        "                return self._process_mixed_batch(image_samples, text_only_samples)\n",
        "            elif len(image_samples) > 0:\n",
        "                # Pure image batch\n",
        "                return self._process_image_batch(image_samples)\n",
        "            elif len(text_only_samples) > 0 and self.handle_text_only:\n",
        "                # Pure text batch v·ªõi placeholder images\n",
        "                return self._process_text_only_batch(text_only_samples)\n",
        "            else:\n",
        "                raise ValueError(\"No valid samples to process or text-only handling disabled!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical error in hybrid data collator: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise e\n",
        "\n",
        "    def _process_image_batch(self, image_samples):\n",
        "        \"\"\"Process batch ch·ªâ c√≥ image samples.\"\"\"\n",
        "        print(\"Processing pure image batch...\")\n",
        "\n",
        "        texts = []\n",
        "        images_list = []\n",
        "\n",
        "        for idx, example in image_samples:\n",
        "            conv = example[\"conversations\"]\n",
        "            images = self._extract_images_from_conversation(conv)\n",
        "\n",
        "            # Generate text\n",
        "            text = self.processor.apply_chat_template(\n",
        "                conv, tokenize=False, add_generation_prompt=False\n",
        "            )\n",
        "\n",
        "            # Validate token count\n",
        "            image_token_count = text.count('<image>')\n",
        "            actual_image_count = len(images)\n",
        "\n",
        "            print(f\"Image sample {idx}: {actual_image_count} images, {image_token_count} tokens\")\n",
        "\n",
        "            # Sync tokens v√† images\n",
        "            if image_token_count != actual_image_count:\n",
        "                if image_token_count < actual_image_count:\n",
        "                    images = images[:image_token_count] if image_token_count > 0 else images[:1]\n",
        "                elif image_token_count > actual_image_count:\n",
        "                    # Th√™m placeholder images\n",
        "                    while len(images) < image_token_count:\n",
        "                        images.append(self._create_placeholder_image())\n",
        "\n",
        "            texts.append(text)\n",
        "            images_list.append(images)\n",
        "\n",
        "        return self._create_batch(texts, images_list)\n",
        "\n",
        "    def _process_text_only_batch(self, text_only_samples):\n",
        "        \"\"\"Process batch ch·ªâ c√≥ text samples v·ªõi placeholder images.\"\"\"\n",
        "        print(\"Processing text-only batch with placeholder images...\")\n",
        "\n",
        "        texts = []\n",
        "        images_list = []\n",
        "\n",
        "        for idx, example in text_only_samples:\n",
        "            conv = example[\"conversations\"]\n",
        "\n",
        "            # Convert to text-only format\n",
        "            text_only_conv = self._create_text_only_conversation(conv)\n",
        "\n",
        "            # Generate text\n",
        "            text = self.processor.apply_chat_template(\n",
        "                text_only_conv, tokenize=False, add_generation_prompt=False\n",
        "            )\n",
        "\n",
        "            # Add 1 placeholder image v√† corresponding token\n",
        "            placeholder = self._create_placeholder_image()\n",
        "            text_with_image_token = self._insert_image_token_strategically(text, num_images=1)\n",
        "\n",
        "            print(f\"Text sample {idx}: Added 1 placeholder image and token\")\n",
        "\n",
        "            texts.append(text_with_image_token)\n",
        "            images_list.append([placeholder])\n",
        "\n",
        "        return self._create_batch(texts, images_list)\n",
        "\n",
        "    def _process_mixed_batch(self, image_samples, text_only_samples):\n",
        "        \"\"\"Process mixed batch c√≥ c·∫£ image v√† text-only samples.\"\"\"\n",
        "        print(\"Processing mixed batch...\")\n",
        "\n",
        "        texts = []\n",
        "        images_list = []\n",
        "\n",
        "        # Process image samples first\n",
        "        for idx, example in image_samples:\n",
        "            conv = example[\"conversations\"]\n",
        "            images = self._extract_images_from_conversation(conv)\n",
        "\n",
        "            text = self.processor.apply_chat_template(\n",
        "                conv, tokenize=False, add_generation_prompt=False\n",
        "            )\n",
        "\n",
        "            image_token_count = text.count('<image>')\n",
        "            actual_image_count = len(images)\n",
        "\n",
        "            print(f\"Mixed - Image sample {idx}: {actual_image_count} images, {image_token_count} tokens\")\n",
        "\n",
        "            # Sync tokens v√† images\n",
        "            if image_token_count != actual_image_count:\n",
        "                if image_token_count < actual_image_count:\n",
        "                    images = images[:image_token_count] if image_token_count > 0 else images[:1]\n",
        "                elif image_token_count > actual_image_count:\n",
        "                    while len(images) < image_token_count:\n",
        "                        images.append(self._create_placeholder_image())\n",
        "\n",
        "            texts.append(text)\n",
        "            images_list.append(images)\n",
        "\n",
        "        # Process text-only samples with placeholders\n",
        "        for idx, example in text_only_samples:\n",
        "            conv = example[\"conversations\"]\n",
        "            text_only_conv = self._create_text_only_conversation(conv)\n",
        "\n",
        "            text = self.processor.apply_chat_template(\n",
        "                text_only_conv, tokenize=False, add_generation_prompt=False\n",
        "            )\n",
        "\n",
        "            # Add placeholder\n",
        "            placeholder = self._create_placeholder_image()\n",
        "            text_with_token = self._insert_image_token_strategically(text, num_images=1)\n",
        "\n",
        "            print(f\"Mixed - Text sample {idx}: Added placeholder image and token\")\n",
        "\n",
        "            texts.append(text_with_token)\n",
        "            images_list.append([placeholder])\n",
        "\n",
        "        return self._create_batch(texts, images_list)\n",
        "\n",
        "    def _create_batch(self, texts, images_list):\n",
        "        \"\"\"Create final batch tensor.\"\"\"\n",
        "        print(f\"Creating batch: {len(texts)} texts, {len(images_list)} image lists\")\n",
        "\n",
        "        # Final validation\n",
        "        for i, (text, imgs) in enumerate(zip(texts, images_list)):\n",
        "            token_count = text.count('<image>')\n",
        "            image_count = len(imgs)\n",
        "\n",
        "            if token_count != image_count:\n",
        "                print(f\"‚ö†Ô∏è  Sample {i}: Token/image mismatch ({token_count} vs {image_count})\")\n",
        "                # Fix mismatch\n",
        "                if token_count > image_count:\n",
        "                    while len(imgs) < token_count:\n",
        "                        imgs.append(self._create_placeholder_image())\n",
        "                    images_list[i] = imgs\n",
        "                elif image_count > token_count:\n",
        "                    images_list[i] = imgs[:token_count] if token_count > 0 else imgs[:1]\n",
        "\n",
        "        # Process with processor\n",
        "        print(\"Sending to processor...\")\n",
        "        batch = self.processor(\n",
        "            text=texts,\n",
        "            images=images_list,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=CONFIG[\"max_seq_length\"]\n",
        "        )\n",
        "\n",
        "        # Create labels\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        print(f\"Batch created successfully: {batch.keys()}\")\n",
        "        if \"pixel_values\" in batch:\n",
        "            print(f\"pixel_values shape: {batch['pixel_values'].shape}\")\n",
        "        print(f\"input_ids shape: {batch['input_ids'].shape}\")\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyMoQG3cRYFa"
      },
      "source": [
        "## MODEL SETUP AND TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfsiJq7FRVrq"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_processor(config: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Load and setup Gemma3N model and processor.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, processor)\n",
        "    \"\"\"\n",
        "    print(\"Loading Gemma3N model and processor...\")\n",
        "\n",
        "    # Load model and processor\n",
        "    model, processor = FastVisionModel.from_pretrained(\n",
        "        config[\"model_name\"],\n",
        "        max_seq_length=config[\"max_seq_length\"],\n",
        "        load_in_4bit=config[\"load_in_4bit\"],\n",
        "        use_gradient_checkpointing=\"unsloth\" if config[\"use_gradient_checkpointing\"] else False,\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = FastVisionModel.get_peft_model(\n",
        "        model,\n",
        "        finetune_vision_layers=True,\n",
        "        finetune_language_layers=True,\n",
        "        finetune_attention_modules=True,\n",
        "        finetune_mlp_modules=True,\n",
        "        r=config[\"lora_r\"],\n",
        "        lora_alpha=config[\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_dropout\"],\n",
        "        bias=\"none\",\n",
        "        random_state=config[\"seed\"],\n",
        "        use_rslora=False,\n",
        "        target_modules=\"all-linear\",\n",
        "        modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
        "    )\n",
        "\n",
        "    # Setup chat template\n",
        "    processor = get_chat_template(processor, \"gemma-3n\")\n",
        "\n",
        "    print(\"Model and processor setup complete!\")\n",
        "    return model, processor\n",
        "\n",
        "def create_trainer(model, processor, train_dataset, config: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Create optimized SFTTrainer.\n",
        "\n",
        "    Args:\n",
        "        model: Prepared model\n",
        "        processor: Model processor\n",
        "        train_dataset: Training dataset\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        Configured SFTTrainer\n",
        "    \"\"\"\n",
        "    # Enable training\n",
        "    FastVisionModel.for_training(model)\n",
        "\n",
        "    # Create data collator\n",
        "    data_collator = HybridVisionDataCollator(\n",
        "        processor,\n",
        "        handle_text_only=True  # Enable text-only handling for mixed dataset\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = SFTConfig(\n",
        "        # Basic training settings\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        max_steps=config[\"max_steps\"],\n",
        "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
        "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "\n",
        "        # Optimization settings\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        warmup_ratio=config[\"warmup_ratio\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        optim=\"adamw_torch_fused\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "\n",
        "        # Memory optimization\n",
        "        gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False} if config[\"use_gradient_checkpointing\"] else {},\n",
        "        max_grad_norm=0.3,\n",
        "\n",
        "        # Logging and saving\n",
        "        logging_steps=config[\"logging_steps\"],\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=config[\"save_steps\"],\n",
        "        report_to=config[\"report_to\"],\n",
        "\n",
        "        # Vision-specific settings\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        max_length=config[\"max_seq_length\"],\n",
        "\n",
        "        # Reproducibility\n",
        "        seed=config[\"seed\"],\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        processing_class=processor.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXGsW0bRpNB"
      },
      "source": [
        "## COMET ML SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3LIcs2cRjK6"
      },
      "outputs": [],
      "source": [
        "def setup_comet_ml(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Setup Comet ML experiment tracking with full configuration support.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration dictionary\n",
        "    \"\"\"\n",
        "    if config[\"report_to\"] == \"comet_ml\":\n",
        "        try:\n",
        "            # Initialize Comet experiment\n",
        "            experiment_kwargs = {\n",
        "                \"workspace\": COMET_CONFIG.get(\"workspace\"),\n",
        "                \"project_name\": COMET_CONFIG.get(\"project\"),\n",
        "                \"auto_metric_logging\": COMET_CONFIG.get(\"auto_metric_logging\", True),\n",
        "                \"auto_param_logging\": COMET_CONFIG.get(\"auto_param_logging\", True),\n",
        "                \"auto_histogram_weight_logging\": COMET_CONFIG.get(\"auto_histogram_weight_logging\", True),\n",
        "                \"auto_histogram_gradient_logging\": COMET_CONFIG.get(\"auto_histogram_gradient_logging\", True),\n",
        "                \"auto_histogram_activation_logging\": COMET_CONFIG.get(\"auto_histogram_activation_logging\", False),\n",
        "            }\n",
        "\n",
        "            # Remove None values\n",
        "            experiment_kwargs = {k: v for k, v in experiment_kwargs.items() if v is not None}\n",
        "\n",
        "            experiment = comet_ml.Experiment(**experiment_kwargs)\n",
        "\n",
        "            # Log configuration\n",
        "            experiment.log_parameters(config)\n",
        "\n",
        "            # Add tags\n",
        "            tags = COMET_CONFIG.get(\"tags\", [\"gemma3n\", \"vision-language\", \"math-tutor\"])\n",
        "            for tag in tags:\n",
        "                experiment.add_tag(tag)\n",
        "\n",
        "            # Log additional metadata\n",
        "            experiment.log_other(\"dataset\", \"ngohongthai/exam-sixth_grade-instruct-dataset\")\n",
        "            experiment.log_other(\"model_base\", \"unsloth/gemma-3n-E4B\")\n",
        "            experiment.log_other(\"task\", \"sixth-grade-math-tutoring\")\n",
        "            experiment.log_other(\"language\", \"vietnamese\")\n",
        "\n",
        "            print(f\"‚úÖ Comet ML experiment initialized\")\n",
        "            print(f\"üîó Experiment URL: {experiment.url}\")\n",
        "            print(f\"üìä Workspace: {COMET_CONFIG.get('workspace', 'default')}\")\n",
        "            print(f\"üìÅ Project: {COMET_CONFIG.get('project', 'gemma3n-math-tutor')}\")\n",
        "\n",
        "            # Set environment variables for transformers integration\n",
        "            os.environ[\"COMET_PROJECT_NAME\"] = COMET_CONFIG.get(\"project\", \"gemma3n-math-tutor\")\n",
        "            if COMET_CONFIG.get(\"workspace\"):\n",
        "                os.environ[\"COMET_WORKSPACE\"] = COMET_CONFIG[\"workspace\"]\n",
        "\n",
        "            return experiment\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ùå comet_ml not installed. Please install with: pip install comet-ml\")\n",
        "            print(\"Falling back to tensorboard logging...\")\n",
        "            config[\"report_to\"] = \"tensorboard\"\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to initialize Comet ML: {e}\")\n",
        "            print(\"Possible causes:\")\n",
        "            print(\"- Invalid API key or workspace/project names\")\n",
        "            print(\"- Network connection issues\")\n",
        "            print(\"- Missing permissions\")\n",
        "            print(\"Falling back to tensorboard logging...\")\n",
        "            config[\"report_to\"] = \"tensorboard\"\n",
        "            return None\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDMP-TxOSVYh"
      },
      "source": [
        "## TRAININGGGGGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQqWqPSUSPMS"
      },
      "outputs": [],
      "source": [
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSnftdS3Scvi",
        "outputId": "791a8336-e4af-46b1-fc82-d90089048d4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/mathpal/mathpal-gemma3n/4e7cd6e71c614c0eb81b9802883202b0\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Comet ML experiment initialized\n",
            "üîó Experiment URL: https://www.comet.com/mathpal/mathpal-gemma3n/4e7cd6e71c614c0eb81b9802883202b0\n",
            "üìä Workspace: mathpal\n",
            "üìÅ Project: mathpal-gemma3n\n"
          ]
        }
      ],
      "source": [
        "comet_experiment = setup_comet_ml(CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "d9a651ae12a24c06951f1ef75b02bcf5",
            "9afd3126bd284160bdc066629e06ddd5",
            "f40ec5c86fe0418aa42f5d72b2217104",
            "e43443df6c01415b9ddbb449f3bfceab",
            "14c9d0b0a422477587349e391450ac1e",
            "ae956ba0daa84f6a8c50b0d11588132f",
            "539281d273784b4ba18e0b78a6876332",
            "58329ad1f42742c783914621fb60ff23",
            "4df4ca75923c4571a2736eb345bcf195",
            "bbfd8793c3eb4120b74aee7a931a51f4",
            "95a91385411643cd86ccb7f3882ab32c"
          ]
        },
        "id": "pQP3SurSSgPx",
        "outputId": "195397f2-7d60-4039-e3bd-724cc6897578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Gemma3N model and processor...\n",
            "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9a651ae12a24c06951f1ef75b02bcf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n",
            "Model and processor setup complete!\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup model and processor\n",
        "model, processor = setup_model_and_processor(CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0dScB_ATUYC",
        "outputId": "809fc22d-d18c-4a38-8b20-1b3a2fc3f286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset: ngohongthai/exam-sixth_grade-instruct-dataset, split: train\n",
            "Processing 1010 samples...\n",
            "Processed 100/1010 samples\n",
            "Processed 200/1010 samples\n",
            "Processed 300/1010 samples\n",
            "Processed 400/1010 samples\n",
            "Processed 500/1010 samples\n",
            "Processed 600/1010 samples\n",
            "Processed 700/1010 samples\n",
            "Processed 800/1010 samples\n",
            "Processed 900/1010 samples\n",
            "Processed 1000/1010 samples\n",
            "Successfully processed 1010 samples\n",
            "\n",
            "Dataset Statistics:\n",
            "- Training samples: 1010\n",
            "- Samples with images: 117\n",
            "- Text-only samples: 893\n"
          ]
        }
      ],
      "source": [
        "# 2. Prepare dataset\n",
        "train_dataset = prepare_dataset(CONFIG[\"dataset_name\"], CONFIG[\"train_split\"])\n",
        "\n",
        "# Print dataset statistics\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"- Training samples: {len(train_dataset)}\")\n",
        "\n",
        "# Count samples with images\n",
        "samples_with_images = 0\n",
        "for sample in train_dataset:\n",
        "    for conv in sample[\"conversations\"]:\n",
        "        for content in conv.get(\"content\", []):\n",
        "            if content.get(\"type\") == \"image\":\n",
        "                samples_with_images += 1\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "\n",
        "print(f\"- Samples with images: {samples_with_images}\")\n",
        "print(f\"- Text-only samples: {len(train_dataset) - samples_with_images}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSLFcddVVpn7"
      },
      "outputs": [],
      "source": [
        "# 3. Create trainer\n",
        "trainer = create_trainer(model, processor, train_dataset, CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "OhvUn43JVbA0",
        "outputId": "90ca3254-da41-4c93-9eff-6b8443d5069d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting training...\n",
            "- Output directory: mathpal-gemma3n/base_line\n",
            "- Max steps: 200\n",
            "- Batch size: 1\n",
            "- Gradient accumulation: 8\n",
            "- Effective batch size: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,010 | Num Epochs = 2 | Total steps = 200\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 76,840,960 of 7,926,819,152 (0.97% trained)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m String value length exceeds 1000 characters and will be truncated. Provided value: 'LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='unsloth/gemma-3n-e4b-unsloth-bnb-4bit', revision=None, inference_mode=False, r=32, target_modules='(?:.*?(?:vision|image|visual|patch|language|text).*?(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense).*?(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj|correction_coefs|prediction_coefs|modality_router|linear_left|linear_right|per_layer_input_gate|per_layer_projection|0|1|2|ffw_layer_1|ffw_layer_2|pos_proj|post|linear_start|linear_end|embedding_projection).*?)|(?:\\\\bmodel\\\\.layers\\\\.[\\\\d]{1,}\\\\.(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense)\\\\.(?:(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj|correction_coefs|prediction_coefs|modality_router|linear_left|linear_right|per_layer_input_gate|per_layer_projection|0|1|2|ffw_layer_1|ffw_layer_2|pos_proj|post|linear_start|linear_end|embedding_projection)))', exclude_modules=None, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing hybrid batch of 1 examples...\n",
            "Sample 0: TEXT ONLY üìù\n",
            "Batch composition: 0 image samples, 1 text-only\n",
            "Critical error in hybrid data collator: No valid samples to process or text-only handling disabled!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-270888095.py\", line 147, in __call__\n",
            "    raise ValueError(\"No valid samples to process or text-only handling disabled!\")\n",
            "ValueError: No valid samples to process or text-only handling disabled!\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No valid samples to process or text-only handling disabled!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3344690389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2238\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/memory.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No executable batch size found, reached zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_reduce_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/loss_utils.py\u001b[0m in \u001b[0;36m_unsloth_get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mbatch_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-270888095.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_image_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-270888095.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_text_only_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_only_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid samples to process or text-only handling disabled!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No valid samples to process or text-only handling disabled!"
          ]
        }
      ],
      "source": [
        " # 4. Start training\n",
        "print(f\"\\nüöÄ Starting training...\")\n",
        "print(f\"- Output directory: {CONFIG['output_dir']}\")\n",
        "print(f\"- Max steps: {CONFIG['max_steps']}\")\n",
        "print(f\"- Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
        "print(f\"- Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"- Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
        "\n",
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVuoyZtXVn7"
      },
      "source": [
        "## DEBUGGING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AnjjZxsVzNS"
      },
      "outputs": [],
      "source": [
        "def debug_raw_dataset():\n",
        "    \"\"\"Debug raw dataset tr∆∞·ªõc khi processing.\"\"\"\n",
        "    print(\"üîç Debugging raw dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Load raw dataset\n",
        "        raw_dataset = load_dataset(\"ngohongthai/exam-sixth_grade-instruct-dataset\", split=\"train\")\n",
        "        print(f\"Raw dataset size: {len(raw_dataset)}\")\n",
        "\n",
        "        # Check first few samples\n",
        "        for i in range(min(3, len(raw_dataset))):\n",
        "            sample = raw_dataset[i]\n",
        "            print(f\"\\nSample {i}:\")\n",
        "            print(f\"  Keys: {sample.keys()}\")\n",
        "            print(f\"  Question: {sample['question'][:100]}...\")\n",
        "            print(f\"  Solution: {sample['solution'][:100]}...\")\n",
        "            print(f\"  Question has images: {'![' in sample['question']}\")\n",
        "            print(f\"  Solution has images: {'![' in sample['solution']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading raw dataset: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "debug_raw_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3xT3sf7XZUj"
      },
      "outputs": [],
      "source": [
        "def debug_processed_dataset():\n",
        "    \"\"\"Debug processed dataset.\"\"\"\n",
        "    print(\"\\nüîç Debugging processed dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Process small subset\n",
        "        raw_dataset = load_dataset(\"ngohongthai/exam-sixth_grade-instruct-dataset\", split=\"train[:5]\")\n",
        "\n",
        "        processed_data = []\n",
        "        for i, sample in enumerate(raw_dataset):\n",
        "            try:\n",
        "                processed_sample = process_math_sample(sample)\n",
        "                processed_data.append(processed_sample)\n",
        "\n",
        "                print(f\"\\nProcessed sample {i}:\")\n",
        "                print(f\"  Conversations: {len(processed_sample['conversations'])}\")\n",
        "\n",
        "                for j, conv in enumerate(processed_sample['conversations']):\n",
        "                    print(f\"  Conv {j}: role={conv['role']}, content_items={len(conv['content'])}\")\n",
        "                    for k, content in enumerate(conv['content']):\n",
        "                        if content['type'] == 'image':\n",
        "                            img = content.get('image')\n",
        "                            print(f\"    Content {k}: image, type={type(img)}, size={getattr(img, 'size', 'unknown')}\")\n",
        "                        else:\n",
        "                            print(f\"    Content {k}: text, length={len(content.get('text', ''))}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample {i}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "        return len(processed_data) > 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processed dataset debug: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "debug_processed_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVea9mywXvbc"
      },
      "outputs": [],
      "source": [
        "def debug_processor():\n",
        "    \"\"\"Debug processor setup.\"\"\"\n",
        "    print(\"\\nüîç Debugging processor...\")\n",
        "\n",
        "    try:\n",
        "        model, processor = setup_model_and_processor(CONFIG)\n",
        "\n",
        "        print(\"‚úÖ Model and processor loaded successfully\")\n",
        "        print(f\"Processor type: {type(processor)}\")\n",
        "        print(f\"Tokenizer type: {type(processor.tokenizer)}\")\n",
        "        print(f\"Has image processor: {hasattr(processor, 'image_processor')}\")\n",
        "\n",
        "        # Test simple processing\n",
        "        test_text = \"Hello world\"\n",
        "        test_image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        try:\n",
        "            batch = processor(\n",
        "                text=[test_text],\n",
        "                images=[[test_image]],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            )\n",
        "            print(\"‚úÖ Processor test successful\")\n",
        "            print(f\"Batch keys: {batch.keys()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Processor test failed: {e}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processor debug: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "debug_processor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOAg_W6jX-O7"
      },
      "outputs": [],
      "source": [
        "def debug_data_collator():\n",
        "    \"\"\"Debug data collator v·ªõi real data.\"\"\"\n",
        "    print(\"\\nüîç Debugging data collator...\")\n",
        "\n",
        "    try:\n",
        "        # Setup processor\n",
        "        model, processor = setup_model_and_processor(CONFIG)\n",
        "\n",
        "        # Create collator\n",
        "        collator = HybridVisionDataCollator(processor, handle_text_only=True)\n",
        "\n",
        "        # Load v√† process m·ªôt sample\n",
        "        raw_dataset = load_dataset(\"ngohongthai/exam-sixth_grade-instruct-dataset\", split=\"train[:3]\")\n",
        "\n",
        "        processed_samples = []\n",
        "        for sample in raw_dataset:\n",
        "            try:\n",
        "                processed = process_math_sample(sample)\n",
        "                processed_samples.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                # T·∫°o fallback sample\n",
        "                fallback_sample = {\n",
        "                    \"conversations\": [\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [{\"type\": \"text\", \"text\": \"Test question\"}]\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": [{\"type\": \"text\", \"text\": \"Test answer\"}]\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                processed_samples.append(fallback_sample)\n",
        "\n",
        "        print(f\"Processed {len(processed_samples)} samples\")\n",
        "\n",
        "        # Test collator\n",
        "        try:\n",
        "            batch = collator(processed_samples)\n",
        "            print(\"‚úÖ Data collator test successful!\")\n",
        "            print(f\"Batch keys: {batch.keys()}\")\n",
        "            for key, value in batch.items():\n",
        "                if hasattr(value, 'shape'):\n",
        "                    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Data collator failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data collator debug: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "debug_data_collator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ehh47jyYeic"
      },
      "outputs": [],
      "source": [
        "def test_data_collator(train_dataset, processor, num_samples=2):\n",
        "    \"\"\"Test the data collator with a few samples to catch issues early.\"\"\"\n",
        "    print(f\"\\nüß™ Testing data collator with {num_samples} samples...\")\n",
        "\n",
        "    try:\n",
        "        # Create data collator\n",
        "        collator = HybridVisionDataCollator(processor, handle_text_only=True)\n",
        "\n",
        "        # Test with a small batch\n",
        "        test_samples = [train_dataset[i] for i in range(min(num_samples, len(train_dataset)))]\n",
        "\n",
        "        print(f\"Test samples prepared: {len(test_samples)}\")\n",
        "\n",
        "        # Try to collate\n",
        "        batch = collator(test_samples)\n",
        "\n",
        "        print(\"‚úÖ Data collator test passed!\")\n",
        "        print(f\"Batch keys: {batch.keys()}\")\n",
        "        for key, value in batch.items():\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"  {key}: {value.shape}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {type(value)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Data collator test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "test_data_collator(train_dataset, processor, num_samples=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Rw3m3cQaY5c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14c9d0b0a422477587349e391450ac1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4df4ca75923c4571a2736eb345bcf195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "539281d273784b4ba18e0b78a6876332": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58329ad1f42742c783914621fb60ff23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a91385411643cd86ccb7f3882ab32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9afd3126bd284160bdc066629e06ddd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae956ba0daa84f6a8c50b0d11588132f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_539281d273784b4ba18e0b78a6876332",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ae956ba0daa84f6a8c50b0d11588132f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbfd8793c3eb4120b74aee7a931a51f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a651ae12a24c06951f1ef75b02bcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9afd3126bd284160bdc066629e06ddd5",
              "IPY_MODEL_f40ec5c86fe0418aa42f5d72b2217104",
              "IPY_MODEL_e43443df6c01415b9ddbb449f3bfceab"
            ],
            "layout": "IPY_MODEL_14c9d0b0a422477587349e391450ac1e"
          }
        },
        "e43443df6c01415b9ddbb449f3bfceab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbfd8793c3eb4120b74aee7a931a51f4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_95a91385411643cd86ccb7f3882ab32c",
            "value": "‚Äá3/3‚Äá[00:41&lt;00:00,‚Äá12.33s/it]"
          }
        },
        "f40ec5c86fe0418aa42f5d72b2217104": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58329ad1f42742c783914621fb60ff23",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4df4ca75923c4571a2736eb345bcf195",
            "value": 3
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
