{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Gemma 3N Fine-tuning for Sixth Grade Math (Refactored)\n",
        "\n",
        "Refactored version ƒë·ªÉ fine-tune Gemma 3N v·ªõi dataset to√°n l·ªõp 6 s·ª≠ d·ª•ng Unsloth.\n",
        "\n",
        "## Thay ƒë·ªïi ch√≠nh:\n",
        "- ‚úÖ S·ª≠a l·ªói dataset field mapping (question/answer thay v√¨ question/solution)\n",
        "- ‚úÖ Ch·ªçn Gemma 3N E2B thay v√¨ E4B cho mobile deployment\n",
        "- ‚úÖ Lo·∫°i b·ªè Colab-specific code\n",
        "- ‚úÖ T·ªëi ∆∞u config cho mobile deployment\n",
        "- ‚úÖ ƒê∆°n gi·∫£n h√≥a data collator\n",
        "- ‚úÖ Th√™m validation v√† error handling\n",
        "- ‚úÖ Improved documentation v√† logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t dependencies\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt.\"\"\"\n",
        "    packages = [\n",
        "        \"unsloth\",\n",
        "        \"--upgrade timm\",  # Cho Gemma 3N\n",
        "        \"comet-ml\"\n",
        "    ]\n",
        "    \n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + package.split())\n",
        "            print(f\"‚úÖ Installed {package}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
        "\n",
        "install_requirements()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "from typing import Tuple, List, Dict, Any, Optional\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth import FastVisionModel, get_chat_template\n",
        "\n",
        "# Comet ML (optional)\n",
        "try:\n",
        "    import comet_ml\n",
        "    COMET_AVAILABLE = True\n",
        "    print(\"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\")\n",
        "except ImportError:\n",
        "    COMET_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è comet_ml kh√¥ng c√≥ s·∫µn. Logging s·∫Ω ch·ªâ d√πng tensorboard.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration\n",
        "\n",
        "C·∫•u h√¨nh ƒë∆∞·ª£c t·ªëi ∆∞u cho mobile deployment v·ªõi Gemma 3N E2B.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration t·ªëi ∆∞u cho mobile deployment\n",
        "CONFIG = {\n",
        "    # Model settings - E2B t·ªët h∆°n cho mobile (nh·ªè h∆°n, √≠t tham s·ªë h∆°n)\n",
        "    \"model_name\": \"unsloth/gemma-3n-E2B\",  # Thay ƒë·ªïi t·ª´ E4B sang E2B\n",
        "    \"max_seq_length\": 1024,  # Gi·∫£m t·ª´ 2048 ƒë·ªÉ t·ªëi ∆∞u memory cho mobile\n",
        "    \"load_in_4bit\": True,\n",
        "\n",
        "    # Dataset settings\n",
        "    \"dataset_name\": \"ngohongthai/exam-sixth_grade-instruct-dataset\",\n",
        "    \"train_split\": \"train\",\n",
        "\n",
        "    # Training settings - t·ªëi ∆∞u cho mobile\n",
        "    \"output_dir\": \"./gemma3n_e2b_sixth_grade\",\n",
        "    \"max_steps\": 100,  # Gi·∫£m s·ªë steps ƒë·ªÉ training nhanh h∆°n\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 4,  # Gi·∫£m t·ª´ 8\n",
        "    \"learning_rate\": 5e-5,  # TƒÉng learning rate m·ªôt ch√∫t\n",
        "    \"warmup_ratio\": 0.1,  # TƒÉng warmup\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"logging_steps\": 10,\n",
        "    \"save_steps\": 25,\n",
        "\n",
        "    # LoRA settings - t·ªëi ∆∞u cho mobile\n",
        "    \"lora_r\": 16,  # Gi·∫£m t·ª´ 32\n",
        "    \"lora_alpha\": 16,  # Gi·∫£m t·ª´ 32\n",
        "    \"lora_dropout\": 0.05,  # Th√™m m·ªôt ch√∫t dropout\n",
        "\n",
        "    # System settings\n",
        "    \"use_gradient_checkpointing\": True,  # Enable ƒë·ªÉ ti·∫øt ki·ªám memory\n",
        "    \"report_to\": \"comet_ml\" if COMET_AVAILABLE else \"tensorboard\",\n",
        "    \"seed\": 42,\n",
        "    \n",
        "    # Mobile optimization flags\n",
        "    \"optimize_for_mobile\": True,\n",
        "    \"target_device\": \"mobile\"\n",
        "}\n",
        "\n",
        "# Comet ML settings (optional)\n",
        "COMET_CONFIG = {\n",
        "    \"api_key\": os.getenv(\"COMET_API_KEY\"),\n",
        "    \"workspace\": os.getenv(\"COMET_WORKSPACE\", \"default\"),\n",
        "    \"project\": os.getenv(\"COMET_PROJECT\", \"gemma3n-sixth-grade\"),\n",
        "    \"experiment_name\": \"gemma3n_e2b_mobile_optimized\",\n",
        "    \"tags\": [\n",
        "        \"gemma3n-e2b\",\n",
        "        \"multimodal\",\n",
        "        \"math-tutor\",\n",
        "        \"vietnamese\",\n",
        "        \"sixth-grade\",\n",
        "        \"mobile-optimized\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"üì± Model ƒë∆∞·ª£c ch·ªçn: {CONFIG['model_name']} (t·ªëi ∆∞u cho mobile)\")\n",
        "print(f\"üìä Max sequence length: {CONFIG['max_seq_length']}\")\n",
        "print(f\"üéØ Target: {CONFIG['target_device']} deployment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === IMAGE PROCESSING UTILITIES ===\n",
        "\n",
        "def url_to_image(url: str, timeout: int = 10) -> Optional[Image.Image]:\n",
        "    \"\"\"Download v√† convert URL th√†nh PIL Image.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "        return image\n",
        "    except (requests.exceptions.RequestException, IOError) as e:\n",
        "        print(f\"Failed to load image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_image_urls_from_markdown(text: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Extract image URLs t·ª´ markdown text v√† thay th·∫ø b·∫±ng placeholders.\"\"\"\n",
        "    # Pattern cho markdown images: ![alt](url)\n",
        "    image_pattern = r\"!\\[.*?\\]\\((.*?)\\)\"\n",
        "    image_urls = re.findall(image_pattern, text)\n",
        "    # Remove image markdown syntax\n",
        "    cleaned_text = re.sub(image_pattern, \" [IMAGE] \", text).strip()\n",
        "    return cleaned_text, image_urls\n",
        "\n",
        "def process_markdown_for_model(text: str) -> Tuple[str, List[Image.Image]]:\n",
        "    \"\"\"Process markdown text ƒë·ªÉ extract text v√† images cho multimodal model.\"\"\"\n",
        "    cleaned_text, image_urls = extract_image_urls_from_markdown(text)\n",
        "    # Download images\n",
        "    images = []\n",
        "    for url in image_urls:\n",
        "        image = url_to_image(url)\n",
        "        if image:\n",
        "            images.append(image)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Failed to load image from {url}\")\n",
        "    return cleaned_text, images\n",
        "\n",
        "print(\"‚úÖ Image processing utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === DATASET PROCESSING ===\n",
        "\n",
        "def create_conversation_content(text: str, images: List[Image.Image]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"T·∫°o conversation content list v·ªõi text v√† images.\"\"\"\n",
        "    content = []\n",
        "    # Add text content n·∫øu c√≥\n",
        "    if text.strip():\n",
        "        content.append({\"type\": \"text\", \"text\": text.strip()})\n",
        "    # Add images\n",
        "    for image in images:\n",
        "        content.append({\"type\": \"image\", \"image\": image})\n",
        "    # Ensure c√≥ √≠t nh·∫•t m·ªôt content item\n",
        "    if not content:\n",
        "        content.append({\"type\": \"text\", \"text\": \"\"})\n",
        "    return content\n",
        "\n",
        "def process_math_sample(sample: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"Process m·ªôt math problem sample th√†nh conversation format.\n",
        "    \n",
        "    FIXED: Dataset c√≥ structure ['question', 'solution'] - revert l·∫°i\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process question\n",
        "        question_text, question_images = process_markdown_for_model(sample[\"question\"])\n",
        "        user_content = create_conversation_content(question_text, question_images)\n",
        "        \n",
        "        # Process solution (REVERTED: dataset th·ª±c s·ª± c√≥ 'solution' ch·ª© kh√¥ng ph·∫£i 'answer')\n",
        "        solution_text, solution_images = process_markdown_for_model(sample[\"solution\"])\n",
        "        assistant_content = create_conversation_content(solution_text, solution_images)\n",
        "        \n",
        "        # Create conversation\n",
        "        conversations = [\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ]\n",
        "        \n",
        "        return {\"conversations\": conversations}\n",
        "        \n",
        "    except KeyError as e:\n",
        "        print(f\"‚ùå Missing key in sample: {e}\")\n",
        "        print(f\"Available keys: {list(sample.keys())}\")\n",
        "        # Fallback v·ªõi empty content\n",
        "        return {\n",
        "            \"conversations\": [\n",
        "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Error processing sample\"}]},\n",
        "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Sorry, I cannot process this question.\"}]}\n",
        "            ]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing sample: {e}\")\n",
        "        return {\n",
        "            \"conversations\": [\n",
        "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Error processing sample\"}]},\n",
        "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Sorry, I cannot process this question.\"}]}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "def prepare_dataset(dataset_name: str, split: str, max_samples: Optional[int] = None) -> Dataset:\n",
        "    \"\"\"Load v√† prepare math dataset v·ªõi improved error handling.\"\"\"\n",
        "    print(f\"üì• Loading dataset: {dataset_name}, split: {split}\")\n",
        "    \n",
        "    try:\n",
        "        # Load dataset\n",
        "        raw_dataset = load_dataset(dataset_name, split=split)\n",
        "        \n",
        "        if max_samples:\n",
        "            raw_dataset = raw_dataset.select(range(min(max_samples, len(raw_dataset))))\n",
        "        \n",
        "        print(f\"üìä Dataset size: {len(raw_dataset)}\")\n",
        "        \n",
        "        # Ki·ªÉm tra structure c·ªßa dataset\n",
        "        if len(raw_dataset) > 0:\n",
        "            sample_keys = list(raw_dataset[0].keys())\n",
        "            print(f\"üìã Dataset columns: {sample_keys}\")\n",
        "            \n",
        "            # Verify expected keys\n",
        "            required_keys = [\"question\", \"answer\"]\n",
        "            missing_keys = [key for key in required_keys if key not in sample_keys]\n",
        "            if missing_keys:\n",
        "                print(f\"‚ö†Ô∏è Warning: Missing required keys: {missing_keys}\")\n",
        "                print(f\"Available keys: {sample_keys}\")\n",
        "        \n",
        "        print(f\"üîÑ Processing {len(raw_dataset)} samples...\")\n",
        "        processed_data = []\n",
        "        \n",
        "        for i, sample in enumerate(raw_dataset):\n",
        "            try:\n",
        "                processed_sample = process_math_sample(sample)\n",
        "                processed_data.append(processed_sample)\n",
        "                \n",
        "                if (i + 1) % 50 == 0:\n",
        "                    print(f\"‚úÖ Processed {i + 1}/{len(raw_dataset)} samples\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error processing sample {i}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"‚úÖ Successfully processed {len(processed_data)} samples\")\n",
        "        return Dataset.from_list(processed_data)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"‚úÖ Dataset processing functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SIMPLIFIED DATA COLLATOR ===\n",
        "\n",
        "class SimplifiedVisionDataCollator:\n",
        "    \"\"\"Simplified data collator cho multimodal data. D·ªÖ hi·ªÉu v√† maintain h∆°n.\"\"\"\n",
        "    \n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "        self.placeholder_image = None\n",
        "    \n",
        "    def _create_placeholder_image(self):\n",
        "        \"\"\"T·∫°o placeholder image cho text-only samples.\"\"\"\n",
        "        if self.placeholder_image is None:\n",
        "            self.placeholder_image = Image.new('RGB', (32, 32), color=(245, 245, 245))\n",
        "        return self.placeholder_image\n",
        "    \n",
        "    def _extract_images_from_conversation(self, conv):\n",
        "        \"\"\"Extract t·∫•t c·∫£ images t·ª´ conversation.\"\"\"\n",
        "        images = []\n",
        "        for message in conv:\n",
        "            for content in message.get(\"content\", []):\n",
        "                if content.get(\"type\") == \"image\" and \"image\" in content:\n",
        "                    img = content[\"image\"]\n",
        "                    if img and hasattr(img, 'convert'):\n",
        "                        try:\n",
        "                            img = img.convert('RGB')\n",
        "                            if img.size[0] > 0 and img.size[1] > 0:\n",
        "                                images.append(img)\n",
        "                        except Exception:\n",
        "                            continue\n",
        "        return images\n",
        "    \n",
        "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch c·ªßa examples.\"\"\"\n",
        "        try:\n",
        "            print(f\"üîÑ Processing batch of {len(examples)} examples...\")\n",
        "            \n",
        "            texts = []\n",
        "            images_list = []\n",
        "            \n",
        "            for idx, example in enumerate(examples):\n",
        "                conv = example[\"conversations\"]\n",
        "                \n",
        "                # Extract images\n",
        "                images = self._extract_images_from_conversation(conv)\n",
        "                \n",
        "                # Generate text using chat template\n",
        "                text = self.processor.apply_chat_template(\n",
        "                    conv, tokenize=False, add_generation_prompt=False\n",
        "                )\n",
        "                \n",
        "                # Handle image tokens\n",
        "                image_token_count = text.count('<image>')\n",
        "                actual_image_count = len(images)\n",
        "                \n",
        "                # Ensure consistency gi·ªØa image tokens v√† actual images\n",
        "                if actual_image_count == 0:\n",
        "                    # Text-only: add placeholder image v√† token n·∫øu c·∫ßn\n",
        "                    if image_token_count == 0:\n",
        "                        # Add m·ªôt image token ·ªü ƒë·∫ßu user message\n",
        "                        text = text.replace('<|user|>', '<|user|>\\\\n<image>')\n",
        "                    images = [self._create_placeholder_image()]\n",
        "                elif image_token_count < actual_image_count:\n",
        "                    # Truncate images\n",
        "                    images = images[:max(1, image_token_count)]\n",
        "                elif image_token_count > actual_image_count:\n",
        "                    # Add placeholder images\n",
        "                    while len(images) < image_token_count:\n",
        "                        images.append(self._create_placeholder_image())\n",
        "                \n",
        "                texts.append(text)\n",
        "                images_list.append(images)\n",
        "                \n",
        "                print(f\"Example {idx}: {len(images)} images, {text.count('<image>')} tokens\")\n",
        "            \n",
        "            # Process v·ªõi processor\n",
        "            print(\"üì§ Sending to processor...\")\n",
        "            batch = self.processor(\n",
        "                text=texts,\n",
        "                images=images_list,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=CONFIG[\"max_seq_length\"]\n",
        "            )\n",
        "            \n",
        "            # Create labels\n",
        "            labels = batch[\"input_ids\"].clone()\n",
        "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "            batch[\"labels\"] = labels\n",
        "            \n",
        "            print(f\"‚úÖ Batch created: {batch.keys()}\")\n",
        "            if \"pixel_values\" in batch:\n",
        "                print(f\"üì∏ pixel_values shape: {batch['pixel_values'].shape}\")\n",
        "            print(f\"üìù input_ids shape: {batch['input_ids'].shape}\")\n",
        "            \n",
        "            return batch\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in data collator: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise e\n",
        "\n",
        "print(\"‚úÖ Simplified data collator loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === MODEL SETUP V√Ä TRAINING FUNCTIONS ===\n",
        "\n",
        "def setup_model_and_processor(config: Dict[str, Any]):\n",
        "    \"\"\"Load v√† setup Gemma3N model v√† processor.\"\"\"\n",
        "    print(f\"ü§ñ Loading {config['model_name']} model and processor...\")\n",
        "    \n",
        "    try:\n",
        "        # Load model v√† processor\n",
        "        model, processor = FastVisionModel.from_pretrained(\n",
        "            config[\"model_name\"],\n",
        "            max_seq_length=config[\"max_seq_length\"],\n",
        "            load_in_4bit=config[\"load_in_4bit\"],\n",
        "            use_gradient_checkpointing=\"unsloth\" if config[\"use_gradient_checkpointing\"] else False,\n",
        "        )\n",
        "        \n",
        "        # Apply LoRA v·ªõi mobile-optimized settings\n",
        "        model = FastVisionModel.get_peft_model(\n",
        "            model,\n",
        "            finetune_vision_layers=True,\n",
        "            finetune_language_layers=True,\n",
        "            finetune_attention_modules=True,\n",
        "            finetune_mlp_modules=True,\n",
        "            r=config[\"lora_r\"],\n",
        "            lora_alpha=config[\"lora_alpha\"],\n",
        "            lora_dropout=config[\"lora_dropout\"],\n",
        "            bias=\"none\",\n",
        "            random_state=config[\"seed\"],\n",
        "            use_rslora=False,\n",
        "            target_modules=\"all-linear\",\n",
        "            modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
        "        )\n",
        "        \n",
        "        # Setup chat template\n",
        "        processor = get_chat_template(processor, \"gemma-3n\")\n",
        "        \n",
        "        print(\"‚úÖ Model and processor setup complete!\")\n",
        "        return model, processor\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up model: {e}\")\n",
        "        raise\n",
        "\n",
        "def setup_comet_ml(config: Dict[str, Any]) -> Optional[object]:\n",
        "    \"\"\"Setup Comet ML experiment tracking.\"\"\"\n",
        "    if not COMET_AVAILABLE or config[\"report_to\"] != \"comet_ml\":\n",
        "        print(\"üìä Using tensorboard for logging\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        experiment = comet_ml.Experiment(\n",
        "            workspace=COMET_CONFIG.get(\"workspace\"),\n",
        "            project_name=COMET_CONFIG.get(\"project\"),\n",
        "            auto_metric_logging=True,\n",
        "            auto_param_logging=True,\n",
        "        )\n",
        "        \n",
        "        # Log configuration\n",
        "        experiment.log_parameters(config)\n",
        "        \n",
        "        # Add tags\n",
        "        for tag in COMET_CONFIG.get(\"tags\", []):\n",
        "            experiment.add_tag(tag)\n",
        "        \n",
        "        print(f\"‚úÖ Comet ML experiment initialized: {experiment.url}\")\n",
        "        return experiment\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to initialize Comet ML: {e}\")\n",
        "        print(\"üìä Falling back to tensorboard\")\n",
        "        config[\"report_to\"] = \"tensorboard\"\n",
        "        return None\n",
        "\n",
        "def create_trainer(model, processor, train_dataset, config: Dict[str, Any]):\n",
        "    \"\"\"Create mobile-optimized SFTTrainer.\"\"\"\n",
        "    # Enable training\n",
        "    FastVisionModel.for_training(model)\n",
        "    \n",
        "    # Create simplified data collator\n",
        "    data_collator = SimplifiedVisionDataCollator(processor)\n",
        "    \n",
        "    # Mobile-optimized training arguments\n",
        "    training_args = SFTConfig(\n",
        "        # Basic training settings\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        max_steps=config[\"max_steps\"],\n",
        "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
        "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "        \n",
        "        # Optimization settings\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        warmup_ratio=config[\"warmup_ratio\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        optim=\"adamw_torch_fused\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        \n",
        "        # Memory optimization cho mobile\n",
        "        gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False} if config[\"use_gradient_checkpointing\"] else {},\n",
        "        max_grad_norm=0.3,\n",
        "        dataloader_pin_memory=False,  # T·ªëi ∆∞u cho mobile\n",
        "        \n",
        "        # Logging v√† saving\n",
        "        logging_steps=config[\"logging_steps\"],\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=config[\"save_steps\"],\n",
        "        report_to=config[\"report_to\"],\n",
        "        \n",
        "        # Vision-specific settings\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        max_length=config[\"max_seq_length\"],\n",
        "        \n",
        "        # Reproducibility\n",
        "        seed=config[\"seed\"],\n",
        "        \n",
        "        # Mobile optimization\n",
        "        fp16=True,  # Use fp16 cho mobile efficiency\n",
        "        dataloader_num_workers=1,  # Gi·∫£m workers\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        processing_class=processor.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "    )\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "print(\"‚úÖ Training functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TRAINING PIPELINE ===\n",
        "\n",
        "print(\"üöÄ STARTING GEMMA 3N MOBILE-OPTIMIZED TRAINING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Create output directory\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "print(f\"üìÅ Output directory: {CONFIG['output_dir']}\")\n",
        "\n",
        "# 2. Setup Comet ML (optional)\n",
        "comet_experiment = setup_comet_ml(CONFIG)\n",
        "\n",
        "# 3. Setup model v√† processor\n",
        "print(\"\\nüöÄ Step 1: Setting up model and processor...\")\n",
        "model, processor = setup_model_and_processor(CONFIG)\n",
        "\n",
        "# 4. Prepare dataset\n",
        "print(\"\\nüöÄ Step 2: Preparing dataset...\")\n",
        "# Load m·ªôt subset nh·ªè ƒë·ªÉ test tr∆∞·ªõc (c√≥ th·ªÉ thay ƒë·ªïi max_samples=None ƒë·ªÉ load to√†n b·ªô)\n",
        "train_dataset = prepare_dataset(\n",
        "    CONFIG[\"dataset_name\"], \n",
        "    CONFIG[\"train_split\"],\n",
        "    max_samples=50  # Test v·ªõi 50 samples tr∆∞·ªõc, sau ƒë√≥ c√≥ th·ªÉ b·ªè parameter n√†y\n",
        ")\n",
        "\n",
        "# Dataset statistics\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   - Training samples: {len(train_dataset)}\")\n",
        "\n",
        "# Count samples v·ªõi images\n",
        "samples_with_images = 0\n",
        "for sample in train_dataset:\n",
        "    for conv in sample[\"conversations\"]:\n",
        "        for content in conv.get(\"content\", []):\n",
        "            if content.get(\"type\") == \"image\":\n",
        "                samples_with_images += 1\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "\n",
        "print(f\"   - Samples with images: {samples_with_images}\")\n",
        "print(f\"   - Text-only samples: {len(train_dataset) - samples_with_images}\")\n",
        "\n",
        "# 5. Create trainer\n",
        "print(\"\\nüöÄ Step 3: Creating trainer...\")\n",
        "trainer = create_trainer(model, processor, train_dataset, CONFIG)\n",
        "\n",
        "# 6. Test data collator tr∆∞·ªõc khi training\n",
        "print(\"\\nüß™ Testing data collator v·ªõi sample batch...\")\n",
        "try:\n",
        "    test_samples = [train_dataset[i] for i in range(min(2, len(train_dataset)))]\n",
        "    test_batch = trainer.data_collator(test_samples)\n",
        "    print(\"‚úÖ Data collator test passed!\")\n",
        "    print(f\"   Batch keys: {list(test_batch.keys())}\")\n",
        "    for key, value in test_batch.items():\n",
        "        if hasattr(value, 'shape'):\n",
        "            print(f\"   {key}: {value.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data collator test failed: {e}\")\n",
        "    print(\"Please check the dataset format and try again.\")\n",
        "    raise\n",
        "\n",
        "# 7. Start training\n",
        "print(\"\\nüöÄ Step 4: Starting training...\")\n",
        "print(f\"   üì± Model: {CONFIG['model_name']} (optimized for mobile)\")\n",
        "print(f\"   üìÅ Output directory: {CONFIG['output_dir']}\")\n",
        "print(f\"   üéØ Max steps: {CONFIG['max_steps']}\")\n",
        "print(f\"   üì¶ Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
        "print(f\"   üîÑ Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"   üí™ Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    trainer_stats = trainer.train()\n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "    print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 8. Save model cho mobile deployment\n",
        "print(\"\\nüíæ Saving model for mobile deployment...\")\n",
        "\n",
        "try:\n",
        "    # Save LoRA adapters\n",
        "    lora_save_path = f\"{CONFIG['output_dir']}/lora_adapters\"\n",
        "    model.save_pretrained(lora_save_path)\n",
        "    processor.tokenizer.save_pretrained(lora_save_path)\n",
        "    print(f\"‚úÖ LoRA adapters saved to: {lora_save_path}\")\n",
        "    \n",
        "    # Save merged model for inference\n",
        "    merged_save_path = f\"{CONFIG['output_dir']}/merged_model\"\n",
        "    model.save_pretrained_merged(merged_save_path, processor.tokenizer, save_method=\"merged_16bit\")\n",
        "    print(f\"‚úÖ Merged model saved to: {merged_save_path}\")\n",
        "    \n",
        "    # Save mobile-optimized GGUF format (optional)\n",
        "    try:\n",
        "        gguf_save_path = f\"{CONFIG['output_dir']}/gguf_model\"\n",
        "        model.save_pretrained_gguf(gguf_save_path, processor.tokenizer, quantization_method=\"q4_k_m\")\n",
        "        print(f\"‚úÖ GGUF model saved to: {gguf_save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è GGUF export failed (optional): {e}\")\n",
        "    \n",
        "    print(\"\\nüéâ All models saved successfully!\")\n",
        "    print(f\"üì± Ready for mobile deployment!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving models: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 9. Test model sau khi training\n",
        "print(\"\\nüß™ Testing fine-tuned model...\")\n",
        "\n",
        "try:\n",
        "    # Enable inference mode\n",
        "    FastVisionModel.for_inference(model)\n",
        "    \n",
        "    # Test prompt\n",
        "    test_prompt = \"\"\"\n",
        "<|user|>\n",
        "B√†i to√°n: M·ªôt h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 12cm v√† chi·ªÅu r·ªông 8cm. T√≠nh di·ªán t√≠ch c·ªßa h√¨nh ch·ªØ nh·∫≠t n√†y.\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = processor.tokenizer(\n",
        "        test_prompt.strip(),\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=CONFIG[\"max_seq_length\"]\n",
        "    )\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(\"üìù Test Input:\")\n",
        "    print(test_prompt.strip())\n",
        "    print(\"\\nü§ñ Model Response:\")\n",
        "    print(response[len(test_prompt.strip()):])\n",
        "    \n",
        "    print(\"\\n‚úÖ Model test completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üì± Model: {CONFIG['model_name']} (Mobile Optimized)\")\n",
        "print(f\"üìä Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"üéØ Training steps: {CONFIG['max_steps']}\")\n",
        "print(f\"üíæ Output directory: {CONFIG['output_dir']}\")\n",
        "print(f\"üîß LoRA rank: {CONFIG['lora_r']}\")\n",
        "print(f\"üìè Max sequence length: {CONFIG['max_seq_length']}\")\n",
        "\n",
        "print(\"\\nüìÅ Saved Models:\")\n",
        "print(f\"   - LoRA adapters: {CONFIG['output_dir']}/lora_adapters\")\n",
        "print(f\"   - Merged model: {CONFIG['output_dir']}/merged_model\")\n",
        "print(f\"   - GGUF model: {CONFIG['output_dir']}/gguf_model (if available)\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"   1. Test model v·ªõi real data\")\n",
        "print(\"   2. Convert to mobile format (TensorFlow Lite, Core ML, etc.)\")\n",
        "print(\"   3. Deploy to mobile app\")\n",
        "print(\"   4. Optimize inference performance\")\n",
        "print(\"   5. Add evaluation metrics\")\n",
        "\n",
        "print(\"\\nüí° Mobile Deployment Tips:\")\n",
        "print(\"   - S·ª≠ d·ª•ng GGUF format cho llama.cpp\")\n",
        "print(\"   - Consider quantization ƒë·ªÉ gi·∫£m model size\")\n",
        "print(\"   - Test tr√™n target mobile devices\")\n",
        "print(\"   - Monitor memory usage v√† inference speed\")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cleanup Comet experiment\n",
        "if comet_experiment:\n",
        "    try:\n",
        "        comet_experiment.end()\n",
        "        print(\"üìä Comet ML experiment ended\")\n",
        "    except:\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
