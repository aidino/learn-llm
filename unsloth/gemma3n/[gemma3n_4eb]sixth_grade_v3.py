# -*- coding: utf-8 -*-
"""[Gemma3n-4eb]sixth-grade_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EYKRLrlEVls89pQwCkrN4mRBZ5lN4z0m
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Install latest transformers for Gemma 3N
# !pip install --no-deps --upgrade timm # Only for Gemma 3N
# !pip install comet-ml

"""## Load model"""

from unsloth import FastVisionModel # FastLanguageModel for LLMs
import torch

model, processor = FastVisionModel.from_pretrained(
    "unsloth/gemma-3n-E4B",
    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for long context
)

model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers     = True, # False if not finetuning vision layers
    finetune_language_layers   = True, # False if not finetuning language layers
    finetune_attention_modules = True, # False if not finetuning attention layers
    finetune_mlp_modules       = True, # False if not finetuning MLP layers

    r = 32,                           # The larger, the higher the accuracy, but might overfit
    lora_alpha = 32,                  # Recommended alpha == r at least
    lora_dropout = 0,
    bias = "none",
    random_state = 3407,
    use_rslora = False,               # We support rank stabilized LoRA
    loftq_config = None,               # And LoftQ
    target_modules = "all-linear",    # Optional now! Can specify a list if needed
    modules_to_save=[
        "lm_head",
        "embed_tokens",
    ],
)

"""## Data preparation"""

import re
import requests
from PIL import Image
from io import BytesIO
from typing import Tuple, List, Dict, Any, Optional

def url_to_image(url: str) -> Optional[Image.Image]:
    """
    Tải ảnh từ một URL và chuyển đổi nó thành đối tượng PIL.Image.

    Args:
        url: Đường dẫn URL của hình ảnh.

    Returns:
        Một đối tượng PIL.Image nếu thành công, ngược lại trả về None.
    """
    try:
        # Gửi yêu cầu GET để tải nội dung của ảnh
        response = requests.get(url, timeout=10)
        # Kiểm tra nếu yêu cầu thành công (status code 200)
        response.raise_for_status()

        # Đọc nội dung ảnh từ response và mở bằng Pillow
        image = Image.open(BytesIO(response.content)).convert("RGB")
        return image
    except requests.exceptions.RequestException as e:
        print(f"Lỗi khi tải ảnh từ URL {url}: {e}")
        return None
    except IOError as e:
        print(f"Lỗi khi mở file ảnh từ URL {url}: {e}")
        return None

def extract_and_replace_images(markdown_text: str) -> Tuple[str, List[str]]:
    """
    Tách các đường link ảnh từ văn bản Markdown, thay thế chúng bằng thẻ <image>
    và trả về văn bản đã xử lý cùng với danh sách các link ảnh.
    """
    image_pattern = r"!\[.*?\]\((.*?)\)"
    image_urls = re.findall(image_pattern, markdown_text)
    processed_text = re.sub(image_pattern, " ", markdown_text)
    return processed_text, image_urls

def process_markdown_for_model(markdown_text: str) -> Dict[str, Any]:
    """
    Xử lý toàn bộ văn bản markdown để chuẩn bị cho model đa phương thức.

    Hàm này sẽ:
    1. Trích xuất URL ảnh và thay thế bằng thẻ <image>.
    2. Tải và chuyển đổi các URL thành danh sách đối tượng PIL.Image.

    Args:
        markdown_text: Chuỗi văn bản đầu vào.

    Returns:
        Một dictionary chứa:
        - 'text': văn bản đã xử lý.
        - 'images': danh sách các đối tượng PIL.Image.
    """
    # Bước 1: Trích xuất text và URLs
    cleaned_text, image_urls = extract_and_replace_images(markdown_text)

    # Bước 2: Chuyển đổi URLs thành đối tượng PIL.Image
    pil_images = []
    for url in image_urls:
        image_obj = url_to_image(url)
        if image_obj:
            pil_images.append(image_obj)

    return cleaned_text, pil_images

from datasets import load_dataset
train_dataset = load_dataset("ngohongthai/exam-sixth_grade-instruct-dataset", split = "train")
test_dataset = load_dataset("ngohongthai/exam-sixth_grade-instruct-dataset", split = "test")

def process_message(text, role):
  processed_text, images_data = process_markdown_for_model(text)
  user_content = [
      {"type": "text", "text": processed_text},
  ]
  if images_data is not None:
    # Kiểm tra xem images_data có phải là một list (mảng) hay không
    if isinstance(images_data, list):
      # Nếu là một danh sách ảnh, duyệt qua và thêm từng ảnh
      for image in images_data:
        user_content.append({"type": "image", "image": image})
    else:
      # Nếu không phải list, coi đó là một ảnh duy nhất và thêm vào
      user_content.append({"type": "image", "image": images_data})

  return {
          "role": role,
          "content": user_content,
      }

def process_conversation(sample):
  user_message = process_message(sample["question"], "user")
  assistant_message = process_message(sample["solution"], "assistant")
  conversation = [user_message, assistant_message]

  return {"conversations": conversation}

test_sample = test_dataset[6]
test_sample

test_conversation = process_conversation(test_sample)
test_conversation

converted_train_dataset = [process_conversation(sample) for sample in train_dataset]
converted_train_dataset[0]

converted_train_dataset[1]

from unsloth import get_chat_template

processor = get_chat_template(
    processor,
    "gemma-3n"
)

"""## Training"""

import os
from google.colab import userdata

# Load the Comet.ml API key from Colab secrets
comet_ml_api_key = userdata.get('COMET_API_KEY')

# Set the COMET_API_KEY environment variable
os.environ['COMET_API_KEY'] = comet_ml_api_key

from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

FastVisionModel.for_training(model) # Enable for training!

trainer = SFTTrainer(
    model=model,
    train_dataset=converted_train_dataset,
    processing_class=processor.tokenizer,
    data_collator=UnslothVisionDataCollator(model, processor),
    args = SFTConfig(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        gradient_checkpointing = True,

        # use reentrant checkpointing
        gradient_checkpointing_kwargs = {"use_reentrant": False},
        max_grad_norm = 0.3,              # max gradient norm based on QLoRA paper
        warmup_ratio = 0.03,
        max_steps = 60,
        #num_train_epochs = 2,          # Set this instead of max_steps for full training runs
        learning_rate = 2e-4,
        logging_steps = 1,
        save_strategy="steps",
        optim = "adamw_torch_fused",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine",
        seed = 3407,
        output_dir = "outputs",
        report_to = "comet_ml",             # For Weights and Biases

        # You MUST put the below items for vision finetuning:
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        max_length = 2048,
    )
)

trainer_stats = trainer.train()

