{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ü¶• Unsloth Gemma3N Fine-tuning - Optimized Version\n",
        "## Fine-tune Gemma3N for Vietnamese Math Tutoring (Sixth Grade)\n",
        "\n",
        "### Key Improvements:\n",
        "- ‚úÖ Simplified data processing\n",
        "- ‚úÖ Proper FastVisionModel usage  \n",
        "- ‚úÖ Optimized data collator\n",
        "- ‚úÖ Better error handling\n",
        "- ‚úÖ Unsloth best practices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth and dependencies\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    %pip install unsloth\n",
        "else:\n",
        "    # Colab environment\n",
        "    %pip install --no-deps --upgrade timm  # For Gemma 3N\n",
        "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton\n",
        "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    %pip install --no-deps unsloth\n",
        "    %pip install comet-ml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "from typing import Tuple, List, Dict, Any, Optional\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import comet_ml\n",
        "\n",
        "# Import Unsloth for vision models\n",
        "from unsloth import FastVisionModel, get_chat_template\n",
        "\n",
        "print(\"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\")\n",
        "print(\"üì¶ All dependencies loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚öôÔ∏è Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Optimized for Unsloth and Vietnamese Math Tutoring\n",
        "CONFIG = {\n",
        "    # Model settings\n",
        "    \"model_name\": \"unsloth/gemma-3n-E4B\",\n",
        "    \"max_seq_length\": 2048,\n",
        "    \"load_in_4bit\": True,\n",
        "    \n",
        "    # Dataset settings\n",
        "    \"dataset_name\": \"ngohongthai/exam-sixth_grade-instruct-dataset\",\n",
        "    \"train_split\": \"train\",\n",
        "    \n",
        "    # Training settings - Conservative for stability\n",
        "    \"output_dir\": \"mathpal-gemma3n/optimized_baseline\",\n",
        "    \"max_steps\": 100,  # Reduced for testing\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"logging_steps\": 5,\n",
        "    \"save_steps\": 25,\n",
        "    \n",
        "    # LoRA settings - Following Unsloth recommendations\n",
        "    \"lora_r\": 16,  # Reduced from 32 for stability\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.0,  # 0 is optimized for Unsloth\n",
        "    \n",
        "    # System settings\n",
        "    \"use_gradient_checkpointing\": \"unsloth\",  # Use Unsloth's optimized version\n",
        "    \"report_to\": None,  # Disable for now\n",
        "    \"seed\": 42,\n",
        "    \n",
        "    # Image processing\n",
        "    \"max_images_per_sample\": 3,  # Limit for memory efficiency\n",
        "    \"image_timeout\": 5,  # Faster timeout for unreachable images\n",
        "}\n",
        "\n",
        "print(f\"üîß Configuration loaded:\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"   Max steps: {CONFIG['max_steps']}\")\n",
        "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified Data Processing Functions\n",
        "def download_image_safe(url: str, timeout: int = 5) -> Optional[Image.Image]:\n",
        "    \"\"\"Safely download image with timeout and error handling.\"\"\"\n",
        "    try:\n",
        "        if not url or not url.startswith(('http://', 'https://')):\n",
        "            return None\n",
        "            \n",
        "        response = requests.get(url, timeout=timeout, stream=True)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        content_type = response.headers.get('content-type', '')\n",
        "        if not content_type.startswith('image/'):\n",
        "            return None\n",
        "            \n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "        \n",
        "        # Validate image size\n",
        "        if image.size[0] < 10 or image.size[1] < 10:\n",
        "            return None\n",
        "            \n",
        "        return image\n",
        "        \n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_images_from_markdown(text: str, max_images: int = 3) -> Tuple[str, List[Image.Image]]:\n",
        "    \"\"\"Extract images from markdown text and clean the text.\"\"\"\n",
        "    image_pattern = r\"!\\[.*?\\]\\((.*?)\\)\"\n",
        "    image_urls = re.findall(image_pattern, text)\n",
        "    \n",
        "    # Limit number of images for memory efficiency\n",
        "    image_urls = image_urls[:max_images]\n",
        "    \n",
        "    # Remove image markdown syntax and clean text\n",
        "    cleaned_text = re.sub(image_pattern, \" [IMAGE] \", text)\n",
        "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "    \n",
        "    # Download images\n",
        "    images = []\n",
        "    for url in image_urls:\n",
        "        image = download_image_safe(url, timeout=CONFIG[\"image_timeout\"])\n",
        "        if image:\n",
        "            images.append(image)\n",
        "    \n",
        "    return cleaned_text, images\n",
        "\n",
        "def process_sample_to_conversation(sample: Dict[str, str]) -> Dict[str, Any]:\n",
        "    \"\"\"Convert a dataset sample to conversation format.\"\"\"\n",
        "    # Process question and solution\n",
        "    question_text, question_images = extract_images_from_markdown(\n",
        "        sample[\"question\"], max_images=CONFIG[\"max_images_per_sample\"]\n",
        "    )\n",
        "    solution_text, solution_images = extract_images_from_markdown(\n",
        "        sample[\"solution\"], max_images=CONFIG[\"max_images_per_sample\"]\n",
        "    )\n",
        "    \n",
        "    # Create user message content\n",
        "    user_content = [{\"type\": \"text\", \"text\": question_text}]\n",
        "    for img in question_images:\n",
        "        user_content.append({\"type\": \"image\", \"image\": img})\n",
        "    \n",
        "    # Create assistant message content\n",
        "    assistant_content = [{\"type\": \"text\", \"text\": solution_text}]\n",
        "    for img in solution_images:\n",
        "        assistant_content.append({\"type\": \"image\", \"image\": img})\n",
        "    \n",
        "    # Create conversation in chat format\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "    ]\n",
        "    \n",
        "    return {\"messages\": messages}\n",
        "\n",
        "print(\"‚úÖ Data processing functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified Data Collator for Vision-Language Models\n",
        "class OptimizedVisionDataCollator:\n",
        "    \"\"\"Simplified data collator that handles both text-only and multimodal samples.\"\"\"\n",
        "    \n",
        "    def __init__(self, processor, max_length: int = 2048):\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.placeholder_image = None\n",
        "    \n",
        "    def _get_placeholder_image(self):\n",
        "        \"\"\"Create a small placeholder image for text-only samples.\"\"\"\n",
        "        if self.placeholder_image is None:\n",
        "            self.placeholder_image = Image.new('RGB', (32, 32), color=(240, 240, 240))\n",
        "        return self.placeholder_image\n",
        "    \n",
        "    def _extract_content(self, messages: List[Dict]) -> Tuple[str, List[Image.Image]]:\n",
        "        \"\"\"Extract text and images from messages.\"\"\"\n",
        "        images = []\n",
        "        \n",
        "        # Apply chat template to get formatted text\n",
        "        try:\n",
        "            formatted_text = self.processor.apply_chat_template(\n",
        "                messages, tokenize=False, add_generation_prompt=False\n",
        "            )\n",
        "        except Exception:\n",
        "            # Fallback to simple formatting\n",
        "            formatted_text = \"\"\n",
        "            for msg in messages:\n",
        "                role = msg.get(\"role\", \"\")\n",
        "                content = msg.get(\"content\", [])\n",
        "                for item in content:\n",
        "                    if item.get(\"type\") == \"text\":\n",
        "                        formatted_text += f\"{role}: {item.get('text', '')}\\\\n\"\n",
        "        \n",
        "        # Extract images from all messages\n",
        "        for message in messages:\n",
        "            for content_item in message.get(\"content\", []):\n",
        "                if content_item.get(\"type\") == \"image\":\n",
        "                    img = content_item.get(\"image\")\n",
        "                    if img and hasattr(img, 'convert'):\n",
        "                        images.append(img.convert('RGB'))\n",
        "        \n",
        "        # Ensure we have at least one image for the processor\n",
        "        if not images:\n",
        "            images = [self._get_placeholder_image()]\n",
        "            # Add image token to text if not present\n",
        "            if '<image>' not in formatted_text:\n",
        "                formatted_text = '<image>\\\\n' + formatted_text\n",
        "        \n",
        "        return formatted_text, images\n",
        "    \n",
        "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate a batch of examples.\"\"\"\n",
        "        batch_texts = []\n",
        "        batch_images = []\n",
        "        \n",
        "        for example in examples:\n",
        "            messages = example.get(\"messages\", [])\n",
        "            text, images = self._extract_content(messages)\n",
        "            batch_texts.append(text)\n",
        "            batch_images.append(images)\n",
        "        \n",
        "        try:\n",
        "            # Process with the vision processor\n",
        "            batch = self.processor(\n",
        "                text=batch_texts,\n",
        "                images=batch_images,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length\n",
        "            )\n",
        "            \n",
        "            # Create labels for training\n",
        "            if \"input_ids\" in batch:\n",
        "                labels = batch[\"input_ids\"].clone()\n",
        "                # Mask padding tokens\n",
        "                if hasattr(self.processor, 'tokenizer') and hasattr(self.processor.tokenizer, 'pad_token_id'):\n",
        "                    labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "                batch[\"labels\"] = labels\n",
        "            \n",
        "            return batch\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in data collator: {e}\")\n",
        "            # Return minimal valid batch\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor([[0]]),\n",
        "                \"labels\": torch.tensor([[-100]])\n",
        "            }\n",
        "\n",
        "print(\"‚úÖ Optimized data collator loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Setup with FastVisionModel - Optimized for Gemma3N\n",
        "def setup_gemma3n_model(config: Dict[str, Any]):\n",
        "    \"\"\"Setup Gemma3N model using FastVisionModel.\"\"\"\n",
        "    print(\"üîß Loading Gemma3N model with FastVisionModel...\")\n",
        "    \n",
        "    # Load model and processor\n",
        "    model, processor = FastVisionModel.from_pretrained(\n",
        "        config[\"model_name\"],\n",
        "        max_seq_length=config[\"max_seq_length\"],\n",
        "        load_in_4bit=config[\"load_in_4bit\"],\n",
        "        use_gradient_checkpointing=config[\"use_gradient_checkpointing\"]\n",
        "    )\n",
        "    \n",
        "    print(\"üéØ Applying PEFT (LoRA) configuration...\")\n",
        "    \n",
        "    # Apply LoRA with Unsloth optimizations\n",
        "    model = FastVisionModel.get_peft_model(\n",
        "        model,\n",
        "        # Vision and language layers\n",
        "        finetune_vision_layers=True,\n",
        "        finetune_language_layers=True,\n",
        "        finetune_attention_modules=True,\n",
        "        finetune_mlp_modules=True,\n",
        "        \n",
        "        # LoRA settings - optimized for Unsloth\n",
        "        r=config[\"lora_r\"],\n",
        "        lora_alpha=config[\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_dropout\"],  # 0 is optimized\n",
        "        bias=\"none\",  # \"none\" is optimized\n",
        "        \n",
        "        # Unsloth optimizations\n",
        "        use_gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "        random_state=config[\"seed\"],\n",
        "        use_rslora=False,  # Disabled for stability\n",
        "        \n",
        "        # Target modules - let Unsloth decide for vision models\n",
        "        target_modules=\"all-linear\",\n",
        "        modules_to_save=[\"lm_head\", \"embed_tokens\"]\n",
        "    )\n",
        "    \n",
        "    # Setup chat template for Gemma3N\n",
        "    processor = get_chat_template(processor, \"gemma-3n\")\n",
        "    \n",
        "    print(\"‚úÖ Model and processor setup complete!\")\n",
        "    print(f\"   Model type: {type(model).__name__}\")\n",
        "    print(f\"   Processor type: {type(processor).__name__}\")\n",
        "    \n",
        "    return model, processor\n",
        "\n",
        "# Prepare dataset function\n",
        "def prepare_dataset_optimized(dataset_name: str, split: str) -> Dataset:\n",
        "    \"\"\"Load and prepare the dataset with progress tracking.\"\"\"\n",
        "    print(f\"üì• Loading dataset: {dataset_name}, split: {split}\")\n",
        "    raw_dataset = load_dataset(dataset_name, split=split)\n",
        "    \n",
        "    print(f\"üîÑ Processing {len(raw_dataset)} samples...\")\n",
        "    \n",
        "    processed_data = []\n",
        "    errors = 0\n",
        "    \n",
        "    for i, sample in enumerate(raw_dataset):\n",
        "        try:\n",
        "            processed_sample = process_sample_to_conversation(sample)\n",
        "            processed_data.append(processed_sample)\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            if errors <= 5:  # Log first 5 errors\n",
        "                print(f\"‚ö†Ô∏è Error processing sample {i}: {e}\")\n",
        "        \n",
        "        # Progress update\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"   Processed {i + 1}/{len(raw_dataset)} samples (errors: {errors})\")\n",
        "    \n",
        "    success_rate = (len(processed_data) / len(raw_dataset)) * 100\n",
        "    print(f\"‚úÖ Successfully processed {len(processed_data)}/{len(raw_dataset)} samples ({success_rate:.1f}%)\")\n",
        "    \n",
        "    if errors > 0:\n",
        "        print(f\"‚ö†Ô∏è {errors} samples failed to process\")\n",
        "    \n",
        "    return Dataset.from_list(processed_data)\n",
        "\n",
        "print(\"‚úÖ Model setup and dataset functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Setup - Optimized for Unsloth\n",
        "def create_optimized_trainer(model, processor, train_dataset, config: Dict[str, Any]):\n",
        "    \"\"\"Create optimized SFTTrainer with Unsloth best practices.\"\"\"\n",
        "    print(\"üîß Creating optimized trainer...\")\n",
        "    \n",
        "    # Enable training mode\n",
        "    FastVisionModel.for_training(model)\n",
        "    \n",
        "    # Create data collator\n",
        "    data_collator = OptimizedVisionDataCollator(\n",
        "        processor, \n",
        "        max_length=config[\"max_seq_length\"]\n",
        "    )\n",
        "    \n",
        "    # Training arguments optimized for Unsloth\n",
        "    training_args = SFTConfig(\n",
        "        # Basic settings\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        max_steps=config[\"max_steps\"],\n",
        "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
        "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "        \n",
        "        # Optimization\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        warmup_ratio=config[\"warmup_ratio\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        \n",
        "        # Unsloth optimized settings\n",
        "        optim=\"adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        \n",
        "        # Memory optimization\n",
        "        gradient_checkpointing=True,\n",
        "        dataloader_pin_memory=False,  # Can cause issues with images\n",
        "        max_grad_norm=0.3,\n",
        "        \n",
        "        # Logging and saving\n",
        "        logging_steps=config[\"logging_steps\"],\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=config[\"save_steps\"],\n",
        "        report_to=config[\"report_to\"],\n",
        "        \n",
        "        # Vision-specific settings\n",
        "        remove_unused_columns=False,  # Important for vision models\n",
        "        dataset_text_field=\"\",  # We handle formatting in collator\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        \n",
        "        # Reproducibility\n",
        "        seed=config[\"seed\"],\n",
        "        \n",
        "        # Stability\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported()\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        processing_class=processor.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Optimized trainer created successfully!\")\n",
        "    return trainer\n",
        "\n",
        "print(\"‚úÖ Training setup functions loaded\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Execute Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create output directory and load model\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "print(f\"üìÅ Output directory: {CONFIG['output_dir']}\")\n",
        "\n",
        "# Load model and processor\n",
        "model, processor = setup_gemma3n_model(CONFIG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Prepare dataset\n",
        "train_dataset = prepare_dataset_optimized(CONFIG[\"dataset_name\"], CONFIG[\"train_split\"])\n",
        "\n",
        "# Dataset statistics\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Total samples: {len(train_dataset)}\")\n",
        "\n",
        "# Count multimodal vs text-only samples\n",
        "multimodal_count = 0\n",
        "for sample in train_dataset[:100]:  # Check first 100 for speed\n",
        "    messages = sample.get(\"messages\", [])\n",
        "    has_image = any(\n",
        "        content.get(\"type\") == \"image\" \n",
        "        for msg in messages \n",
        "        for content in msg.get(\"content\", [])\n",
        "    )\n",
        "    if has_image:\n",
        "        multimodal_count += 1\n",
        "\n",
        "estimated_multimodal = (multimodal_count / min(100, len(train_dataset))) * len(train_dataset)\n",
        "print(f\"   Estimated multimodal samples: {estimated_multimodal:.0f}\")\n",
        "print(f\"   Estimated text-only samples: {len(train_dataset) - estimated_multimodal:.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Test data collator\n",
        "print(\"üß™ Testing data collator...\")\n",
        "try:\n",
        "    test_collator = OptimizedVisionDataCollator(processor, max_length=CONFIG[\"max_seq_length\"])\n",
        "    test_batch = test_collator([train_dataset[0]])\n",
        "    \n",
        "    print(\"‚úÖ Data collator test passed!\")\n",
        "    print(f\"   Batch keys: {list(test_batch.keys())}\")\n",
        "    for key, value in test_batch.items():\n",
        "        if hasattr(value, 'shape'):\n",
        "            print(f\"   {key}: {value.shape}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data collator test failed: {e}\")\n",
        "    print(\"Please check the data format and try again.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Create trainer and start training\n",
        "trainer = create_optimized_trainer(model, processor, train_dataset, CONFIG)\n",
        "\n",
        "print(f\"\\nüöÄ Starting optimized training...\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"   Max steps: {CONFIG['max_steps']}\")\n",
        "print(f\"   Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
        "print(f\"   Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"   LoRA rank: {CONFIG['lora_r']}\")\n",
        "\n",
        "try:\n",
        "    trainer_stats = trainer.train()\n",
        "    \n",
        "    print(\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    # Clean up GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Save model\n",
        "print(\"üíæ Saving trained model...\")\n",
        "\n",
        "try:\n",
        "    # Save LoRA adapters\n",
        "    model.save_pretrained_merged(\n",
        "        CONFIG[\"output_dir\"], \n",
        "        processor.tokenizer, \n",
        "        save_method=\"lora\"\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Model saved to {CONFIG['output_dir']}\")\n",
        "    \n",
        "    # Optionally save to HuggingFace Hub\n",
        "    # model.push_to_hub_merged(\n",
        "    #     \"your-username/gemma3n-math-tutor\", \n",
        "    #     processor.tokenizer, \n",
        "    #     save_method=\"lora\",\n",
        "    #     private=True\n",
        "    # )\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to save model: {e}\")\n",
        "\n",
        "print(\"\\nüéâ Fine-tuning completed successfully!\")\n",
        "print(f\"üìÅ Model artifacts saved in: {CONFIG['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Test inference with the trained model\n",
        "print(\"üîÆ Testing inference...\")\n",
        "\n",
        "try:\n",
        "    # Enable inference mode\n",
        "    FastVisionModel.for_inference(model)\n",
        "    \n",
        "    # Test prompt\n",
        "    test_messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Gi·∫£i b√†i to√°n sau: M·ªôt h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 8cm v√† chi·ªÅu r·ªông 5cm. T√≠nh chu vi c·ªßa h√¨nh ch·ªØ nh·∫≠t.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Format input\n",
        "    formatted_input = processor.apply_chat_template(\n",
        "        test_messages, \n",
        "        tokenize=False, \n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = processor(\n",
        "        text=[formatted_input],\n",
        "        images=[[Image.new('RGB', (32, 32), color=(240, 240, 240))]],  # Placeholder\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs.to(model.device),\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(\"‚úÖ Inference test successful!\")\n",
        "    print(f\"\\nüìù Test Response:\")\n",
        "    print(response)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Inference test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nüéØ Fine-tuning pipeline completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
