{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fine-tuning Gemma3N v·ªõi Unsloth\n",
    "\n",
    "Notebook n√†y s·ª≠ d·ª•ng th∆∞ vi·ªán Unsloth ƒë·ªÉ fine-tuning model Gemma3N - m·ªôt vision-language model c√≥ th·ªÉ x·ª≠ l√Ω c·∫£ text v√† nhi·ªÅu images.\n",
    "\n",
    "## C√°c y√™u c·∫ßu:\n",
    "- S·ª≠ d·ª•ng FastVisionModel ƒë·ªÉ load model \"unsloth/gemma-3n-E4B\"\n",
    "- S·ª≠ d·ª•ng 4bit quantization ƒë·ªÉ gi·∫£m memory\n",
    "- Dataset bao g·ªìm instruction (text + nhi·ªÅu images) v√† answer (text + nhi·ªÅu images)\n",
    "- Fine-tuning c·∫£ vision layers v√† language layers\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f685e1d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. C√†i ƒë·∫∑t v√† Import th∆∞ vi·ªán\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t Unsloth n·∫øu ch∆∞a c√≥\n",
    "# !pip install unsloth\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install transformers datasets pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from unsloth import FastVisionModel  # FastVisionModel for vision-language models\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Load Model Gemma3N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model v√† processor theo requirements\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3n-E4B\",\n",
    "    load_in_4bit=True,  # Use 4bit ƒë·ªÉ gi·∫£m memory usage. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    "    max_seq_length=2048,  # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh t√πy theo nhu c·∫ßu\n",
    "    dtype=None,  # Auto detect\n",
    "    # token=\"hf_...\",  # S·ª≠ d·ª•ng n·∫øu model gated\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model v√† processor ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng!\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Processor: {processor.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Configure PEFT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model cho fine-tuning theo requirements\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,     # False n·∫øu kh√¥ng fine-tuning vision layers\n",
    "    finetune_language_layers=True,   # False n·∫øu kh√¥ng fine-tuning language layers\n",
    "    finetune_attention_modules=True, # False n·∫øu kh√¥ng fine-tuning attention layers\n",
    "    finetune_mlp_modules=True,       # False n·∫øu kh√¥ng fine-tuning MLP layers\n",
    "\n",
    "    r=32,                           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha=32,                  # Recommended alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,               # We support rank stabilized LoRA\n",
    "    loftq_config=None,              # And LoftQ\n",
    "    target_modules=\"all-linear\",    # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PEFT model configuration ho√†n th√†nh!\")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. T·∫°o Dataset m·∫´u\n",
    "\n",
    "Dataset bao g·ªìm:\n",
    "- Instruction: text + nhi·ªÅu images\n",
    "- Answer: text + nhi·ªÅu images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o dataset m·∫´u cho vision-language training\n",
    "def create_sample_images(size=(224, 224), num_images=3):\n",
    "    \"\"\"T·∫°o images m·∫´u cho demo\"\"\"\n",
    "    images = []\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Red, Green, Blue\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # T·∫°o image m√†u ƒë∆°n gi·∫£n\n",
    "        img_array = np.full((*size, 3), colors[i % len(colors)], dtype=np.uint8)\n",
    "        img = Image.fromarray(img_array)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# T·∫°o sample dataset\n",
    "def create_vision_language_dataset():\n",
    "    \"\"\"T·∫°o dataset m·∫´u cho vision-language fine-tuning\"\"\"\n",
    "    \n",
    "    # Sample data v·ªõi format ph√π h·ª£p cho vision-language model\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe what you see in these images:\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[1]},\n",
    "                        {\"type\": \"text\", \"text\": \"What are the main colors?\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"I can see two images with distinct colors. The first image is predominantly red, and the second image is predominantly green. These are solid color images that demonstrate different hues.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Compare these images and tell me the differences:\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[2]}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"The main difference between these two images is their color. The first image is red while the second image is blue. Both images appear to be solid color backgrounds with the same dimensions.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"What do you think about this sequence of images?\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[1]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[2]}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"This sequence shows three images with primary colors: red, green, and blue (RGB). This represents the fundamental color components used in digital displays and image processing. The sequence demonstrates the three primary colors of light.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return Dataset.from_list(sample_data)\n",
    "\n",
    "# T·∫°o dataset\n",
    "dataset = create_vision_language_dataset()\n",
    "print(f\"‚úÖ Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Sample keys: {dataset[0].keys()}\")\n",
    "print(f\"First sample structure: {type(dataset[0]['messages'])}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Data Processing Function\n",
    "\n",
    "Chu·∫©n b·ªã function ƒë·ªÉ x·ª≠ l√Ω data cho training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a580646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(examples):\n",
    "    \"\"\"Apply chat template cho dataset\"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Process messages ƒë·ªÉ t·∫°o text format ph√π h·ª£p\n",
    "        formatted_text = \"\"\n",
    "        \n",
    "        for message in messages:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                formatted_text += f\"<|system|>\\\\n{content[0]['text']}\\\\n\"\n",
    "            elif role == \"user\":\n",
    "                formatted_text += f\"<|user|>\\\\n\"\n",
    "                for item in content:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        formatted_text += item[\"text\"] + \" \"\n",
    "                    elif item[\"type\"] == \"image\":\n",
    "                        formatted_text += \"<image> \"  # Placeholder cho image\n",
    "                formatted_text += \"\\\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                formatted_text += f\"<|assistant|>\\\\n\"\n",
    "                for item in content:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        formatted_text += item[\"text\"]\n",
    "                formatted_text += \"\\\\n\"\n",
    "        \n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(\n",
    "    apply_chat_template,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset formatting completed\")\n",
    "print(\"Sample formatted text:\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma3n_finetuned\",\n",
    "    per_device_train_batch_size=1,  # Gi·∫£m batch size v√¨ vision model t·ªën memory\n",
    "    gradient_accumulation_steps=4,   # ƒê·ªÉ c√≥ effective batch size = 4\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,  # S·ªë steps √≠t cho demo, tƒÉng l√™n cho training th·ª±c t·∫ø\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_8bit\",  # Optimize memory usage\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    dataloader_pin_memory=False,  # Gi·∫£m memory usage\n",
    "    remove_unused_columns=False,  # Gi·ªØ l·∫°i columns cho vision data\n",
    "    seed=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {training_args.max_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Setup SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,  # S·ª≠ d·ª•ng tokenizer t·ª´ processor\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=1,  # Tr√°nh multiprocessing issues\n",
    "    packing=False,  # T·∫Øt packing cho vision model\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer initialized successfully\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Training dataset size: {len(formatted_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B·∫Øt ƒë·∫ßu training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This may take some time depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Clear cache tr∆∞·ªõc khi training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train model\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"Total time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    print(\"This might be due to memory constraints or compatibility issues.\")\n",
    "    print(\"Try reducing batch_size or max_steps.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d40193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and processor\n",
    "output_dir = \"./gemma3n_finetuned_final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save using Unsloth's optimized save method\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "    \n",
    "    # Optional: Save as merged model for deployment\n",
    "    print(\"Saving merged model for deployment...\")\n",
    "    model.save_pretrained_merged(\n",
    "        f\"{output_dir}_merged\", \n",
    "        processor.tokenizer, \n",
    "        save_method=\"merged_16bit\"\n",
    "    )\n",
    "    print(f\"‚úÖ Merged model saved to {output_dir}_merged\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Save error: {e}\")\n",
    "    print(\"Trying alternative save method...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Inference Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference v·ªõi model ƒë√£ fine-tune\n",
    "print(\"üß™ Testing inference...\")\n",
    "\n",
    "try:\n",
    "    # Enable inference mode\n",
    "    FastVisionModel.for_inference(model)\n",
    "    \n",
    "    # Prepare test input\n",
    "    test_prompt = \"\"\"<|system|>\n",
    "You are a helpful vision-language assistant.\n",
    "<|user|>\n",
    "What do you see in this image? <image>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = processor.tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"‚úÖ Inference successful!\")\n",
    "    print(\"Generated response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Inference error: {e}\")\n",
    "    print(\"This is expected if using sample data without real images.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 11. Memory Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "print(\"üßπ Cleaning up memory...\")\n",
    "\n",
    "del model\n",
    "del processor\n",
    "del trainer\n",
    "del dataset\n",
    "del formatted_dataset\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory freed. Available: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated():.2f} MB\")\n",
    "\n",
    "print(\"‚úÖ Cleanup completed\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng th·ª±c t·∫ø\n",
    "\n",
    "### ƒê·ªÉ s·ª≠ d·ª•ng v·ªõi dataset th·ª±c t·∫ø:\n",
    "\n",
    "1. **Chu·∫©n b·ªã dataset**: Thay th·∫ø sample dataset b·∫±ng dataset th·ª±c v·ªõi format:\n",
    "   ```python\n",
    "   {\n",
    "       \"messages\": [\n",
    "           {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": [\n",
    "                   {\"type\": \"text\", \"text\": \"Your instruction\"},\n",
    "                   {\"type\": \"image\", \"image\": PIL_Image_object},\n",
    "                   # Nhi·ªÅu images kh√°c...\n",
    "               ]\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"assistant\",\n",
    "               \"content\": [\n",
    "                   {\"type\": \"text\", \"text\": \"Response text\"},\n",
    "                   {\"type\": \"image\", \"image\": PIL_Image_object},  # Optional\n",
    "               ]\n",
    "           }\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **ƒêi·ªÅu ch·ªânh hyperparameters**:\n",
    "   - TƒÉng `max_steps` cho training d√†i h∆°n\n",
    "   - ƒêi·ªÅu ch·ªânh `learning_rate` t√πy theo dataset\n",
    "   - TƒÉng `per_device_train_batch_size` n·∫øu c√≥ ƒë·ªß GPU memory\n",
    "\n",
    "3. **Monitoring training**:\n",
    "   - S·ª≠ d·ª•ng TensorBoard ho·∫∑c Wandb ƒë·ªÉ theo d√µi loss\n",
    "   - Implement validation set cho early stopping\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Load model t·ª´ saved checkpoint\n",
    "   - S·ª≠ d·ª•ng `FastVisionModel.for_inference()` cho inference nhanh h∆°n\n",
    "\n",
    "### Tips:\n",
    "- Gemma3N l√† vision-language model m·∫°nh, c·∫ßn GPU v·ªõi √≠t nh·∫•t 16GB VRAM\n",
    "- S·ª≠ d·ª•ng gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám memory\n",
    "- Fine-tune t·ª´ t·ª´: b·∫Øt ƒë·∫ßu v·ªõi learning rate th·∫•p\n",
    "- Test v·ªõi sample nh·ªè tr∆∞·ªõc khi train full dataset\n",
    "\n",
    "### S·ª≠ d·ª•ng model ƒë√£ fine-tune:\n",
    "\n",
    "```python\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "# Load fine-tuned model\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"./gemma3n_finetuned_final\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# S·ª≠ d·ª•ng cho inference v·ªõi images th·ª±c t·∫ø\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
