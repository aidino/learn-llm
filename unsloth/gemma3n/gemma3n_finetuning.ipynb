{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fine-tuning Gemma3N với Unsloth\n",
    "\n",
    "Notebook này sử dụng thư viện Unsloth để fine-tuning model Gemma3N - một vision-language model có thể xử lý cả text và nhiều images.\n",
    "\n",
    "## Các yêu cầu:\n",
    "- Sử dụng FastVisionModel để load model \"unsloth/gemma-3n-E4B\"\n",
    "- Sử dụng 4bit quantization để giảm memory\n",
    "- Dataset bao gồm instruction (text + nhiều images) và answer (text + nhiều images)\n",
    "- Fine-tuning cả vision layers và language layers\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f685e1d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Cài đặt và Import thư viện\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt Unsloth nếu chưa có\n",
    "# !pip install unsloth\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install transformers datasets pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "from unsloth import FastVisionModel  # FastVisionModel for vision-language models\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Load Model Gemma3N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model và processor theo requirements\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3n-E4B\",\n",
    "    load_in_4bit=True,  # Use 4bit để giảm memory usage. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    "    max_seq_length=2048,  # Có thể điều chỉnh tùy theo nhu cầu\n",
    "    dtype=None,  # Auto detect\n",
    "    # token=\"hf_...\",  # Sử dụng nếu model gated\n",
    ")\n",
    "\n",
    "print(\"✅ Model và processor đã được load thành công!\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Processor: {processor.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Configure PEFT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model cho fine-tuning theo requirements\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,     # False nếu không fine-tuning vision layers\n",
    "    finetune_language_layers=True,   # False nếu không fine-tuning language layers\n",
    "    finetune_attention_modules=True, # False nếu không fine-tuning attention layers\n",
    "    finetune_mlp_modules=True,       # False nếu không fine-tuning MLP layers\n",
    "\n",
    "    r=32,                           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha=32,                  # Recommended alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,               # We support rank stabilized LoRA\n",
    "    loftq_config=None,              # And LoftQ\n",
    "    target_modules=\"all-linear\",    # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"✅ PEFT model configuration hoàn thành!\")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Tạo Dataset mẫu\n",
    "\n",
    "Dataset bao gồm:\n",
    "- Instruction: text + nhiều images\n",
    "- Answer: text + nhiều images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo dataset mẫu cho vision-language training\n",
    "def create_sample_images(size=(224, 224), num_images=3):\n",
    "    \"\"\"Tạo images mẫu cho demo\"\"\"\n",
    "    images = []\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Red, Green, Blue\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Tạo image màu đơn giản\n",
    "        img_array = np.full((*size, 3), colors[i % len(colors)], dtype=np.uint8)\n",
    "        img = Image.fromarray(img_array)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Tạo sample dataset\n",
    "def create_vision_language_dataset():\n",
    "    \"\"\"Tạo dataset mẫu cho vision-language fine-tuning\"\"\"\n",
    "    \n",
    "    # Sample data với format phù hợp cho vision-language model\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe what you see in these images:\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[1]},\n",
    "                        {\"type\": \"text\", \"text\": \"What are the main colors?\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"I can see two images with distinct colors. The first image is predominantly red, and the second image is predominantly green. These are solid color images that demonstrate different hues.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Compare these images and tell me the differences:\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[2]}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"The main difference between these two images is their color. The first image is red while the second image is blue. Both images appear to be solid color backgrounds with the same dimensions.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful vision-language assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"What do you think about this sequence of images?\"},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[0]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[1]},\n",
    "                        {\"type\": \"image\", \"image\": create_sample_images()[2]}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"This sequence shows three images with primary colors: red, green, and blue (RGB). This represents the fundamental color components used in digital displays and image processing. The sequence demonstrates the three primary colors of light.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return Dataset.from_list(sample_data)\n",
    "\n",
    "# Tạo dataset\n",
    "dataset = create_vision_language_dataset()\n",
    "print(f\"✅ Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Sample keys: {dataset[0].keys()}\")\n",
    "print(f\"First sample structure: {type(dataset[0]['messages'])}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Data Processing Function\n",
    "\n",
    "Chuẩn bị function để xử lý data cho training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a580646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(examples):\n",
    "    \"\"\"Apply chat template cho dataset\"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Process messages để tạo text format phù hợp\n",
    "        formatted_text = \"\"\n",
    "        \n",
    "        for message in messages:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                formatted_text += f\"<|system|>\\\\n{content[0]['text']}\\\\n\"\n",
    "            elif role == \"user\":\n",
    "                formatted_text += f\"<|user|>\\\\n\"\n",
    "                for item in content:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        formatted_text += item[\"text\"] + \" \"\n",
    "                    elif item[\"type\"] == \"image\":\n",
    "                        formatted_text += \"<image> \"  # Placeholder cho image\n",
    "                formatted_text += \"\\\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                formatted_text += f\"<|assistant|>\\\\n\"\n",
    "                for item in content:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        formatted_text += item[\"text\"]\n",
    "                formatted_text += \"\\\\n\"\n",
    "        \n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(\n",
    "    apply_chat_template,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"✅ Dataset formatting completed\")\n",
    "print(\"Sample formatted text:\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma3n_finetuned\",\n",
    "    per_device_train_batch_size=1,  # Giảm batch size vì vision model tốn memory\n",
    "    gradient_accumulation_steps=4,   # Để có effective batch size = 4\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,  # Số steps ít cho demo, tăng lên cho training thực tế\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_8bit\",  # Optimize memory usage\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    dataloader_pin_memory=False,  # Giảm memory usage\n",
    "    remove_unused_columns=False,  # Giữ lại columns cho vision data\n",
    "    seed=3407,\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {training_args.max_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Setup SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,  # Sử dụng tokenizer từ processor\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=1,  # Tránh multiprocessing issues\n",
    "    packing=False,  # Tắt packing cho vision model\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"✅ SFTTrainer initialized successfully\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Training dataset size: {len(formatted_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bắt đầu training\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"This may take some time depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Clear cache trước khi training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train model\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"Total time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed with error: {e}\")\n",
    "    print(\"This might be due to memory constraints or compatibility issues.\")\n",
    "    print(\"Try reducing batch_size or max_steps.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d40193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and processor\n",
    "output_dir = \"./gemma3n_finetuned_final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save using Unsloth's optimized save method\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"✅ Model saved to {output_dir}\")\n",
    "    \n",
    "    # Optional: Save as merged model for deployment\n",
    "    print(\"Saving merged model for deployment...\")\n",
    "    model.save_pretrained_merged(\n",
    "        f\"{output_dir}_merged\", \n",
    "        processor.tokenizer, \n",
    "        save_method=\"merged_16bit\"\n",
    "    )\n",
    "    print(f\"✅ Merged model saved to {output_dir}_merged\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Save error: {e}\")\n",
    "    print(\"Trying alternative save method...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Inference Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference với model đã fine-tune\n",
    "print(\"🧪 Testing inference...\")\n",
    "\n",
    "try:\n",
    "    # Enable inference mode\n",
    "    FastVisionModel.for_inference(model)\n",
    "    \n",
    "    # Prepare test input\n",
    "    test_prompt = \"\"\"<|system|>\n",
    "You are a helpful vision-language assistant.\n",
    "<|user|>\n",
    "What do you see in this image? <image>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = processor.tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"✅ Inference successful!\")\n",
    "    print(\"Generated response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Inference error: {e}\")\n",
    "    print(\"This is expected if using sample data without real images.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 11. Memory Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "print(\"🧹 Cleaning up memory...\")\n",
    "\n",
    "del model\n",
    "del processor\n",
    "del trainer\n",
    "del dataset\n",
    "del formatted_dataset\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory freed. Available: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated():.2f} MB\")\n",
    "\n",
    "print(\"✅ Cleanup completed\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. Hướng dẫn sử dụng thực tế\n",
    "\n",
    "### Để sử dụng với dataset thực tế:\n",
    "\n",
    "1. **Chuẩn bị dataset**: Thay thế sample dataset bằng dataset thực với format:\n",
    "   ```python\n",
    "   {\n",
    "       \"messages\": [\n",
    "           {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": [\n",
    "                   {\"type\": \"text\", \"text\": \"Your instruction\"},\n",
    "                   {\"type\": \"image\", \"image\": PIL_Image_object},\n",
    "                   # Nhiều images khác...\n",
    "               ]\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"assistant\",\n",
    "               \"content\": [\n",
    "                   {\"type\": \"text\", \"text\": \"Response text\"},\n",
    "                   {\"type\": \"image\", \"image\": PIL_Image_object},  # Optional\n",
    "               ]\n",
    "           }\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Điều chỉnh hyperparameters**:\n",
    "   - Tăng `max_steps` cho training dài hơn\n",
    "   - Điều chỉnh `learning_rate` tùy theo dataset\n",
    "   - Tăng `per_device_train_batch_size` nếu có đủ GPU memory\n",
    "\n",
    "3. **Monitoring training**:\n",
    "   - Sử dụng TensorBoard hoặc Wandb để theo dõi loss\n",
    "   - Implement validation set cho early stopping\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Load model từ saved checkpoint\n",
    "   - Sử dụng `FastVisionModel.for_inference()` cho inference nhanh hơn\n",
    "\n",
    "### Tips:\n",
    "- Gemma3N là vision-language model mạnh, cần GPU với ít nhất 16GB VRAM\n",
    "- Sử dụng gradient checkpointing để tiết kiệm memory\n",
    "- Fine-tune từ từ: bắt đầu với learning rate thấp\n",
    "- Test với sample nhỏ trước khi train full dataset\n",
    "\n",
    "### Sử dụng model đã fine-tune:\n",
    "\n",
    "```python\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "# Load fine-tuned model\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"./gemma3n_finetuned_final\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# Sử dụng cho inference với images thực tế\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
