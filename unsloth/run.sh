#!/bin/bash

# Script cháº¡y tá»± Ä‘á»™ng toÃ n bá»™ quÃ¡ trÃ¬nh fine-tuning Gemma 3n
# Tá»‘i Æ°u cho RTX 3070 8GB VRAM

set -e  # Exit on any error

echo "ðŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Fine-tuning Gemma 3n vá»›i QLora"
echo "=================================================="

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸ $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Check if we're in the right directory
if [ ! -f "gemma3n_qlora_finetune.py" ]; then
    print_error "KhÃ´ng tÃ¬m tháº¥y gemma3n_qlora_finetune.py"
    print_error "Vui lÃ²ng cháº¡y script tá»« thÆ° má»¥c chá»©a cÃ¡c file cáº§n thiáº¿t"
    exit 1
fi

# Step 1: Activate virtual environment
print_status "KÃ­ch hoáº¡t virtual environment..."
if [ -f "/home/dino/workdpace/learn-llm/.venv/bin/activate" ]; then
    source /home/dino/workdpace/learn-llm/.venv/bin/activate
    print_success "Virtual environment Ä‘Ã£ Ä‘Æ°á»£c kÃ­ch hoáº¡t"
else
    print_warning "KhÃ´ng tÃ¬m tháº¥y virtual environment táº¡i /home/dino/workdpace/learn-llm/.venv/"
    print_warning "Tiáº¿p tá»¥c vá»›i Python global environment"
fi

# Step 2: Check Python and CUDA
print_status "Kiá»ƒm tra Python vÃ  CUDA..."
python --version
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
    print_success "CUDA vÃ  GPU available"
else
    print_error "CUDA hoáº·c nvidia-smi khÃ´ng available"
    exit 1
fi

# Step 3: Install dependencies if needed
print_status "Kiá»ƒm tra vÃ  cÃ i Ä‘áº·t dependencies..."
if [ ! -f "requirements.txt" ]; then
    print_error "KhÃ´ng tÃ¬m tháº¥y requirements.txt"
    exit 1
fi

# Check if unsloth is installed
if ! python -c "import unsloth" 2>/dev/null; then
    print_status "CÃ i Ä‘áº·t Unsloth vÃ  dependencies..."
    python setup_environment.py
else
    print_success "Dependencies Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t"
fi

# Step 4: Check .env file
print_status "Kiá»ƒm tra cáº¥u hÃ¬nh Opik..."
if [ ! -f ".env" ]; then
    print_warning "KhÃ´ng tÃ¬m tháº¥y file .env"
    print_status "Táº¡o file .env máº«u..."
    cat > .env << EOF
# Opik Configuration
OPIK_API_KEY=your_opik_api_key_here
OPIK_WORKSPACE=your_workspace_name

# HuggingFace (if needed)
HF_TOKEN=your_huggingface_token

# Other settings
CUDA_VISIBLE_DEVICES=0
EOF
    print_warning "Vui lÃ²ng chá»‰nh sá»­a file .env vá»›i credentials thá»±c táº¿"
    print_warning "Press Enter Ä‘á»ƒ tiáº¿p tá»¥c vá»›i dummy credentials (chá»‰ cho testing)..."
    read -r
fi

# Step 5: Clear GPU memory
print_status "Dá»n dáº¹p GPU memory..."
python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print('GPU memory cleared')
"

# Step 6: Run the training
print_status "Báº¯t Ä‘áº§u fine-tuning..."
print_status "QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt Ä‘áº¿n vÃ i giá» tÃ¹y thuá»™c vÃ o cáº¥u hÃ¬nh"

# Run with error handling
# Check which file to run
if [ -f "gemma_multimodal_qlora_finetuning.py" ]; then
    print_status "Cháº¡y phiÃªn báº£n multimodal Ä‘Ã£ sá»­a lá»—i..."
    TRAINING_SCRIPT="gemma_multimodal_qlora_finetuning.py"
else
    print_status "Cháº¡y phiÃªn báº£n cÆ¡ báº£n..."
    TRAINING_SCRIPT="gemma3n_qlora_finetune.py"
fi

if python "$TRAINING_SCRIPT"; then
    print_success "Fine-tuning hoÃ n táº¥t thÃ nh cÃ´ng!"
    
    # Show results
    echo ""
    echo "ðŸ“Š Káº¿t quáº£:"
    if [ -d "gemma3n_qlora_finetuned" ]; then
        print_success "Model Ä‘Ã£ Ä‘Æ°á»£c save táº¡i: ./gemma3n_qlora_finetuned/"
        ls -la gemma3n_qlora_finetuned/
    fi
    
    if [ -d "gemma3n_qlora_finetuned_merged" ]; then
        print_success "Merged model: ./gemma3n_qlora_finetuned_merged/"
    fi
    
    if [ -d "gemma3n_qlora_finetuned_gguf" ]; then
        print_success "GGUF model: ./gemma3n_qlora_finetuned_gguf/"
    fi
    
    echo ""
    print_success "ðŸŽ‰ HOÃ€N Táº¤T!"
    echo "Kiá»ƒm tra Opik dashboard Ä‘á»ƒ xem experiment tracking"
    echo ""
    echo "Äá»ƒ test model:"
    echo "python -c \"from gemma3n_qlora_finetune import *; trainer = GemmaTrainer(Config()); trainer.load_model_and_tokenizer(); trainer.test_inference()\""
    
else
    print_error "Fine-tuning tháº¥t báº¡i!"
    echo ""
    echo "ðŸ” Debugging tips:"
    echo "1. Kiá»ƒm tra VRAM: nvidia-smi"
    echo "2. Giáº£m batch size trong Config class"
    echo "3. Kiá»ƒm tra log errors á»Ÿ trÃªn"
    echo "4. Cháº¡y: python setup_environment.py Ä‘á»ƒ kiá»ƒm tra setup"
    exit 1
fi

# Step 7: Cleanup (optional)
read -p "CÃ³ muá»‘n dá»n dáº¹p temporary files? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    print_status "Dá»n dáº¹p temporary files..."
    if [ -d "outputs" ]; then
        rm -rf outputs
        print_success "ÄÃ£ xÃ³a thÆ° má»¥c outputs"
    fi
    
    python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print('âœ… GPU memory cleared')
"
fi

echo ""
print_success "Script hoÃ n táº¥t!"
echo "Happy fine-tuning! ðŸš€" 