{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543640c6",
   "metadata": {},
   "source": [
    "# Evaluation concept\n",
    "\n",
    "\n",
    "Khi làm việc với các ứng dụng **LLM**, **nút thắt cổ chai** (`bottleneck`) cho việc lặp lại/cải tiến nhanh hơn thường là **quy trình đánh giá** (`evaluation process`). Mặc dù có thể xem xét thủ công đầu ra của ứng dụng LLM, quy trình này rất chậm và không có **khả năng mở rộng** (`scalable`). Thay vì xem xét thủ công, Opik cho phép bạn **tự động hóa việc đánh giá** ứng dụng LLM của mình.\n",
    "\n",
    "Để hiểu cách chạy các bài đánh giá trong Opik, điều quan trọng là trước tiên bạn phải làm quen với các khái niệm sau:\n",
    "\n",
    "* **Dataset**: Một `dataset` là một tập hợp các mẫu (`samples`) mà ứng dụng LLM của bạn sẽ được đánh giá trên đó. Các `dataset` chỉ lưu trữ `input` (đầu vào) và `expected outputs` (kết quả mong đợi) cho mỗi mẫu, đầu ra từ ứng dụng LLM của bạn sẽ được tính toán và chấm điểm trong suốt quá trình đánh giá.\n",
    "* **Experiment**: Một `experiment` là một lần đánh giá duy nhất cho ứng dụng LLM của bạn. Trong một `experiment`, chúng tôi xử lý từng mục trong `dataset`, tính toán đầu ra dựa trên ứng dụng LLM của bạn và sau đó chấm điểm đầu ra đó.\n",
    "\n",
    "![](https://prod.ferndocs.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Fopik.docs.buildwithfern.com%2Fdocs%2Fopik%2F2025-07-23T08%3A18%3A29.817Z%2Fimg%2Fevaluation%2Fevaluation_concepts.png&w=3840&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a712bf",
   "metadata": {},
   "source": [
    "## Datasets 📊\n",
    "\n",
    "Bước đầu tiên để tự động hóa việc đánh giá ứng dụng LLM của bạn là tạo một **dataset**. Đây là một tập hợp các mẫu (`samples`) mà ứng dụng LLM của bạn sẽ được đánh giá trên đó. Mỗi **dataset** được tạo thành từ các **`Dataset Items`**, nơi lưu trữ **`input`** (đầu vào), **`expected output`** (kết quả mong đợi) và các **`metadata`** (siêu dữ liệu) khác cho một mẫu duy nhất.\n",
    "\n",
    "Do tầm quan trọng của **datasets** trong quy trình đánh giá, các nhóm thường dành một lượng thời gian đáng kể để tuyển chọn và chuẩn bị chúng. Có ba cách chính để tạo một **dataset**:\n",
    "\n",
    "1.  **Tuyển chọn thủ công các ví dụ** ✍️\n",
    "    Là bước đầu tiên, bạn có thể tự tuyển chọn một bộ ví dụ dựa trên kiến thức của mình về ứng dụng đang xây dựng. Bạn cũng có thể tận dụng các **chuyên gia trong lĩnh vực** (`subject matter experts`) để hỗ trợ việc tạo **dataset**.\n",
    "\n",
    "2.  **Sử dụng dữ liệu tổng hợp (`synthetic data`)** ⚙️\n",
    "    Nếu không có đủ dữ liệu để tạo ra một bộ ví dụ đa dạng, bạn có thể sử dụng các công cụ tạo dữ liệu tổng hợp để giúp bạn tạo **dataset**. `LangChain cookbook` có một ví dụ tuyệt vời về cách sử dụng các công cụ này để tạo **dataset**.\n",
    "\n",
    "3.  **Tận dụng dữ liệu thực tế (`production data`)** 📈\n",
    "    Nếu ứng dụng của bạn đang hoạt động trong môi trường `production`, bạn có thể tận dụng dữ liệu được tạo ra để bổ sung cho **dataset** của mình. Mặc dù đây thường không phải là bước đầu tiên, nhưng nó là một cách tuyệt vời để làm phong phú **dataset** của bạn với dữ liệu từ thế giới thực.\n",
    "\n",
    "***\n",
    "\n",
    "Nếu bạn đang sử dụng Opik để giám sát môi trường `production`, bạn có thể dễ dàng thêm các **`traces`** (dấu vết) vào **dataset** của mình bằng cách chọn chúng trong giao-diện-người-dùng (`UI`) và chọn `Add to dataset` trong menu thả xuống `Actions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e31292",
   "metadata": {},
   "source": [
    "\n",
    "## Experiments 🧪\n",
    "\n",
    "**`Experiments`** là khối xây dựng cốt lõi của nền tảng đánh giá Opik. Mỗi khi bạn chạy một bài đánh giá mới, một **`experiment`** mới sẽ được tạo ra. Mỗi **`experiment`** bao gồm hai thành phần chính:\n",
    "\n",
    "* **`Experiment Configuration`**: Đối tượng cấu hình (`configuration object`) liên kết với mỗi **`experiment`** cho phép bạn theo dõi một số **`metadata`**. Bạn thường sẽ dùng trường này để lưu trữ, ví dụ như, **`prompt template`** đã được sử dụng cho **`experiment`** đó.\n",
    "* **`Experiment Items`**: Các **`Experiment items`** lưu trữ **`input`** (đầu vào), **`expected output`** (kết quả mong đợi), **`actual output`** (kết quả thực tế) và **`feedback scores`** (điểm phản hồi) cho mỗi mẫu trong **`dataset`** đã được xử lý trong một **`experiment`**.\n",
    "\n",
    "Ngoài ra, đối với mỗi **`experiment`**, bạn sẽ có thể thấy **điểm số trung bình** (`average scores`) cho từng **chỉ số** (`metric`).\n",
    "\n",
    "---\n",
    "\n",
    "## Cấu hình Thử nghiệm (Experiment Configuration) ⚙️\n",
    "\n",
    "Một trong những ưu điểm chính của việc có một nền tảng đánh giá tự động là **khả năng lặp lại/cải tiến nhanh chóng**. Nhược điểm chính là việc theo dõi những gì đã thay đổi giữa hai lần lặp khác nhau của một **`experiment`** có thể trở nên khó khăn.\n",
    "\n",
    "Đối tượng **`experiment configuration`** cho phép bạn lưu trữ một số **`metadata`** liên quan đến một **`experiment`** cụ thể. Điều này hữu ích cho việc theo dõi những thứ như **`prompt template`**, **`model`** đã sử dụng, **`temperature`**, v.v.\n",
    "\n",
    "Sau đó, bạn có thể so sánh cấu hình của hai **`experiments`** khác nhau từ **giao-diện-người-dùng** (`UI`) của Opik để xem những gì đã thay đổi.\n",
    "\n",
    "![](https://prod.ferndocs.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Fopik.docs.buildwithfern.com%2Fdocs%2Fopik%2F2025-07-23T08%3A18%3A29.817Z%2Fimg%2Fevaluation%2Fcompare_experiment_config.png&w=3840&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37ad38",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment Items 📝\n",
    "\n",
    "**`Experiment items`** lưu trữ **`input`** (đầu vào), **`expected output`** (kết quả mong đợi), **`actual output`** (kết quả thực tế) và **`feedback scores`** (điểm phản hồi) cho mỗi mẫu trong `dataset` đã được xử lý trong một `experiment`. Ngoài ra, một **`trace`** (dấu vết) được liên kết với mỗi `item` để cho phép bạn dễ dàng hiểu tại sao một `item` cụ thể lại được chấm điểm như vậy.\n",
    "![](https://prod.ferndocs.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Fopik.docs.buildwithfern.com%2Fdocs%2Fopik%2F2025-07-23T08%3A18%3A29.817Z%2Fimg%2Fevaluation%2Fexperiment_items.png&w=3840&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3274343",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
